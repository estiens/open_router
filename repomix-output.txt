This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-08-15T19:06:22.476Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.github/
  workflows/
    main.yml
bin/
  console
  setup
examples/
  model_selection_example.rb
  smart_completion_example.rb
  structured_outputs_example.rb
  tool_calling_example.rb
lib/
  open_router/
    client.rb
    http.rb
    model_registry.rb
    model_selector.rb
    response.rb
    schema.rb
    tool_call.rb
    tool.rb
    version.rb
  open_router.rb
sig/
  open_router.rbs
spec/
  integration/
    structured_output_flow_spec.rb
  vcr/
    basic_completion_spec.rb
    error_handling_spec.rb
    model_fallback_spec.rb
    model_registry_spec.rb
    response_healing_spec.rb
    simple_healing_test.rb
    structured_outputs_spec.rb
    tool_calling_spec.rb
  client_capability_validation_spec.rb
  client_integration_spec.rb
  contract_tests_spec.rb
  debug_healing_spec.rb
  force_structured_output_spec.rb
  healing_with_errors_spec.rb
  integration_scenarios_spec.rb
  model_registry_spec.rb
  model_selector_spec.rb
  mutation_test_examples.rb
  open_router_spec.rb
  performance_spec.rb
  property_based_examples.rb
  response_healing_spec.rb
  response_mode_spec.rb
  response_spec.rb
  schema_enhancements_spec.rb
  schema_spec.rb
  spec_helper.rb
  tool_call_spec.rb
  tool_spec.rb
.gitignore
.repomixignore
.rspec
.rubocop_todo.yml
.rubocop.yml
.ruby-version
Gemfile
Gemfile.lock
open_router.gemspec
Rakefile
test_structured_output_fix.rb

================================================================
Files
================================================================

================
File: .github/workflows/main.yml
================
name: Ruby

on:
  push:
    branches:
      - main

  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest
    name: Ruby ${{ matrix.ruby }}
    strategy:
      matrix:
        ruby:
          - '3.2.2'

    steps:
    - uses: actions/checkout@v3
    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ matrix.ruby }}
        bundler-cache: true
    - name: Run the default task
      run: bundle exec rake
      env:
        ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}

================
File: bin/console
================
#!/usr/bin/env ruby
# frozen_string_literal: true

require "bundler/setup"
require "open_router"

# You can add fixtures and/or initialization code here to make experimenting
# with your gem easier. You can also use a different console, if you like.

require "irb"
IRB.start(__FILE__)

================
File: bin/setup
================
#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'
set -vx

bundle install

# Do any other automated setup that you need to do here

================
File: examples/model_selection_example.rb
================
#!/usr/bin/env ruby
# frozen_string_literal: true

# Example demonstrating the ModelSelector functionality
# Run this with: ruby -I lib examples/model_selection_example.rb

require "open_router"

# Configure OpenRouter (you would set your actual API key)
OpenRouter.configure do |config|
  config.access_token = ENV["OPENROUTER_API_KEY"] || "your-api-key-here"
  config.site_name = "ModelSelector Example"
  config.site_url = "https://example.com"
end

puts "🤖 OpenRouter ModelSelector Examples"
puts "=" * 50

# Example 1: Basic cost optimization
puts "\n1. Basic cost optimization:"
selector = OpenRouter::ModelSelector.new
cheapest_model = selector.optimize_for(:cost).choose

if cheapest_model
  puts "   Cheapest model: #{cheapest_model}"
  cost_info = OpenRouter::ModelRegistry.get_model_info(cheapest_model)
  puts "   Cost: $#{cost_info[:cost_per_1k_tokens][:input]} per 1k input tokens"
else
  puts "   No models available"
end

# Example 2: Find models with specific capabilities
puts "\n2. Models with function calling capability:"
function_models = OpenRouter::ModelSelector.new
                                           .require(:function_calling)
                                           .optimize_for(:cost)
                                           .choose_with_fallbacks(limit: 3)

if function_models.any?
  function_models.each_with_index do |model, i|
    puts "   #{i + 1}. #{model}"
  end
else
  puts "   No models with function calling found"
end

# Example 3: Budget-constrained selection with multiple requirements
puts "\n3. Budget-constrained selection ($0.01 max, with vision):"
budget_model = OpenRouter::ModelSelector.new
                                        .within_budget(max_cost: 0.01)
                                        .require(:vision)
                                        .optimize_for(:cost)
                                        .choose

if budget_model
  puts "   Selected: #{budget_model}"
  model_info = OpenRouter::ModelRegistry.get_model_info(budget_model)
  puts "   Capabilities: #{model_info[:capabilities].join(", ")}"
  puts "   Cost: $#{model_info[:cost_per_1k_tokens][:input]} per 1k input tokens"
else
  puts "   No models found within budget with vision capability"
end

# Example 4: Provider preferences
puts "\n4. Prefer specific providers:"
provider_model = OpenRouter::ModelSelector.new
                                          .prefer_providers("anthropic", "openai")
                                          .require(:function_calling)
                                          .optimize_for(:cost)
                                          .choose

if provider_model
  puts "   Selected: #{provider_model}"
  provider = provider_model.split("/").first
  puts "   Provider: #{provider}"
else
  puts "   No models found from preferred providers"
end

# Example 5: Latest models with fallback
puts "\n5. Latest models (with graceful fallback):"
latest_model = OpenRouter::ModelSelector.new
                                        .optimize_for(:latest)
                                        .require(:function_calling)
                                        .min_context(100_000)
                                        .choose_with_fallback

if latest_model
  puts "   Selected: #{latest_model}"
  model_info = OpenRouter::ModelRegistry.get_model_info(latest_model)
  puts "   Context length: #{model_info[:context_length]} tokens"
  puts "   Released: #{Time.at(model_info[:created_at])}"
else
  puts "   No suitable models found"
end

# Example 6: Complex chaining example
puts "\n6. Complex requirements with method chaining:"
complex_selector = OpenRouter::ModelSelector.new
                                            .optimize_for(:performance)
                                            .require(:function_calling, :structured_outputs)
                                            .within_budget(max_cost: 0.05)
                                            .avoid_patterns("*-free", "*-preview")
                                            .prefer_providers("anthropic", "openai")

models = complex_selector.choose_with_fallbacks(limit: 2)
if models.any?
  puts "   Found #{models.length} suitable models:"
  models.each_with_index do |model, i|
    model_info = OpenRouter::ModelRegistry.get_model_info(model)
    puts "   #{i + 1}. #{model} (#{model_info[:performance_tier]} tier)"
  end
else
  puts "   No models meet all requirements"
end

# Example 7: Cost estimation
puts "\n7. Cost estimation:"
if cheapest_model
  estimated_cost = OpenRouter::ModelSelector.new.estimate_cost(
    cheapest_model,
    input_tokens: 2000,
    output_tokens: 500
  )
  puts "   Cost for 2000 input + 500 output tokens with #{cheapest_model}:"
  puts "   $#{estimated_cost.round(6)}"
end

# Example 8: Selection criteria inspection
puts "\n8. Selection criteria:"
criteria = OpenRouter::ModelSelector.new
                                    .optimize_for(:cost)
                                    .require(:function_calling)
                                    .within_budget(max_cost: 0.02)
                                    .selection_criteria

puts "   Strategy: #{criteria[:strategy]}"
puts "   Required capabilities: #{criteria[:requirements][:capabilities]}"
puts "   Max cost: $#{criteria[:requirements][:max_input_cost]}"

puts "\n✅ ModelSelector examples completed!"

================
File: examples/smart_completion_example.rb
================
#!/usr/bin/env ruby
# frozen_string_literal: true

# Example demonstrating the Client's smart completion methods
# Run this with: ruby -I lib examples/smart_completion_example.rb

require "open_router"

# NOTE: This example shows the interface but won't make real API calls
# To test with real API calls, set your OPENROUTER_API_KEY environment variable

puts "🧠 Smart Completion Examples"
puts "=" * 40

# Create a client
client = OpenRouter::Client.new

# Example 1: Using the select_model helper
puts "\n1. Using select_model helper:"
selector = client.select_model
                 .optimize_for(:cost)
                 .require(:function_calling)
                 .within_budget(max_cost: 0.01)

selected_model = selector.choose
puts "   Selected model: #{selected_model}"

# Example 2: Smart completion with requirements
puts "\n2. Smart completion interface:"
requirements = {
  capabilities: [:function_calling],
  max_input_cost: 0.01,
  providers: {
    prefer: %w[anthropic openai],
    avoid: ["google"]
  }
}

messages = [
  { role: "user", content: "What is the weather like today?" }
]

puts "   Requirements: #{requirements}"
puts "   Messages: #{messages}"

# NOTE: This would make a real API call if OPENROUTER_API_KEY is set
# For demo purposes, we'll show what model would be selected
selector_for_smart = OpenRouter::ModelSelector.new
                                              .optimize_for(:cost)
                                              .require(*requirements[:capabilities])
                                              .within_budget(max_cost: requirements[:max_input_cost])
                                              .prefer_providers(*requirements[:providers][:prefer])
                                              .avoid_providers(*requirements[:providers][:avoid])

smart_model = selector_for_smart.choose
puts "   Would use model: #{smart_model}"

# Example 3: Smart completion with fallback
puts "\n3. Smart completion with fallback:"
fallback_requirements = {
  capabilities: %i[function_calling vision],
  max_input_cost: 0.005, # Very restrictive budget
  min_context_length: 100_000
}

fallback_selector = OpenRouter::ModelSelector.new
                                             .optimize_for(:cost)
                                             .require(*fallback_requirements[:capabilities])
                                             .within_budget(max_cost: fallback_requirements[:max_input_cost])
                                             .min_context(fallback_requirements[:min_context_length])

fallback_models = fallback_selector.choose_with_fallbacks(limit: 3)
puts "   Fallback candidates: #{fallback_models}"

# Example 4: Demonstrating graceful degradation
puts "\n4. Graceful degradation example:"
degradation_model = fallback_selector.choose_with_fallback
puts "   Graceful fallback selected: #{degradation_model}"

# Example 5: Show the actual method signatures
puts "\n5. Available Client methods:"
puts "   - client.select_model() -> ModelSelector"
puts "   - client.smart_complete(messages, requirements:, optimization:)"
puts "   - client.smart_complete_with_fallback(messages, requirements:, max_retries:)"

puts "\n✅ Smart completion examples completed!"
puts "\n💡 To test with real API calls:"
puts "   export OPENROUTER_API_KEY=your_key_here"
puts "   ruby -I lib examples/smart_completion_example.rb"

================
File: examples/structured_outputs_example.rb
================
#!/usr/bin/env ruby
# frozen_string_literal: true

require "bundler/setup"
require "open_router"

# Configure the client
OpenRouter.configure do |config|
  config.access_token = ENV["OPENROUTER_API_KEY"]
  config.site_name = "OpenRouter Ruby Gem Example"
  config.site_url = "https://github.com/OlympiaAI/open_router"
end

OpenRouter::Client.new

# Example 1: Simple structured output
puts "=== Example 1: Simple Weather Schema ==="

weather_schema = OpenRouter::Schema.define("weather") do
  strict true

  string :location, required: true, description: "City or location name"
  number :temperature, required: true, description: "Temperature in Celsius"
  string :conditions, required: true, description: "Weather conditions"
  string :humidity, description: "Humidity percentage"

  no_additional_properties
end

puts "Schema definition:"
puts weather_schema.to_json

# Example 2: Complex nested schema
puts "\n=== Example 2: Complex User Profile Schema ==="

user_schema = OpenRouter::Schema.define("user_profile") do
  string :name, required: true, description: "Full name"
  integer :age, required: true, minimum: 0, maximum: 150
  string :email, required: true, description: "Email address"

  object :address, required: true do
    string :street, required: true
    string :city, required: true
    string :state, required: true
    string :zip_code, required: true
  end

  array :hobbies do
    string description: "A hobby or interest"
  end

  object :preferences do
    boolean :newsletter, description: "Wants to receive newsletter"
    string :theme, description: "UI theme preference"
  end
end

puts "User schema:"
puts JSON.pretty_generate(user_schema.to_h)

# Example 3: Using schemas with API calls
puts "\n=== Example 3: Structured Output API Call ==="

# Simulate a structured response
mock_weather_response = {
  "id" => "chatcmpl-123",
  "choices" => [
    {
      "message" => {
        "role" => "assistant",
        "content" => '{"location": "San Francisco", "temperature": 22, "conditions": "Partly cloudy", "humidity": "65%"}'
      }
    }
  ]
}

response = OpenRouter::Response.new(mock_weather_response, response_format: weather_schema)
puts "Parsed structured output:"
puts response.structured_output.inspect

# Check if output is valid
if response.valid_structured_output?
  puts "✅ Output is valid according to schema"
else
  puts "❌ Output validation failed:"
  puts response.validation_errors
end

# Example 4: Working with different response formats
puts "\n=== Example 4: Different Response Format Styles ==="

# Style 1: Schema object directly
format1 = weather_schema

# Style 2: Hash with schema object
format2 = {
  type: "json_schema",
  json_schema: weather_schema
}

# Style 3: Raw hash format
format3 = {
  type: "json_schema",
  json_schema: {
    name: "simple_weather",
    strict: true,
    schema: {
      type: "object",
      properties: {
        temp: { type: "number" },
        desc: { type: "string" }
      },
      required: %w[temp desc],
      additionalProperties: false
    }
  }
}

puts "All three formats are supported:"
puts "1. Direct schema object: #{format1.class}"
puts "2. Hash with schema object: #{format2[:json_schema].class}"
puts "3. Raw hash format: #{format3[:json_schema].class}"

# Example 5: Real API call example (commented out)
puts "\n=== Example 5: Real API Usage ==="

# # Uncomment to make a real API call
# begin
#   response = client.complete(
#     [{ role: "user", content: "What's the weather like in Tokyo right now?" }],
#     model: "openai/gpt-4o",
#     response_format: weather_schema
#   )
#
#   if response.structured_output
#     weather = response.structured_output
#     puts "Location: #{weather['location']}"
#     puts "Temperature: #{weather['temperature']}°C"
#     puts "Conditions: #{weather['conditions']}"
#     puts "Humidity: #{weather['humidity']}" if weather['humidity']
#
#     if response.valid_structured_output?
#       puts "✅ Response validates against schema"
#     else
#       puts "❌ Validation errors:"
#       response.validation_errors.each { |error| puts "  - #{error}" }
#     end
#   end
#
# rescue OpenRouter::ServerError => e
#   puts "API Error: #{e.message}"
# rescue OpenRouter::StructuredOutputError => e
#   puts "Structured Output Error: #{e.message}"
# rescue => e
#   puts "Unexpected error: #{e.message}"
# end

puts "\n(Structured outputs example complete - uncomment the API call section to test with real API)"

# Example 6: Schema validation demonstration
puts "\n=== Example 6: Schema Validation Demo ==="

if weather_schema.validation_available?
  puts "JSON Schema validation is available"

  # Valid data
  valid_data = {
    "location" => "London",
    "temperature" => 18,
    "conditions" => "Rainy"
  }

  # Invalid data
  invalid_data = {
    "location" => "London",
    "temperature" => "eighteen", # Should be number
    "conditions" => "Rainy"
  }

  puts "Valid data validation: #{weather_schema.validate(valid_data)}"
  puts "Invalid data validation: #{weather_schema.validate(invalid_data)}"

  unless weather_schema.validate(invalid_data)
    puts "Validation errors for invalid data:"
    weather_schema.validation_errors(invalid_data).each do |error|
      puts "  - #{error}"
    end
  end
else
  puts "JSON Schema validation not available (install json-schema gem for validation)"
end

================
File: examples/tool_calling_example.rb
================
#!/usr/bin/env ruby
# frozen_string_literal: true

require "bundler/setup"
require "open_router"

# Configure the client
OpenRouter.configure do |config|
  config.access_token = ENV["OPENROUTER_API_KEY"]
  config.site_name = "OpenRouter Ruby Gem Example"
  config.site_url = "https://github.com/OlympiaAI/open_router"
end

OpenRouter::Client.new

# Example 1: Define a tool using the DSL
puts "=== Example 1: Tool Definition with DSL ==="

search_tool = OpenRouter::Tool.define do
  name "search_gutenberg_books"
  description "Search for books in the Project Gutenberg library"

  parameters do
    array :search_terms, required: true do
      string description: "Search term for finding books"
    end
    integer :max_results, description: "Maximum number of results to return"
  end
end

puts "Tool definition:"
puts search_tool.to_json

# Example 2: Define a tool using hash format
puts "\n=== Example 2: Tool Definition with Hash ==="

weather_tool = OpenRouter::Tool.new({
                                      name: "get_weather",
                                      description: "Get current weather for a location",
                                      parameters: {
                                        type: "object",
                                        properties: {
                                          location: {
                                            type: "string",
                                            description: "City and state, e.g. San Francisco, CA"
                                          },
                                          unit: {
                                            type: "string",
                                            enum: %w[celsius fahrenheit],
                                            description: "Temperature unit"
                                          }
                                        },
                                        required: ["location"]
                                      }
                                    })

puts "Tool definition:"
puts weather_tool.to_json

# Example 3: Tool calling conversation
puts "\n=== Example 3: Tool Calling Conversation ==="

def simulate_search(_search_terms, max_results = 10)
  # Simulate a search function
  results = [
    { title: "Programming Ruby", author: "Dave Thomas", year: 2004 },
    { title: "The Ruby Programming Language", author: "David Flanagan", year: 2008 }
  ]

  results.first(max_results).to_json
end

def simulate_weather(location, unit = "fahrenheit")
  # Simulate a weather API call
  {
    location:,
    temperature: unit == "celsius" ? 22 : 72,
    conditions: "Sunny",
    unit:
  }.to_json
end

# Initial message
messages = [
  { role: "user", content: "Can you search for Ruby programming books and also tell me the weather in San Francisco?" }
]

puts "User: #{messages.first[:content]}"

# Uncomment the following lines to make a real API call:
# begin
#   # Make the tool call request
#   response = client.complete(
#     messages,
#     model: "anthropic/claude-3.5-sonnet",
#     tools: [search_tool, weather_tool],
#     tool_choice: "auto"
#   )
#
#   puts "\nAssistant response:"
#   puts response.content if response.has_content?
#
#   # Handle tool calls
#   if response.has_tool_calls?
#     puts "\nTool calls requested:"
#
#     # Add the assistant's message to conversation
#     messages << response.to_message
#
#     # Execute each tool call
#     response.tool_calls.each do |tool_call|
#       puts "- #{tool_call.name} with arguments: #{tool_call.arguments}"
#
#       # Execute the tool based on its name
#       result = case tool_call.name
#                when "search_gutenberg_books"
#                  args = tool_call.arguments
#                  simulate_search(args["search_terms"], args["max_results"])
#                when "get_weather"
#                  args = tool_call.arguments
#                  simulate_weather(args["location"], args["unit"])
#                else
#                  "Unknown tool: #{tool_call.name}"
#                end
#
#       puts "  Result: #{result}"
#
#       # Add tool result to conversation
#       messages << tool_call.to_result_message(result)
#     end
#
#     # Get final response with tool results
#     final_response = client.complete(
#       messages,
#       model: "anthropic/claude-3.5-sonnet",
#       tools: [search_tool, weather_tool]
#     )
#
#     puts "\nFinal assistant response:"
#     puts final_response.content
#   end
#
# rescue OpenRouter::ServerError => e
#   puts "Error: #{e.message}"
# rescue => e
#   puts "Unexpected error: #{e.message}"
# end

puts "\n(Tool calling example complete - uncomment the API call section to test with real API)"

================
File: lib/open_router/client.rb
================
# frozen_string_literal: true

require "active_support/core_ext/object/blank"
require "active_support/core_ext/hash/indifferent_access"
require "set"

require_relative "http"

module OpenRouter
  class ServerError < StandardError; end

  class Client
    include OpenRouter::HTTP

    # Initializes the client with optional configurations.
    def initialize(access_token: nil, request_timeout: nil, uri_base: nil, extra_headers: {})
      OpenRouter.configuration.access_token = access_token if access_token
      OpenRouter.configuration.request_timeout = request_timeout if request_timeout
      OpenRouter.configuration.uri_base = uri_base if uri_base
      OpenRouter.configuration.extra_headers = extra_headers if extra_headers.any?
      yield(OpenRouter.configuration) if block_given?
    end

    def configuration
      OpenRouter.configuration
    end

    # Performs a chat completion request to the OpenRouter API.
    # @param messages [Array<Hash>] Array of message hashes with role and content, like [{role: "user", content: "What is the meaning of life?"}]
    # @param model [String|Array] Model identifier, or array of model identifiers if you want to fallback to the next model in case of failure
    # @param providers [Array<String>] Optional array of provider identifiers, ordered by priority
    # @param transforms [Array<String>] Optional array of strings that tell OpenRouter to apply a series of transformations to the prompt before sending it to the model. Transformations are applied in-order
    # @param tools [Array<Tool>] Optional array of Tool objects or tool definition hashes for function calling
    # @param tool_choice [String|Hash] Optional tool choice: "auto", "none", "required", or specific tool selection
    # @param response_format [Hash] Optional response format for structured outputs
    # @param extras [Hash] Optional hash of model-specific parameters to send to the OpenRouter API
    # @param stream [Proc, nil] Optional callable object for streaming
    # @return [Response] The completion response wrapped in a Response object.
    def complete(messages, model: "openrouter/auto", providers: [], transforms: [], tools: [], tool_choice: nil, response_format: nil, force_structured_output: nil, extras: {}, stream: nil)
      parameters = { messages: }
      if model.is_a?(String)
        parameters[:model] = model
      elsif model.is_a?(Array)
        parameters[:models] = model
        parameters[:route] = "fallback"
      end
      parameters[:provider] = { provider: { order: providers } } if providers.any?
      parameters[:transforms] = transforms if transforms.any?

      # Add tool calling support
      if tools.any?
        warn_if_unsupported(model, :function_calling, "tool calling")
        parameters[:tools] = serialize_tools(tools)
        parameters[:tool_choice] = tool_choice if tool_choice
      end

      # Add structured output support
      forced_extraction = false
      if response_format
        # Auto-detect if we should force based on model capabilities
        if force_structured_output.nil?
          if model.is_a?(String) && model != "openrouter/auto" && !ModelRegistry.has_capability?(model, :structured_outputs)
            warn "[OpenRouter] Model '#{model}' doesn't support native structured outputs. Automatically using forced extraction mode."
            force_structured_output = true
          else
            force_structured_output = false
          end
        end
        
        if force_structured_output
          # Forced path - inject instructions, DON'T send response_format to API
          inject_schema_instructions!(parameters[:messages], response_format)
          forced_extraction = true
        else
          # Native path - send to API
          warn_if_unsupported(model, :structured_outputs, "structured outputs")
          parameters[:response_format] = serialize_response_format(response_format)
        end
      end

      # Check for vision support if messages contain images
      if messages_contain_images?(messages)
        warn_if_unsupported(model, :vision, "vision/image processing")
      end

      parameters[:stream] = stream if stream
      parameters.merge!(extras)

      begin
        raw_response = post(path: "/chat/completions", parameters:)
      rescue ConfigurationError => e
        # Convert configuration errors to server errors for consistent API
        raise ServerError, e.message
      rescue Faraday::Error => e
        # Convert Faraday errors to OpenRouter errors
        case e
        when Faraday::UnauthorizedError
          raise ServerError, "Unauthorized: #{e.response&.dig(:body, 'error', 'message') || 'Invalid API key'}"
        when Faraday::BadRequestError
          error_message = e.response&.dig(:body, 'error', 'message') || e.message
          raise ServerError, "Bad Request: #{error_message}"
        when Faraday::ServerError
          raise ServerError, "Server Error: #{e.message}"
        else
          raise ServerError, "Network Error: #{e.message}"
        end
      end

      raise ServerError, raw_response.dig("error", "message") if raw_response.presence&.dig("error", "message").present?
      raise ServerError, "Empty response from OpenRouter. Might be worth retrying once or twice." if stream.blank? && raw_response.blank?

      # Return a Response object instead of raw hash
      response = Response.new(raw_response, response_format: response_format, forced_extraction: forced_extraction)
      
      # Set client reference for healing if enabled
      if configuration.auto_heal_responses
        response.client = self
      end
      
      response
    end

    # Fetches the list of available models from the OpenRouter API.
    # @return [Array<Hash>] The list of models.
    def models
      get(path: "/models")["data"]
    end

    # Queries the generation stats for a given id.
    # @param generation_id [String] The generation id returned from a previous request.
    # @return [Hash] The stats including token counts and cost.
    def query_generation_stats(generation_id)
      response = get(path: "/generation?id=#{generation_id}")
      response["data"]
    end

    # Create a new ModelSelector for intelligent model selection
    #
    # @return [ModelSelector] A new ModelSelector instance
    # @example
    #   client = OpenRouter::Client.new
    #   model = client.select_model.optimize_for(:cost).require(:function_calling).choose
    def select_model
      ModelSelector.new
    end

    # Smart completion that automatically selects the best model based on requirements
    #
    # @param messages [Array<Hash>] Array of message hashes
    # @param requirements [Hash] Model selection requirements
    # @param optimization [Symbol] Optimization strategy (:cost, :performance, :latest, :context)
    # @param extras [Hash] Additional parameters for the completion request
    # @return [Response] The completion response
    # @raise [ModelSelectionError] If no suitable model is found
    #
    # @example
    #   response = client.smart_complete(
    #     messages: [{ role: "user", content: "Analyze this data" }],
    #     requirements: { capabilities: [:function_calling], max_input_cost: 0.01 },
    #     optimization: :cost
    #   )
    def smart_complete(messages, requirements: {}, optimization: :cost, **extras)
      selector = ModelSelector.new.optimize_for(optimization)

      # Apply requirements using fluent interface
      selector = selector.require(*requirements[:capabilities]) if requirements[:capabilities]

      if requirements[:max_cost] || requirements[:max_input_cost]
        cost_opts = {}
        cost_opts[:max_cost] = requirements[:max_cost] || requirements[:max_input_cost]
        cost_opts[:max_output_cost] = requirements[:max_output_cost] if requirements[:max_output_cost]
        selector = selector.within_budget(**cost_opts)
      end

      selector = selector.min_context(requirements[:min_context_length]) if requirements[:min_context_length]

      if requirements[:providers]
        case requirements[:providers]
        when Hash
          selector = selector.prefer_providers(*requirements[:providers][:prefer]) if requirements[:providers][:prefer]
          selector = selector.require_providers(*requirements[:providers][:require]) if requirements[:providers][:require]
          selector = selector.avoid_providers(*requirements[:providers][:avoid]) if requirements[:providers][:avoid]
        when Array
          selector = selector.prefer_providers(*requirements[:providers])
        end
      end

      # Select the best model
      model = selector.choose
      raise ModelSelectionError, "No model found matching requirements: #{requirements}" unless model

      # Perform the completion with the selected model
      complete(messages, model:, **extras)
    end

    # Smart completion with automatic fallback to alternative models
    #
    # @param messages [Array<Hash>] Array of message hashes
    # @param requirements [Hash] Model selection requirements
    # @param optimization [Symbol] Optimization strategy
    # @param max_retries [Integer] Maximum number of fallback attempts
    # @param extras [Hash] Additional parameters for the completion request
    # @return [Response] The completion response
    # @raise [ModelSelectionError] If all fallback attempts fail
    #
    # @example
    #   response = client.smart_complete_with_fallback(
    #     messages: [{ role: "user", content: "Hello" }],
    #     requirements: { capabilities: [:function_calling] },
    #     max_retries: 3
    #   )
    def smart_complete_with_fallback(messages, requirements: {}, optimization: :cost, max_retries: 3, **extras)
      selector = ModelSelector.new.optimize_for(optimization)

      # Apply requirements (same logic as smart_complete)
      selector = selector.require(*requirements[:capabilities]) if requirements[:capabilities]

      if requirements[:max_cost] || requirements[:max_input_cost]
        cost_opts = {}
        cost_opts[:max_cost] = requirements[:max_cost] || requirements[:max_input_cost]
        cost_opts[:max_output_cost] = requirements[:max_output_cost] if requirements[:max_output_cost]
        selector = selector.within_budget(**cost_opts)
      end

      selector = selector.min_context(requirements[:min_context_length]) if requirements[:min_context_length]

      if requirements[:providers]
        case requirements[:providers]
        when Hash
          selector = selector.prefer_providers(*requirements[:providers][:prefer]) if requirements[:providers][:prefer]
          selector = selector.require_providers(*requirements[:providers][:require]) if requirements[:providers][:require]
          selector = selector.avoid_providers(*requirements[:providers][:avoid]) if requirements[:providers][:avoid]
        when Array
          selector = selector.prefer_providers(*requirements[:providers])
        end
      end

      # Get fallback models
      fallback_models = selector.choose_with_fallbacks(limit: max_retries + 1)
      raise ModelSelectionError, "No models found matching requirements: #{requirements}" if fallback_models.empty?

      last_error = nil

      fallback_models.each do |model|
          return complete(messages, model:, **extras)
      rescue StandardError => e
          last_error = e
          # Continue to next model in fallback list
      end

      # If we get here, all models failed
      raise ModelSelectionError, "All fallback models failed. Last error: #{last_error&.message}"
    end

    private

    # Track warnings to avoid spamming the same warning multiple times
    @@capability_warnings_shown = Set.new

    # Warn if a model is being used with an unsupported capability
    def warn_if_unsupported(model, capability, feature_name)
      # Skip warnings for array models (fallbacks) or auto-selection
      return if model.is_a?(Array) || model == "openrouter/auto"
      
      unless ModelRegistry.has_capability?(model, capability)
        if configuration.strict_mode
          raise CapabilityError, "Model '#{model}' does not support #{feature_name} (missing :#{capability} capability). Enable non-strict mode to allow this request."
        else
          warning_key = "#{model}:#{capability}"
          return if @@capability_warnings_shown.include?(warning_key)
          
          warn "[OpenRouter Warning] Model '#{model}' may not support #{feature_name} (missing :#{capability} capability). The request will still be attempted."
          @@capability_warnings_shown << warning_key
        end
      end
    end

    # Check if messages contain image content
    def messages_contain_images?(messages)
      messages.any? do |msg|
        content = msg[:content] || msg["content"]
        if content.is_a?(Array)
          content.any? { |part| part.is_a?(Hash) && part[:type] == "image_url" }
        else
          false
        end
      end
    end

    # Serialize tools to the format expected by OpenRouter API
    def serialize_tools(tools)
      tools.map do |tool|
        case tool
        when Tool
          tool.to_h
        when Hash
          tool
        else
          raise ArgumentError, "Tools must be Tool objects or hashes"
        end
      end
    end

    # Serialize response format to the format expected by OpenRouter API
    def serialize_response_format(response_format)
      case response_format
      when Hash
        if response_format[:json_schema].is_a?(Schema)
          response_format.merge(json_schema: response_format[:json_schema].to_h)
        else
          response_format
        end
      when Schema
        {
          type: "json_schema",
          json_schema: response_format.to_h
        }
      else
        response_format
      end
    end

    # Inject schema instructions into messages for forced structured output
    def inject_schema_instructions!(messages, response_format)
      schema = extract_schema(response_format)
      return unless schema

      instruction_content = if schema.respond_to?(:get_format_instructions)
        schema.get_format_instructions
      else
        build_schema_instruction(schema)
      end

      # Add as system message
      messages << { role: "system", content: instruction_content }
    end

    # Extract schema from response_format
    def extract_schema(response_format)
      case response_format
      when Schema
        response_format
      when Hash
        # Handle both Schema objects and raw hash schemas
        if response_format[:json_schema].is_a?(Schema)
          response_format[:json_schema]
        elsif response_format[:json_schema].is_a?(Hash)
          response_format[:json_schema]
        else
          response_format
        end
      end
    end

    # Build schema instruction when schema doesn't have get_format_instructions
    def build_schema_instruction(schema)
      schema_json = schema.respond_to?(:to_h) ? schema.to_h.to_json : schema.to_json
      
      <<~INSTRUCTION
        You must respond with valid JSON matching this exact schema:

        ```json
        #{schema_json}
        ```

        Rules:
        - Return ONLY the JSON object, no other text
        - Ensure all required fields are present
        - Match the exact data types specified
        - Follow any format constraints (email, date, etc.)
        - Do not include trailing commas or comments
      INSTRUCTION
    end
  end
end

================
File: lib/open_router/http.rb
================
# frozen_string_literal: true

module OpenRouter
  module HTTP
    def get(path:)
      conn.get(uri(path:)) do |req|
        req.headers = headers
      end&.body
    end

    def post(path:, parameters:)
      conn.post(uri(path:)) do |req|
        if parameters[:stream].respond_to?(:call)
          req.options.on_data = to_json_stream(user_proc: parameters[:stream])
          parameters[:stream] = true # Necessary to tell OpenRouter to stream.
        end

        req.headers = headers
        req.body = parameters.to_json
      end&.body
    end

    def multipart_post(path:, parameters: nil)
      conn(multipart: true).post(uri(path:)) do |req|
        req.headers = headers.merge({ "Content-Type" => "multipart/form-data" })
        req.body = multipart_parameters(parameters)
      end&.body
    end

    def delete(path:)
      conn.delete(uri(path:)) do |req|
        req.headers = headers
      end&.body
    end

    private

    # Given a proc, returns an outer proc that can be used to iterate over a JSON stream of chunks.
    # For each chunk, the inner user_proc is called giving it the JSON object. The JSON object could
    # be a data object or an error object as described in the OpenRouter API documentation.
    #
    # If the JSON object for a given data or error message is invalid, it is ignored.
    #
    # @param user_proc [Proc] The inner proc to call for each JSON object in the chunk.
    # @return [Proc] An outer proc that iterates over a raw stream, converting it to JSON.
    def to_json_stream(user_proc:)
      proc do |chunk, _|
        chunk.scan(/(?:data|error): (\{.*\})/i).flatten.each do |data|
          user_proc.call(JSON.parse(data))
        rescue JSON::ParserError
          # Ignore invalid JSON.
        end
      end
    end

    def conn(multipart: false)
      Faraday.new do |f|
        f.options[:timeout] = OpenRouter.configuration.request_timeout
        f.request(:multipart) if multipart
        f.use MiddlewareErrors if @log_errors
        f.response :raise_error
        f.response :json

        OpenRouter.configuration.faraday_config&.call(f)
      end
    end

    def uri(path:)
      File.join(OpenRouter.configuration.uri_base, OpenRouter.configuration.api_version, path)
    end

    def headers
      {
        "Authorization" => "Bearer #{OpenRouter.configuration.access_token}",
        "Content-Type" => "application/json",
        "X-Title" => "OpenRouter Ruby Client",
        "HTTP-Referer" => "https://github.com/OlympiaAI/open_router"
      }.merge(OpenRouter.configuration.extra_headers)
    end

    def multipart_parameters(parameters)
      parameters&.transform_values do |value|
        next value unless value.is_a?(File)

        # Doesn't seem like OpenRouter needs mime_type yet, so not worth
        # the library to figure this out. Hence the empty string
        # as the second argument.
        Faraday::UploadIO.new(value, "", value.path)
      end
    end
  end
end

================
File: lib/open_router/model_registry.rb
================
# frozen_string_literal: true

require "json"
require "net/http"
require "uri"
require "tmpdir"
require "fileutils"

module OpenRouter
  class ModelRegistryError < Error; end

  class ModelRegistry
    API_BASE = "https://openrouter.ai/api/v1"
    CACHE_DIR = File.join(Dir.tmpdir, 'openrouter_cache')
    CACHE_DATA_FILE = File.join(CACHE_DIR, 'models_data.json')
    CACHE_METADATA_FILE = File.join(CACHE_DIR, 'cache_metadata.json')

    class << self
      # Fetch models from OpenRouter API
      def fetch_models_from_api
        uri = URI("#{API_BASE}/models")
        response = Net::HTTP.get_response(uri)

        raise ModelRegistryError, "Failed to fetch models from OpenRouter API: #{response.message}" unless response.code == "200"

        JSON.parse(response.body)
      rescue JSON::ParserError => e
        raise ModelRegistryError, "Failed to parse OpenRouter API response: #{e.message}"
      rescue StandardError => e
        raise ModelRegistryError, "Network error fetching models: #{e.message}"
      end

      # Ensure cache directory exists
      def ensure_cache_dir
        FileUtils.mkdir_p(CACHE_DIR) unless Dir.exist?(CACHE_DIR)
      end

      # Check if cache is stale based on TTL
      def cache_stale?
        return true unless File.exist?(CACHE_METADATA_FILE)

        begin
          metadata = JSON.parse(File.read(CACHE_METADATA_FILE))
          cache_time = metadata['cached_at']
          ttl = OpenRouter.configuration.cache_ttl
          
          return true unless cache_time
          
          Time.now.to_i - cache_time.to_i > ttl
        rescue JSON::ParserError, StandardError
          true # If we can't read metadata, consider cache stale
        end
      end

      # Write cache with timestamp metadata
      def write_cache_with_timestamp(models_data)
        ensure_cache_dir
        
        # Write the actual models data
        File.write(CACHE_DATA_FILE, JSON.pretty_generate(models_data))
        
        # Write metadata with timestamp
        metadata = {
          'cached_at' => Time.now.to_i,
          'version' => '1.0',
          'source' => 'openrouter_api'
        }
        File.write(CACHE_METADATA_FILE, JSON.pretty_generate(metadata))
      end

      # Read cache only if it's fresh
      def read_cache_if_fresh
        return nil if cache_stale?
        return nil unless File.exist?(CACHE_DATA_FILE)

        JSON.parse(File.read(CACHE_DATA_FILE))
      rescue JSON::ParserError
        nil
      end

      # Clear local cache (both files and memory)
      def clear_cache!
        FileUtils.rm_rf(CACHE_DIR) if Dir.exist?(CACHE_DIR)
        @processed_models = nil
        @all_models = nil
      end

      # Refresh models data from API
      def refresh!
        clear_cache!
        fetch_and_cache_models
      end

      # Get processed models (fetch if needed)
      def fetch_and_cache_models
        # Try cache first (only if fresh)
        cached_data = read_cache_if_fresh

        if cached_data
          api_data = cached_data
        else
          # Cache is stale or doesn't exist, fetch from API
          api_data = fetch_models_from_api
          write_cache_with_timestamp(api_data)
        end

        @processed_models = process_api_models(api_data["data"])
      end

      # Find original API model data by model ID
      def find_original_model_data(model_id)
        # Get raw models data (not processed)
        cached_data = read_cache_if_fresh
        
        if cached_data
          api_data = cached_data
        else
          api_data = fetch_models_from_api
          write_cache_with_timestamp(api_data)
        end
        
        raw_models = api_data["data"] || []
        raw_models.find { |model| model["id"] == model_id }
      end

      # Convert API model data to our internal format
      def process_api_models(api_models)
        models = {}

        api_models.each do |model_data|
          model_id = model_data["id"]

          models[model_id] = {
            name: model_data["name"],
            cost_per_1k_tokens: {
              input: model_data["pricing"]["prompt"].to_f,
              output: model_data["pricing"]["completion"].to_f
            },
            context_length: model_data["context_length"],
            capabilities: extract_capabilities(model_data),
            description: model_data["description"],
            supported_parameters: model_data["supported_parameters"] || [],
            architecture: model_data["architecture"],
            performance_tier: determine_performance_tier(model_data),
            fallbacks: determine_fallbacks(model_id, model_data),
            created_at: model_data["created"]
          }
        end

        models
      end

      # Extract capabilities from model data
      def extract_capabilities(model_data)
        capabilities = [:chat] # All models support basic chat

        # Check for function calling support
        supported_params = model_data["supported_parameters"] || []
        capabilities << :function_calling if supported_params.include?("tools") && supported_params.include?("tool_choice")

        # Check for structured output support
        capabilities << :structured_outputs if supported_params.include?("structured_outputs") || supported_params.include?("response_format")

        # Check for vision support
        architecture = model_data["architecture"] || {}
        input_modalities = architecture["input_modalities"] || []
        capabilities << :vision if input_modalities.include?("image")

        # Check for large context support
        context_length = model_data["context_length"] || 0
        capabilities << :long_context if context_length > 100_000

        capabilities
      end

      # Determine performance tier based on pricing and capabilities
      def determine_performance_tier(model_data)
        input_cost = model_data["pricing"]["prompt"].to_f

        # Higher cost generally indicates premium models
        if input_cost > 0.00001 # > $0.01 per 1k tokens
          :premium
        else
          :standard
        end
      end

      # Determine fallback models (simplified logic)
      def determine_fallbacks(_model_id, _model_data)
        # For now, return empty array - could be enhanced with smart fallback logic
        []
      end

      # Find the best model matching given requirements
      def find_best_model(requirements = {})
        candidates = models_meeting_requirements(requirements)
        return nil if candidates.empty?

        # If pick_newer is true, prefer newer models over cost
        if requirements[:pick_newer]
          candidates.max_by { |_, specs| specs[:created_at] }
        else
          # Sort by cost (cheapest first) as default strategy
          candidates.min_by { |_, specs| calculate_model_cost(specs, requirements) }
        end
      end

      # Get all models that meet requirements (without sorting)
      def models_meeting_requirements(requirements = {})
        all_models.select do |_model, specs|
          meets_requirements?(specs, requirements)
        end
      end

      # Get fallback models for a given model
      def get_fallbacks(model)
        model_info = get_model_info(model)
        model_info ? model_info[:fallbacks] || [] : []
      end

      # Check if a model exists in the registry
      def model_exists?(model)
        all_models.key?(model)
      end

      # Check if a model has a specific capability
      def has_capability?(model, capability)
        model_info = get_model_info(model)
        return false unless model_info
        
        model_info[:capabilities].include?(capability)
      end

      # Get detailed information about a model
      def get_model_info(model)
        all_models[model]
      end

      # Get all registered models (fetch from API if needed)
      def all_models
        @all_models ||= fetch_and_cache_models
      end

      # Calculate estimated cost for a request
      def calculate_estimated_cost(model, input_tokens: 0, output_tokens: 0)
        model_info = get_model_info(model)
        return 0 unless model_info

        input_cost = (input_tokens / 1000.0) * model_info[:cost_per_1k_tokens][:input]
        output_cost = (output_tokens / 1000.0) * model_info[:cost_per_1k_tokens][:output]

        input_cost + output_cost
      end

      private

      # Check if model specs meet the given requirements
      def meets_requirements?(specs, requirements)
        # Check capability requirements
        if requirements[:capabilities]
          required_caps = Array(requirements[:capabilities])
          return false unless required_caps.all? { |cap| specs[:capabilities].include?(cap) }
        end

        # Check cost requirements
        return false if requirements[:max_input_cost] && (specs[:cost_per_1k_tokens][:input] > requirements[:max_input_cost])

        return false if requirements[:max_output_cost] && (specs[:cost_per_1k_tokens][:output] > requirements[:max_output_cost])

        # Check context length requirements
        return false if requirements[:min_context_length] && (specs[:context_length] < requirements[:min_context_length])

        # Check performance tier requirements
        if requirements[:performance_tier]
          required_tier = requirements[:performance_tier]
          model_tier = specs[:performance_tier]

          # Premium tier can satisfy premium or standard requirements
          # Standard tier can only satisfy standard requirements
          case required_tier
          when :premium
            return false unless model_tier == :premium
          when :standard
            return false unless %i[standard premium].include?(model_tier)
          end
        end

        # Check released after date requirement
        if requirements[:released_after_date]
          required_date = requirements[:released_after_date]
          model_timestamp = specs[:created_at]

          # Convert date to timestamp if needed
          required_timestamp = case required_date
                               when Date
                                 required_date.to_time.to_i
                               when Time
                                 required_date.to_i
                               when Integer
                                 required_date
                               else
                                 return false
                               end

          return false if model_timestamp < required_timestamp
        end

        true
      end

      # Calculate the cost metric for sorting models
      def calculate_model_cost(specs, _requirements)
        # Simple cost calculation for sorting - could be made more sophisticated
        # For now, just use input token cost as the primary metric
        specs[:cost_per_1k_tokens][:input]
      end
    end
  end
end

================
File: lib/open_router/model_selector.rb
================
# frozen_string_literal: true

module OpenRouter
  class ModelSelectionError < Error; end

  # ModelSelector provides a fluent DSL interface for selecting the best AI model
  # based on specific requirements. It wraps the ModelRegistry functionality
  # with an intuitive, chainable API.
  #
  # @example Basic usage
  #   selector = OpenRouter::ModelSelector.new
  #   model = selector.optimize_for(:cost)
  #                   .require(:function_calling, :vision)
  #                   .within_budget(max_cost: 0.01)
  #                   .choose
  #
  # @example With provider preferences
  #   model = OpenRouter::ModelSelector.new
  #                                   .prefer_providers("anthropic", "openai")
  #                                   .require(:function_calling)
  #                                   .choose
  #
  # @example With fallback selection
  #   models = OpenRouter::ModelSelector.new
  #                                     .optimize_for(:performance)
  #                                     .choose_with_fallbacks(limit: 3)
  class ModelSelector
    # Available optimization strategies
    STRATEGIES = {
      cost: { sort_by: :cost, pick_newer: false },
      performance: { sort_by: :performance, pick_newer: false },
      latest: { sort_by: :date, pick_newer: true },
      context: { sort_by: :context_length, pick_newer: false }
    }.freeze

    def initialize(requirements: {}, strategy: :cost, provider_preferences: {}, fallback_options: {})
      @requirements = requirements.dup
      @strategy = strategy
      @provider_preferences = provider_preferences.dup
      @fallback_options = fallback_options.dup
    end

    # Set the optimization strategy for model selection
    #
    # @param strategy [Symbol] The optimization strategy (:cost, :performance, :latest, :context)
    # @return [ModelSelector] Returns self for method chaining
    # @raise [ArgumentError] If strategy is not supported
    #
    # @example
    #   selector.optimize_for(:cost)      # Choose cheapest model
    #   selector.optimize_for(:performance) # Choose highest performance tier
    #   selector.optimize_for(:latest)    # Choose newest model
    #   selector.optimize_for(:context)   # Choose model with largest context window
    def optimize_for(strategy)
      raise ArgumentError, "Unknown strategy: #{strategy}. Available: #{STRATEGIES.keys.join(", ")}" unless STRATEGIES.key?(strategy)

      new_requirements = @requirements.dup

      # Apply strategy-specific requirements
      case strategy
      when :performance
        new_requirements[:performance_tier] = :premium
      when :latest
        new_requirements[:pick_newer] = true
      end

      self.class.new(
        requirements: new_requirements,
        strategy:,
        provider_preferences: @provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Require specific capabilities from the selected model
    #
    # @param capabilities [Array<Symbol>] Required capabilities
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.require(:function_calling)
    #   selector.require(:function_calling, :vision, :structured_outputs)
    def require(*capabilities)
      new_requirements = @requirements.dup
      new_requirements[:capabilities] = Array(new_requirements[:capabilities]) + capabilities
      new_requirements[:capabilities].uniq!

      self.class.new(
        requirements: new_requirements,
        strategy: @strategy,
        provider_preferences: @provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Set budget constraints for model selection
    #
    # @param max_cost [Float] Maximum cost per 1k input tokens
    # @param max_output_cost [Float] Maximum cost per 1k output tokens (optional)
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.within_budget(max_cost: 0.01)
    #   selector.within_budget(max_cost: 0.01, max_output_cost: 0.02)
    def within_budget(max_cost: nil, max_output_cost: nil)
      new_requirements = @requirements.dup
      new_requirements[:max_input_cost] = max_cost if max_cost
      new_requirements[:max_output_cost] = max_output_cost if max_output_cost

      self.class.new(
        requirements: new_requirements,
        strategy: @strategy,
        provider_preferences: @provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Set minimum context length requirement
    #
    # @param tokens [Integer] Minimum context length in tokens
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.min_context(100_000)  # Require at least 100k context
    def min_context(tokens)
      new_requirements = @requirements.dup
      new_requirements[:min_context_length] = tokens

      self.class.new(
        requirements: new_requirements,
        strategy: @strategy,
        provider_preferences: @provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Require models released after a specific date
    #
    # @param date [Date, Time, Integer] Cutoff date (Date/Time object or Unix timestamp)
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.newer_than(Date.new(2024, 1, 1))
    #   selector.newer_than(Time.now - 30.days)
    def newer_than(date)
      new_requirements = @requirements.dup
      new_requirements[:released_after_date] = date

      self.class.new(
        requirements: new_requirements,
        strategy: @strategy,
        provider_preferences: @provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Set provider preferences (soft preference - won't exclude other providers)
    #
    # @param providers [Array<String>] Preferred provider names in order of preference
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.prefer_providers("anthropic", "openai")
    def prefer_providers(*providers)
      new_provider_preferences = @provider_preferences.dup
      new_provider_preferences[:preferred] = providers.flatten

      self.class.new(
        requirements: @requirements,
        strategy: @strategy,
        provider_preferences: new_provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Require specific providers (hard filter - only these providers)
    #
    # @param providers [Array<String>] Required provider names
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.require_providers("anthropic")
    def require_providers(*providers)
      new_provider_preferences = @provider_preferences.dup
      new_provider_preferences[:required] = providers.flatten

      self.class.new(
        requirements: @requirements,
        strategy: @strategy,
        provider_preferences: new_provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Avoid specific providers (blacklist)
    #
    # @param providers [Array<String>] Provider names to avoid
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.avoid_providers("google")
    def avoid_providers(*providers)
      new_provider_preferences = @provider_preferences.dup
      new_provider_preferences[:avoided] = providers.flatten

      self.class.new(
        requirements: @requirements,
        strategy: @strategy,
        provider_preferences: new_provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Avoid models matching specific patterns
    #
    # @param patterns [Array<String>] Glob patterns to avoid
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.avoid_patterns("*-free", "*-preview")
    def avoid_patterns(*patterns)
      new_provider_preferences = @provider_preferences.dup
      new_provider_preferences[:avoided_patterns] = patterns.flatten

      self.class.new(
        requirements: @requirements,
        strategy: @strategy,
        provider_preferences: new_provider_preferences,
        fallback_options: @fallback_options
      )
    end

    # Configure fallback behavior
    #
    # @param max_fallbacks [Integer] Maximum number of fallback models to include
    # @param strategy [Symbol] Fallback strategy (:similar, :cheaper, :different_provider)
    # @return [ModelSelector] Returns self for method chaining
    #
    # @example
    #   selector.with_fallbacks(max: 3, strategy: :similar)
    def with_fallbacks(max: 3, strategy: :similar)
      new_fallback_options = { max_fallbacks: max, strategy: }

      self.class.new(
        requirements: @requirements,
        strategy: @strategy,
        provider_preferences: @provider_preferences,
        fallback_options: new_fallback_options
      )
    end

    # Select the best model based on configured requirements
    #
    # @param return_specs [Boolean] Whether to return model specs along with model ID
    # @return [String, Array] Model ID or [model_id, specs] tuple if return_specs is true
    # @return [nil] If no models match requirements
    #
    # @example
    #   model = selector.choose
    #   model, specs = selector.choose(return_specs: true)
    def choose(return_specs: false)
      # Get all models that meet basic requirements
      candidates = filter_by_providers(ModelRegistry.models_meeting_requirements(@requirements))

      return nil if candidates.empty?

      # Apply strategy-specific sorting
      best_match = apply_strategy_sorting(candidates)

      return nil unless best_match

      return_specs ? best_match : best_match.first
    end

    # Select the best model with fallback options
    #
    # @param limit [Integer] Maximum number of models to return (including primary choice)
    # @return [Array<String>] Array of model IDs in order of preference
    # @return [Array] Empty array if no models match requirements
    #
    # @example
    #   models = selector.choose_with_fallbacks(limit: 3)
    #   # => ["gpt-4", "claude-3-opus", "gpt-3.5-turbo"]
    def choose_with_fallbacks(limit: 3)
      candidates = filter_by_providers(ModelRegistry.models_meeting_requirements(@requirements))

      return [] if candidates.empty?

      # Apply strategy-specific sorting to get ordered list
      sorted_candidates = apply_strategy_sorting_all(candidates)

      # Return up to `limit` models
      sorted_candidates.first(limit).map(&:first)
    end

    # Choose with graceful degradation if no models meet all requirements
    #
    # @return [String, nil] Model ID or nil if no models available at all
    #
    # @example
    #   model = selector.choose_with_fallback
    def choose_with_fallback
      # Try with all requirements first
      result = choose
      return result if result

      # Try dropping least important requirements progressively
      fallback_requirements = @requirements.dup

      # Drop requirements in order of importance (least to most important)
      %i[
        released_after_date
        performance_tier
        max_output_cost
        min_context_length
        max_input_cost
      ].each do |requirement|
        next unless fallback_requirements.key?(requirement)

        fallback_requirements.delete(requirement)
        candidates = filter_by_providers(ModelRegistry.models_meeting_requirements(fallback_requirements))

        unless candidates.empty?
          result = apply_strategy_sorting(candidates)
          return result&.first
        end
      end

      # Last resort: just pick any model that meets capability requirements
      if fallback_requirements[:capabilities]
        basic_requirements = { capabilities: fallback_requirements[:capabilities] }
        candidates = filter_by_providers(ModelRegistry.models_meeting_requirements(basic_requirements))
        result = apply_strategy_sorting(candidates) unless candidates.empty?
        return result&.first if result
      end

      # Final fallback: cheapest available model
      all_candidates = filter_by_providers(ModelRegistry.all_models)
      return nil if all_candidates.empty?

      all_candidates.min_by { |_, specs| specs[:cost_per_1k_tokens][:input] }&.first
    end

    # Get detailed information about the current selection criteria
    #
    # @return [Hash] Hash containing requirements, strategy, and provider preferences
    def selection_criteria
      {
        requirements: deep_dup(@requirements),
        strategy: @strategy,
        provider_preferences: deep_dup(@provider_preferences),
        fallback_options: deep_dup(@fallback_options)
      }
    end

    # Estimate cost for a given model with expected token usage
    #
    # @param model [String] Model ID
    # @param input_tokens [Integer] Expected input tokens
    # @param output_tokens [Integer] Expected output tokens
    # @return [Float] Estimated cost in dollars
    def estimate_cost(model, input_tokens: 1000, output_tokens: 1000)
      ModelRegistry.calculate_estimated_cost(model, input_tokens:, output_tokens:)
    end

    private

    # Deep duplicate a hash or array to avoid shared references
    def deep_dup(obj)
      case obj
      when Hash
        obj.transform_values { |v| deep_dup(v) }
      when Array
        obj.map { |item| deep_dup(item) }
      else
        obj
      end
    end

    # Filter candidates by provider preferences
    def filter_by_providers(candidates)
      return candidates if @provider_preferences.empty?

      filtered = candidates.dup

      # Apply required providers filter (hard requirement)
      if @provider_preferences[:required]
        required_providers = @provider_preferences[:required]
        filtered = filtered.select do |model_id, _|
          provider = extract_provider_from_model_id(model_id)
          required_providers.include?(provider)
        end
      end

      # Apply avoided providers filter
      if @provider_preferences[:avoided]
        avoided_providers = @provider_preferences[:avoided]
        filtered = filtered.reject do |model_id, _|
          provider = extract_provider_from_model_id(model_id)
          avoided_providers.include?(provider)
        end
      end

      # Apply avoided patterns filter
      if @provider_preferences[:avoided_patterns]
        patterns = @provider_preferences[:avoided_patterns]
        filtered = filtered.reject do |model_id, _|
          patterns.any? { |pattern| File.fnmatch(pattern, model_id) }
        end
      end

      filtered
    end

    # Extract provider name from model ID (e.g., "anthropic/claude-3" -> "anthropic")
    def extract_provider_from_model_id(model_id)
      model_id.split("/").first
    end

    # Apply strategy-specific sorting and return best match
    def apply_strategy_sorting(candidates)
      case @strategy
      when :cost
        candidates.min_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
      when :performance
        # Prefer premium tier, then by cost within tier
        candidates.min_by do |_, specs|
          [specs[:performance_tier] == :premium ? 0 : 1, specs[:cost_per_1k_tokens][:input]]
        end
      when :latest
        candidates.max_by { |_, specs| specs[:created_at] }
      when :context
        candidates.max_by { |_, specs| specs[:context_length] }
      else
        candidates.min_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
      end
    end

    # Apply strategy-specific sorting and return all sorted candidates
    def apply_strategy_sorting_all(candidates)
      case @strategy
      when :cost
        candidates.sort_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
      when :performance
        candidates.sort_by do |_, specs|
          [specs[:performance_tier] == :premium ? 0 : 1, specs[:cost_per_1k_tokens][:input]]
        end
      when :latest
        candidates.sort_by { |_, specs| -specs[:created_at] }
      when :context
        candidates.sort_by { |_, specs| -specs[:context_length] }
      else
        candidates.sort_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
      end
    end
  end
end

================
File: lib/open_router/response.rb
================
# frozen_string_literal: true

require "json"

module OpenRouter
  class StructuredOutputError < Error; end

  class Response
    attr_reader :raw_response, :response_format
    attr_accessor :client

    def initialize(raw_response, response_format: nil, forced_extraction: false)
      @raw_response = raw_response.is_a?(Hash) ? raw_response.with_indifferent_access : {}
      @response_format = response_format
      @forced_extraction = forced_extraction
      @client = nil
    end

    # Delegate common hash methods to raw_response for backward compatibility
    def [](key)
      @raw_response[key]
    end

    def dig(*keys)
      @raw_response.dig(*keys)
    end

    def fetch(key, default = nil)
      @raw_response.fetch(key, default)
    end

    def key?(key)
      @raw_response.key?(key)
    end

    def to_h
      @raw_response.to_h
    end

    def to_json(*args)
      @raw_response.to_json(*args)
    end

    # Tool calling methods
    def tool_calls
      @tool_calls ||= parse_tool_calls
    end

    def has_tool_calls?
      !tool_calls.empty?
    end

    # Convert response to message format for conversation continuation
    def to_message
      if has_tool_calls?
        {
          role: "assistant",
          content:,
          tool_calls: raw_tool_calls
        }
      else
        {
          role: "assistant",
          content:
        }
      end
    end

    # Structured output methods
    def structured_output(mode: :strict, auto_heal: nil)
      # Validate mode parameter
      unless [:strict, :gentle].include?(mode)
        raise ArgumentError, "Invalid mode: #{mode}. Must be :strict or :gentle."
      end
      
      return nil unless structured_output_expected? && has_content?

      case mode
      when :strict
        # The existing logic for strict parsing and healing
        should_heal = auto_heal.nil? ?
          (@client && @client.configuration.auto_heal_responses) :
          auto_heal

        result = parse_and_heal_structured_output(auto_heal: should_heal)
        
        # Only validate after parsing if healing is disabled (healing handles its own validation)
        if result && !should_heal
          schema_obj = extract_schema_from_response_format
          if schema_obj && !schema_obj.validate(result)
            validation_errors = schema_obj.validation_errors(result)
            raise StructuredOutputError, "Schema validation failed: #{validation_errors.join(', ')}"
          end
        end
        
        @structured_output ||= result
      when :gentle
        # New gentle mode: best-effort parsing, no healing, no validation
        content_to_parse = @forced_extraction ? extract_json_from_text(content) : content
        return nil if content_to_parse.nil?
        
        begin
          JSON.parse(content_to_parse)
        rescue JSON::ParserError
          nil # Return nil on failure instead of raising an error
        end
      end
    end

    def valid_structured_output?
      return true unless structured_output_expected?

      schema_obj = extract_schema_from_response_format
      return true unless schema_obj

      schema_obj.validate(structured_output)
    end

    def validation_errors
      return [] unless structured_output_expected?

      schema_obj = extract_schema_from_response_format
      return [] unless schema_obj

      schema_obj.validation_errors(structured_output)
    end

    # Content accessors
    def content
      choices.first&.dig("message", "content")
    end

    def choices
      @raw_response["choices"] || []
    end

    def usage
      @raw_response["usage"]
    end

    def id
      @raw_response["id"]
    end

    def model
      @raw_response["model"]
    end

    def created
      @raw_response["created"]
    end

    def object
      @raw_response["object"]
    end

    # Convenience method to check if response has content
    def has_content?
      !content.nil? && !content.empty?
    end

    # Convenience method to check if response indicates an error
    def error?
      @raw_response.key?("error")
    end

    def error_message
      @raw_response.dig("error", "message")
    end

    private

    def parse_tool_calls
      tool_calls_data = choices.first&.dig("message", "tool_calls")
      return [] unless tool_calls_data.is_a?(Array)

      tool_calls_data.map { |tc| ToolCall.new(tc) }
    rescue StandardError => e
      raise ToolCallError, "Failed to parse tool calls: #{e.message}"
    end

    def raw_tool_calls
      choices.first&.dig("message", "tool_calls") || []
    end

    def parse_and_heal_structured_output(auto_heal: false)
      return nil unless structured_output_expected?
      return nil unless has_content?

      content_to_parse = @forced_extraction ? extract_json_from_text(content) : content
      
      if auto_heal && @client
        # For forced extraction: always send full content to provide context for healing
        # For normal responses: send the content as-is
        if @forced_extraction
          healing_content = content  # Always send full response for better healing context
        else
          healing_content = content_to_parse || content
        end
        heal_structured_response(healing_content, extract_schema_from_response_format)
      else
        return nil if content_to_parse.nil? # No JSON found in forced extraction
        
        begin
          JSON.parse(content_to_parse)
        rescue JSON::ParserError => e
          # For forced extraction, be more lenient and return nil on parse failures
          # For regular structured outputs, raise errors to indicate problems
          if @forced_extraction
            nil
          else
            raise StructuredOutputError, "Failed to parse structured output: #{e.message}"
          end
        end
      end
    end

    # Extract JSON from text content (for forced structured output)
    def extract_json_from_text(text)
      return nil if text.nil? || text.empty?
      
      # First try to find JSON in code blocks
      if text.include?("```")
        # Look for ```json or ``` blocks
        json_match = text.match(/```(?:json)?\s*\n?(.*?)\n?```/m)
        if json_match
          candidate = json_match[1].strip
          return candidate unless candidate.empty?
        end
      end
      
      # Try to parse the entire text as JSON
      begin
        JSON.parse(text)
        return text
      rescue JSON::ParserError
        # Look for JSON-like content (starts with { or [)
        json_match = text.match(/(\{.*\}|\[.*\])/m)
        return json_match[1] if json_match
      end
      
      # No JSON found
      nil
    end

    def structured_output_expected?
      return false unless @response_format

      if @response_format.is_a?(Schema)
        true
      elsif @response_format.is_a?(Hash) && @response_format[:type] == "json_schema"
        true
      else
        false
      end
    end


    def extract_schema_from_response_format
      case @response_format
      when Schema
        @response_format
      when Hash
        schema_def = @response_format[:json_schema]
        if schema_def.is_a?(Schema)
          schema_def
        elsif schema_def.is_a?(Hash) && schema_def[:schema]
          # Create a temporary schema object for validation
          Schema.new(
            schema_def[:name] || "response",
            schema_def[:schema],
            strict: schema_def[:strict] || true
          )
        end
      end
    end

    # Healing methods
    def heal_structured_response(content, schema)
      max_attempts = @client.configuration.max_heal_attempts
      healer_model = @client.configuration.healer_model
      
      attempts = 0
      current_content = content

      loop do
        begin
          json = JSON.parse(current_content)
          
          # If we have a schema, validate it
          if schema&.respond_to?(:validate)
            if schema.validate(json)
              return json
            else
              # Schema validation failed - get detailed errors
              validation_errors = schema.validation_errors(json)
              if attempts >= max_attempts
                error_details = validation_errors.any? ? validation_errors.join(', ') : 'Schema validation failed'
                raise StructuredOutputError, "Failed to pass schema validation after #{max_attempts} healing attempts. Last errors: #{error_details}"
              end
              attempts += 1
              error_reason = "Schema validation failed with errors: #{validation_errors.join('; ')}"
              current_content = fix_with_healer_model(current_content, schema, healer_model, error_reason)
            end
          else
            # No schema validation, just return parsed JSON
            return json
          end
        rescue JSON::ParserError => e
          # JSON parsing failed
          if attempts >= max_attempts
            # We have no attempts left. The last heal (if any) failed.
            raise StructuredOutputError, "Failed to parse structured output after #{max_attempts} healing attempts: #{e.message}"
          end
          
          # We have attempts remaining. Increment the counter and try to heal.
          attempts += 1
          current_content = fix_with_healer_model(current_content, schema, healer_model, "Invalid JSON: #{e.message}")
        end
      end
    end

    def fix_with_healer_model(content, schema, healer_model, error_reason)
      if schema
        fix_prompt = build_schema_healing_prompt(content, schema, error_reason)
      else
        fix_prompt = build_json_healing_prompt(content, error_reason)
      end

      begin
        healing_response = @client.complete(
          [{ role: "user", content: fix_prompt }],
          model: healer_model,
          extras: { max_tokens: 2000, temperature: 0 }
        )
        
        healing_response.content
      rescue => e
        # If healing itself fails, return original content and let it fail naturally
        content
      end
    end

    def build_json_healing_prompt(content, error_reason)
      <<~PROMPT
        The following content has a JSON parsing error: #{error_reason}

        Content to fix:
        #{content}

        Please fix this content to be valid JSON. Return ONLY the fixed JSON, no explanations or additional text.
      PROMPT
    end

    def build_schema_healing_prompt(content, schema, error_reason)
      schema_json = schema.respond_to?(:to_h) ? schema.to_h.to_json : schema.to_json
      
      # Detect if this looks like a forced extraction case (contains explanation text)
      is_forced_extraction = @forced_extraction && (content.include?('```') || content.length > 200 || content.include?("\n"))
      
      if is_forced_extraction
        <<~PROMPT
          The following response contains explanatory text and JSON that needs to be extracted and fixed to conform to the provided schema.
          
          Validation Errors:
          #{error_reason}

          Original Response Content:
          #{content}

          Required JSON Schema:
          ```json
          #{schema_json}
          ```

          Please extract and correct the JSON from the response above to produce a valid JSON object that strictly conforms to the schema.
          Return ONLY the fixed, raw JSON object, without any surrounding text or explanations.
        PROMPT
      else
        <<~PROMPT
          The following JSON content is invalid because it failed to validate against the provided JSON Schema.
          
          Validation Errors:
          #{error_reason}

          Original Content to Fix:
          ```json
          #{content}
          ```

          Required JSON Schema:
          ```json
          #{schema_json}
          ```

          Please correct the content to produce a valid JSON object that strictly conforms to the schema.
          Return ONLY the fixed, raw JSON object, without any surrounding text or explanations.
        PROMPT
      end
    end
  end
end

================
File: lib/open_router/schema.rb
================
# frozen_string_literal: true

require "json-schema"

module OpenRouter
  class SchemaValidationError < Error; end

  class Schema
    attr_reader :name, :strict, :schema

    def initialize(name, schema_definition = {}, strict: true)
      @name = name
      @strict = strict
      raise ArgumentError, "Schema definition must be a hash" unless schema_definition.is_a?(Hash)

      @schema = schema_definition
      validate_schema!
    end

    # Class method for defining schemas with a DSL
    def self.define(name, strict: true, &block)
      builder = SchemaBuilder.new
      builder.instance_eval(&block) if block_given?
      new(name, builder.to_h, strict:)
    end

    # Convert to the format expected by OpenRouter API
    def to_h
      # Apply OpenRouter-specific transformations
      openrouter_schema = @schema.dup
      
      # OpenRouter/Azure requires ALL properties to be in the required array
      # even if they are logically optional. This is a deviation from JSON Schema spec
      # but necessary for compatibility.
      if openrouter_schema[:properties] && openrouter_schema[:properties].any?
        all_properties = openrouter_schema[:properties].keys.map(&:to_s)
        openrouter_schema[:required] = all_properties
      end

      {
        name: @name,
        strict: @strict,
        schema: openrouter_schema
      }
    end

    # Get the pure JSON Schema (respects required flags) for testing/validation  
    def pure_schema
      @schema
    end

    def to_json(*args)
      to_h.to_json(*args)
    end

    # Validate data against this schema
    def validate(data)
      JSON::Validator.validate(@schema, data)
    end

    # Get validation errors for data
    def validation_errors(data)
      JSON::Validator.fully_validate(@schema, data)
    end

    # Generate format instructions for model prompting
    def get_format_instructions(forced: false)
      schema_json = to_h.to_json
      
      if forced
        <<~INSTRUCTIONS
          You must format your output as a JSON value that conforms exactly to the following JSON Schema specification:

          #{schema_json}

          CRITICAL: Your entire response must be valid JSON that matches this schema. Do not include any text before or after the JSON. Return ONLY the JSON value itself - no other text, explanations, or formatting.

          example format:
          ```json
          {"field1": "value1", "field2": "value2"}
          ```

          Important guidelines:
          - Ensure all required fields match the schema exactly
          - Use proper JSON formatting (no trailing commas)
          - All string values must be properly quoted
        INSTRUCTIONS
      else
        <<~INSTRUCTIONS
          Please format your output as a JSON value that conforms to the following JSON Schema specification:

          #{schema_json}

          Your response should be valid JSON that matches this schema structure exactly.

          example format:
          ```json
          {"field1": "value1", "field2": "value2"}
          ```

          Important guidelines:
          - Ensure all required fields match the schema
          - Use proper JSON formatting (no trailing commas)  
          - Return ONLY the JSON - no other text or explanations
        INSTRUCTIONS
      end
    end

    private

    def validate_schema!
      raise ArgumentError, "Schema name is required" if @name.nil? || @name.empty?
      raise ArgumentError, "Schema must be a hash" unless @schema.is_a?(Hash)
    end

    # Internal class for building schemas with DSL
    class SchemaBuilder
      def initialize
        @schema = {
          type: "object",
          properties: {},
          required: []
        }
        @strict_mode = true
        # Set additionalProperties to false by default in strict mode
        @schema[:additionalProperties] = false
      end

      def strict(value = true)
        @strict_mode = value
        additional_properties(!value) if value
      end

      def additional_properties(allowed = true)
        @schema[:additionalProperties] = allowed
      end

      def no_additional_properties
        additional_properties(false)
      end

      def property(name, type, required: false, description: nil, **options)
        prop_def = { type: type.to_s }
        prop_def[:description] = description if description
        prop_def.merge!(options)

        @schema[:properties][name] = prop_def
        mark_required(name) if required
      end

      def string(name, required: false, description: nil, **options)
        property(name, :string, required:, description:, **options)
      end

      def integer(name, required: false, description: nil, **options)
        property(name, :integer, required:, description:, **options)
      end

      def number(name, required: false, description: nil, **options)
        property(name, :number, required:, description:, **options)
      end

      def boolean(name, required: false, description: nil, **options)
        property(name, :boolean, required:, description:, **options)
      end

      def array(name, required: false, description: nil, items: nil, &block)
        array_def = { type: "array" }
        array_def[:description] = description if description

        if items
          array_def[:items] = items
        elsif block_given?
          items_builder = ItemsBuilder.new
          items_builder.instance_eval(&block)
          array_def[:items] = items_builder.to_h
        end

        @schema[:properties][name] = array_def
        mark_required(name) if required
      end

      def object(name, required: false, description: nil, &block)
        object_def = {
          type: "object",
          properties: {},
          required: []
        }
        object_def[:description] = description if description

        if block_given?
          object_builder = SchemaBuilder.new
          object_builder.instance_eval(&block)
          nested_schema = object_builder.to_h
          object_def[:properties] = nested_schema[:properties]
          object_def[:required] = nested_schema[:required]
          object_def[:additionalProperties] = nested_schema[:additionalProperties] if nested_schema.key?(:additionalProperties)
        end

        @schema[:properties][name] = object_def
        mark_required(name) if required
      end

      def required(*field_names)
        field_names.each { |name| mark_required(name) }
      end

      def to_h
        @schema.dup
      end

      private

      def mark_required(name)
        # Convert to string to match OpenRouter API expectations
        string_name = name.to_s
        @schema[:required] << string_name unless @schema[:required].include?(string_name)
      end
    end

    # Internal class for building array items
    class ItemsBuilder
      def initialize
        @items = {}
      end

      def string(description: nil, **options)
        @items = { type: "string" }
        @items[:description] = description if description
        @items.merge!(options)
      end

      def integer(description: nil, **options)
        @items = { type: "integer" }
        @items[:description] = description if description
        @items.merge!(options)
      end

      def number(description: nil, **options)
        @items = { type: "number" }
        @items[:description] = description if description
        @items.merge!(options)
      end

      def boolean(description: nil, **options)
        @items = { type: "boolean" }
        @items[:description] = description if description
        @items.merge!(options)
      end

      def object(&block)
        @items = { type: "object", properties: {}, required: [], additionalProperties: false }

        return unless block_given?

          object_builder = SchemaBuilder.new
          object_builder.instance_eval(&block)
          nested_schema = object_builder.to_h
          @items[:properties] = nested_schema[:properties]
          @items[:required] = nested_schema[:required]
          @items[:additionalProperties] = nested_schema[:additionalProperties] if nested_schema.key?(:additionalProperties)
      end

      def to_h
        @items
      end
    end
  end
end

================
File: lib/open_router/tool_call.rb
================
# frozen_string_literal: true

require "json"

module OpenRouter
  class ToolCallError < Error; end

  class ToolCall
    attr_reader :id, :type, :function_name, :arguments_string

    def initialize(tool_call_data)
      @id = tool_call_data["id"]
      @type = tool_call_data["type"]

      raise ToolCallError, "Invalid tool call data: missing function" unless tool_call_data["function"]

        @function_name = tool_call_data["function"]["name"]
        @arguments_string = tool_call_data["function"]["arguments"]
    end

    # Parse the arguments JSON string into a Ruby hash
    def arguments
      @arguments ||= begin
        JSON.parse(@arguments_string)
      rescue JSON::ParserError => e
        raise ToolCallError, "Failed to parse tool call arguments: #{e.message}"
      end
    end

    # Get the function name (alias for consistency)
    def name
      @function_name
    end

    # Execute the tool call with a provided block
    # The block should accept (name, arguments) and return the result
    def execute(&block)
      raise ArgumentError, "Block required for tool execution" unless block_given?

      result = block.call(@function_name, arguments)
      ToolResult.new(self, result)
    end

    # Convert this tool call to a message format for conversation continuation
    def to_message
      {
        role: "assistant",
        content: nil,
        tool_calls: [
          {
            id: @id,
            type: @type,
            function: {
              name: @function_name,
              arguments: @arguments_string
            }
          }
        ]
      }
    end

    # Convert a tool result to a tool message for the conversation
    def to_result_message(result)
      {
        role: "tool",
        tool_call_id: @id,
        name: @function_name,
        content: result.is_a?(String) ? result : result.to_json
      }
    end

    def to_h
      {
        id: @id,
        type: @type,
        function: {
          name: @function_name,
          arguments: @arguments_string
        }
      }
    end

    def to_json(*args)
      to_h.to_json(*args)
    end
  end

  # Represents the result of executing a tool call
  class ToolResult
    attr_reader :tool_call, :result, :error

    def initialize(tool_call, result = nil, error = nil)
      @tool_call = tool_call
      @result = result
      @error = error
    end

    def success?
      @error.nil?
    end

    def failure?
      !success?
    end

    # Convert to message format for conversation continuation
    def to_message
      @tool_call.to_result_message(@error || @result)
    end

    # Create a failed result
    def self.failure(tool_call, error)
      new(tool_call, nil, error)
    end

    # Create a successful result
    def self.success(tool_call, result)
      new(tool_call, result, nil)
    end
  end
end

================
File: lib/open_router/tool.rb
================
# frozen_string_literal: true

module OpenRouter
  class Tool
    attr_reader :type, :function

    def initialize(definition = {})
      if definition.is_a?(Hash) && definition.key?(:function)
        @type = definition[:type] || "function"
        @function = definition[:function]
      elsif definition.is_a?(Hash)
        @type = "function"
        @function = definition
      else
        raise ArgumentError, "Tool definition must be a hash"
      end

      validate_definition!
    end

    # Class method for defining tools with a DSL
    def self.define(&block)
      builder = ToolBuilder.new
      builder.instance_eval(&block) if block_given?
      new(builder.to_h)
    end

    def to_h
      {
        type: @type,
        function: @function
      }
    end

    def to_json(*args)
      to_h.to_json(*args)
    end

    def name
      @function[:name]
    end

    def description
      @function[:description]
    end

    def parameters
      @function[:parameters]
    end

    private

    def validate_definition!
      raise ArgumentError, "Function must have a name" unless @function[:name]
      raise ArgumentError, "Function must have a description" unless @function[:description]
      raise ArgumentError, "Function parameters must be an object" if @function[:parameters] && @function[:parameters][:type] != "object"
    end

    # Internal class for building tool definitions with DSL
    class ToolBuilder
      def initialize
        @definition = {
          name: nil,
          description: nil,
          parameters: {
            type: "object",
            properties: {},
            required: []
          }
        }
      end

      def name(value)
        @definition[:name] = value
      end

      def description(value)
        @definition[:description] = value
      end

      def parameters(&block)
        param_builder = ParametersBuilder.new(@definition[:parameters])
        param_builder.instance_eval(&block) if block_given?
      end

      def to_h
        @definition
      end
    end

    # Internal class for building parameter schemas
    class ParametersBuilder
      def initialize(params_hash)
        @params = params_hash
      end

      def string(name, required: false, description: nil, **options)
        add_property(name, { type: "string", description: }.merge(options).compact)
        mark_required(name) if required
      end

      def integer(name, required: false, description: nil, **options)
        add_property(name, { type: "integer", description: }.merge(options).compact)
        mark_required(name) if required
      end

      def number(name, required: false, description: nil, **options)
        add_property(name, { type: "number", description: }.merge(options).compact)
        mark_required(name) if required
      end

      def boolean(name, required: false, description: nil, **options)
        add_property(name, { type: "boolean", description: }.merge(options).compact)
        mark_required(name) if required
      end

      def array(name, required: false, description: nil, &block)
        array_def = { type: "array", description: }.compact

        if block_given?
          items_builder = ItemsBuilder.new
          items_builder.instance_eval(&block)
          array_def[:items] = items_builder.to_h
        end

        add_property(name, array_def)
        mark_required(name) if required
      end

      def object(name, required: false, description: nil, &block)
        object_def = {
          type: "object",
          description:,
          properties: {},
          required: []
        }.compact

        if block_given?
          object_builder = ParametersBuilder.new(object_def)
          object_builder.instance_eval(&block)
        end

        add_property(name, object_def)
        mark_required(name) if required
      end

      private

      def add_property(name, definition)
        @params[:properties][name] = definition
      end

      def mark_required(name)
        @params[:required] << name unless @params[:required].include?(name)
      end
    end

    # Internal class for building array items
    class ItemsBuilder
      def initialize
        @items = {}
      end

      def string(description: nil, **options)
        @items = { type: "string", description: }.merge(options).compact
      end

      def integer(description: nil, **options)
        @items = { type: "integer", description: }.merge(options).compact
      end

      def number(description: nil, **options)
        @items = { type: "number", description: }.merge(options).compact
      end

      def boolean(description: nil, **options)
        @items = { type: "boolean", description: }.merge(options).compact
      end

      def to_h
        @items
      end
    end
  end
end

================
File: lib/open_router/version.rb
================
# frozen_string_literal: true

module OpenRouter
  VERSION = "0.3.3"
end

================
File: lib/open_router.rb
================
# frozen_string_literal: true

require "faraday"
require "faraday/multipart"

module OpenRouter
  class Error < StandardError; end
  class ConfigurationError < Error; end
  class CapabilityError < Error; end
end

require_relative "open_router/http"
require_relative "open_router/tool"
require_relative "open_router/tool_call"
require_relative "open_router/schema"
require_relative "open_router/response"
require_relative "open_router/model_registry"
require_relative "open_router/model_selector"
require_relative "open_router/client"
require_relative "open_router/version"

module OpenRouter
  class Configuration
    attr_writer :access_token
    attr_accessor :api_version, :extra_headers, :faraday_config, :log_errors, :request_timeout, :uri_base
    
    # Healing configuration
    attr_accessor :auto_heal_responses, :healer_model, :max_heal_attempts
    
    # Cache configuration
    attr_accessor :cache_ttl
    
    # Capability validation configuration
    attr_accessor :strict_mode

    DEFAULT_API_VERSION = "v1"
    DEFAULT_REQUEST_TIMEOUT = 120
    DEFAULT_URI_BASE = "https://openrouter.ai/api"
    DEFAULT_CACHE_TTL = 7 * 24 * 60 * 60 # 7 days in seconds

    def initialize
      self.access_token = nil
      self.api_version = DEFAULT_API_VERSION
      self.extra_headers = {}
      self.log_errors = false
      self.request_timeout = DEFAULT_REQUEST_TIMEOUT
      self.uri_base = DEFAULT_URI_BASE
      
      # Healing defaults
      self.auto_heal_responses = false
      self.healer_model = "openai/gpt-4o-mini"
      self.max_heal_attempts = 2
      
      # Cache defaults
      self.cache_ttl = ENV.fetch('OPENROUTER_CACHE_TTL', DEFAULT_CACHE_TTL).to_i
      
      # Capability validation defaults
      self.strict_mode = ENV.fetch('OPENROUTER_STRICT_MODE', 'false').downcase == 'true'
    end

    def access_token
      return @access_token if @access_token

      raise ConfigurationError, "OpenRouter access token missing!"
    end

    def faraday(&block)
      self.faraday_config = block
    end

    def site_name=(value)
      @extra_headers["X-Title"] = value
    end

    def site_url=(value)
      @extra_headers["HTTP-Referer"] = value
    end
  end

  class << self
    attr_writer :configuration
  end

  def self.configuration
    @configuration ||= OpenRouter::Configuration.new
  end

  def self.configure
    yield(configuration)
  end
end

================
File: sig/open_router.rbs
================
module OpenRouter
  class Client
    include OpenRouter::HTTP

    def initialize: () { (OpenRouter::Configuration) -> void } -> void

    def complete: (
      messages: Array[Hash[Symbol, String]],
      ?model: String,
      ?providers: Array[String],
      ?transforms: Array[String],
      ?extras: Hash[Symbol, untyped],
      ?stream: Proc
    ) -> Hash[String, untyped]

    def models: () -> Array[Hash[String, untyped]]

    def query_generation_stats: (generation_id: String) -> Hash[String, untyped]
  end
end

================
File: spec/integration/structured_output_flow_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "End-to-end structured output scenarios" do
  let(:client) { OpenRouter::Client.new(access_token: "test_token") }
  
  let(:user_schema) do
    OpenRouter::Schema.define("complete_user") do
      string :name, required: true, description: "User's full name"
      integer :age, required: true
      string :email, required: true, format: "email"
      string :role, enum: ["admin", "editor", "viewer"], description: "User role"
      string :status, enum: ["active", "inactive"], description: "Account status"
    end
  end
  
  let(:response_format) do
    {
      type: "json_schema",
      json_schema: user_schema.to_h
    }
  end

  let(:messages) { [{ role: "user", content: "Create a user profile" }] }

  # Mock model capabilities for testing
  before do
    allow(OpenRouter::ModelRegistry).to receive(:has_capability?) do |model, capability|
      case model
      when "native-model"
        capability == :structured_outputs
      when "unsupported-model"
        false
      when "vision-model"  
        [:vision, :structured_outputs].include?(capability)
      else
        false
      end
    end
  end

  describe "model without native structured output support" do
    context "with auto-detection (default behavior)" do
      it "automatically forces extraction and succeeds" do
        # Mock the API response with conversational output containing JSON
        conversational_response = <<~RESPONSE
          I'll create a user profile for you:

          ```json
          {
            "name": "Alice Johnson",
            "age": 28,
            "email": "alice@example.com", 
            "role": "editor",
            "status": "active"
          }
          ```

          This user profile is now ready for use.
        RESPONSE

        expect(client).to receive(:post) do |path:, parameters:|
          # Should NOT include response_format (would cause 400)
          expect(parameters).not_to have_key(:response_format)
          # Should include injected schema instructions
          expect(parameters[:messages].last[:role]).to eq("system")
          expect(parameters[:messages].last[:content]).to include("JSON Schema")
          
          { "choices" => [{ "message" => { "content" => conversational_response } }] }
        end

        response = client.complete(messages, model: "unsupported-model", response_format: response_format)
        
        result = response.structured_output
        expect(result).to eq({
          "name" => "Alice Johnson",
          "age" => 28,
          "email" => "alice@example.com",
          "role" => "editor", 
          "status" => "active"
        })
      end

      it "warns about auto-forcing on unsupported model" do
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => '{"name": "Test", "age": 25, "email": "test@example.com", "role": "viewer", "status": "active"}' } }]
        })

        expect {
          client.complete(messages, model: "unsupported-model", response_format: response_format)
        }.to output(/doesn't support native structured outputs.*automatically using forced extraction/).to_stderr
      end
    end

    context "with explicit force_structured_output: true" do
      it "injects clear schema instructions into prompt" do
        injected_messages = nil
        
        expect(client).to receive(:post) do |path:, parameters:|
          injected_messages = parameters[:messages]
          { "choices" => [{ "message" => { "content" => '{}' } }] }
        end

        client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        
        instruction = injected_messages.last[:content]
        expect(instruction).to include("JSON Schema")
        expect(instruction).to include(user_schema.to_h.to_json)
        expect(instruction).to include("ONLY the JSON object")
      end
    end

    context "with malformed JSON requiring healing" do
      let(:malformed_response) do
        <<~RESPONSE
          Here's the user data:

          ```json
          {
            "name": "Bob Wilson",
            "age": "thirty-two", 
            "email": "bob-at-example-dot-com",
            "role": "administrator",
            "status": "enabled"
          }
          ```
        RESPONSE
      end

      let(:healed_response) do
        '{"name": "Bob Wilson", "age": 32, "email": "bob@example.com", "role": "admin", "status": "active"}'
      end

      it "heals malformed JSON with full context" do
        mock_client = double("client", 
          configuration: double(
            auto_heal_responses: true,
            max_heal_attempts: 2,
            healer_model: "gpt-3.5-turbo"
          )
        )

        healing_prompt = nil
        
        expect(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => malformed_response } }]
        })

        expect(mock_client).to receive(:complete) do |messages, **options|
          healing_prompt = messages.last[:content]
          OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_response}}]})
        end

        response = client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        response.client = mock_client
        
        result = response.structured_output(auto_heal: true)
        
        # Should include full response context in healing
        expect(healing_prompt).to include("Here's the user data")
        expect(healing_prompt).to include("thirty-two")
        expect(healing_prompt).to include("bob-at-example-dot-com")
        
        expect(result).to eq({
          "name" => "Bob Wilson",
          "age" => 32,
          "email" => "bob@example.com", 
          "role" => "admin",
          "status" => "active"
        })
      end
    end

    context "with gentle mode for resilience" do
      it "works in gentle mode without errors on malformed JSON" do
        malformed_json = '{"name": "Test", "age": invalid}'
        
        expect(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => malformed_json } }]
        })

        response = client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        
        result = response.structured_output(mode: :gentle)
        expect(result).to be_nil  # Returns nil instead of raising
      end

      it "returns extracted JSON when possible in gentle mode" do
        valid_json = '{"name": "Test", "age": 25, "email": "test@example.com", "role": "viewer", "status": "active"}'
        
        expect(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => valid_json } }]
        })

        response = client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        
        result = response.structured_output(mode: :gentle)
        expect(result["name"]).to eq("Test")
      end
    end
  end

  describe "model with native structured output support" do
    it "uses native format by default" do
      expect(client).to receive(:post) do |path:, parameters:|
        # Should include response_format for native support
        expect(parameters).to have_key(:response_format)
        expect(parameters[:response_format][:type]).to eq("json_schema")
        # Should NOT modify messages
        expect(parameters[:messages]).to eq(messages)
        
        {
          "choices" => [{
            "message" => {
              "content" => '{"name": "Native User", "age": 30, "email": "native@example.com", "role": "admin", "status": "active"}'
            }
          }]
        }
      end

      response = client.complete(messages, model: "native-model", response_format: response_format)
      
      result = response.structured_output
      expect(result["name"]).to eq("Native User")
    end

    it "can force extraction even on supported models when explicitly requested" do
      expect(client).to receive(:post) do |path:, parameters:|
        # Should NOT include response_format when forcing
        expect(parameters).not_to have_key(:response_format)
        # Should inject instructions
        expect(parameters[:messages].size).to be > messages.size
        
        { "choices" => [{ "message" => { "content" => '{"forced": true}' } }] }
      end

      response = client.complete(messages, model: "native-model", response_format: response_format, force_structured_output: true)
      
      result = response.structured_output
      expect(result["forced"]).to be true
    end

    it "respects mode setting for native responses" do
      allow(client).to receive(:post).and_return({
        "choices" => [{ "message" => { "content" => '{"invalid": json}' } }]
      })

      response = client.complete(messages, model: "native-model", response_format: response_format)
      
      # Gentle mode should return nil on parse failure
      result = response.structured_output(mode: :gentle)
      expect(result).to be_nil
    end
  end

  describe "mixed model scenarios" do
    it "handles model arrays (fallbacks) without forcing" do
      expect(client).to receive(:post) do |path:, parameters:|
        # Should use native format for fallback arrays
        expect(parameters).to have_key(:response_format)
        expect(parameters[:models]).to eq(["unsupported-model", "native-model"])
        
        { "choices" => [{ "message" => { "content" => '{"fallback": "success"}' } }] }
      end

      response = client.complete(messages, model: ["unsupported-model", "native-model"], response_format: response_format)
      
      result = response.structured_output
      expect(result["fallback"]).to eq("success")
    end

    it "handles openrouter/auto without forcing" do
      expect(client).to receive(:post) do |path:, parameters:|
        expect(parameters).to have_key(:response_format)
        expect(parameters[:model]).to eq("openrouter/auto")
        
        { "choices" => [{ "message" => { "content" => '{"auto": "selected"}' } }] }
      end

      response = client.complete(messages, model: "openrouter/auto", response_format: response_format)
      
      result = response.structured_output
      expect(result["auto"]).to eq("selected")
    end
  end

  describe "configuration-driven behavior" do
    context "with global configuration" do
      before do
        OpenRouter.configure do |config|
          config.auto_force_on_unsupported_models = true
          config.default_structured_output_mode = :gentle
          config.auto_heal_responses = false
        end
      end

      after do
        # Reset configuration
        OpenRouter.configure do |config|
          config.auto_force_on_unsupported_models = nil
          config.default_structured_output_mode = :strict  
          config.auto_heal_responses = true
        end
      end

      it "respects global auto-force configuration" do
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => '{"configured": true}' } }]
        })

        # Should auto-force based on config
        expect {
          client.complete(messages, model: "unsupported-model", response_format: response_format)
        }.to output(/automatically using forced extraction/).to_stderr
      end

      it "respects default mode configuration" do
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => '{"bad": json}' } }]
        })

        response = client.complete(messages, model: "native-model", response_format: response_format)
        
        # Should use gentle mode by default
        result = response.structured_output
        expect(result).to be_nil  # Gentle mode returns nil on failure
      end
    end

    context "with per-request overrides" do
      it "allows per-request mode override" do
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => '{"name": "Override", "age": 25, "email": "test@example.com", "role": "viewer", "status": "active"}' } }]
        })

        response = client.complete(messages, model: "native-model", response_format: response_format)
        
        # Override default mode per request
        result = response.structured_output(mode: :strict)
        expect(result["name"]).to eq("Override")
      end

      it "allows per-request force override" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).not_to have_key(:response_format)  # Should be forced
          { "choices" => [{ "message" => { "content" => '{"forced": true}' } }] }
        end

        # Explicitly force on supported model
        response = client.complete(messages, model: "native-model", response_format: response_format, force_structured_output: true)
        
        result = response.structured_output
        expect(result["forced"]).to be true
      end
    end
  end

  describe "complex real-world scenarios" do
    it "handles schema with multiple validation constraints" do
      complex_response = '{"name": "Complex User", "age": 35, "email": "complex@example.com", "role": "admin", "status": "active"}'
      
      allow(client).to receive(:post).and_return({
        "choices" => [{ "message" => { "content" => complex_response } }]
      })

      response = client.complete(messages, model: "native-model", response_format: response_format)
      
      result = response.structured_output
      
      # Verify all schema constraints are met
      expect(result["name"]).to be_a(String)
      expect(result["age"]).to be_a(Integer) 
      expect(result["email"]).to match(/@/)
      expect(["admin", "editor", "viewer"]).to include(result["role"])
      expect(["active", "inactive"]).to include(result["status"])
    end

    it "gracefully handles edge cases in forced extraction" do
      edge_case_responses = [
        "No JSON found in this response at all",
        "```json\n// This is a comment\n{}\n```",
        "```\n{\"no_json_marker\": true}\n```",
        '{"unquoted": field, "trailing": "comma",}'
      ]

      edge_case_responses.each do |response_content|
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => response_content } }]
        })

        response = client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        
        # Gentle mode should handle gracefully
        result = response.structured_output(mode: :gentle)
        expect(result).to be_a(Hash).or(be_nil)
      end
    end

    it "maintains performance with frequent structured output calls" do
      # Simulate multiple calls to ensure warnings don't spam
      5.times do
        allow(client).to receive(:post).and_return({
          "choices" => [{ "message" => { "content" => '{"iteration": true}' } }]
        })

        response = client.complete(messages, model: "unsupported-model", response_format: response_format)
        result = response.structured_output
        expect(result["iteration"]).to be true
      end

      # Should only see one warning despite multiple calls
      # (This is tested more thoroughly in the warning specs)
    end
  end

  describe "error scenarios and recovery" do
    it "provides helpful error messages when all healing attempts fail" do
      allow(client).to receive(:post).and_return({
        "choices" => [{ "message" => { "content" => '{"permanently": "broken"' } }]
      })

      mock_client = double("client", 
        configuration: double(
          auto_heal_responses: true,
          max_heal_attempts: 2,
          healer_model: "gpt-3.5-turbo"
        )
      )

      # Mock healing attempts that continue to fail
      allow(mock_client).to receive(:complete).and_return(
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"still": "broken"'}}]})
      )

      response = client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
      response.client = mock_client

      expect {
        response.structured_output(auto_heal: true)
      }.to raise_error(OpenRouter::StructuredOutputError, /after 2 healing attempts/)
    end

    it "handles network errors gracefully during healing" do
      allow(client).to receive(:post).and_return({
        "choices" => [{ "message" => { "content" => '{"broken": json}' } }]
      })

      mock_client = double("client", 
        configuration: double(
          auto_heal_responses: true,
          max_heal_attempts: 1,
          healer_model: "gpt-3.5-turbo"
        )
      )

      # Mock healing request that fails with network error
      allow(mock_client).to receive(:complete).and_raise(StandardError, "Network timeout")

      response = client.complete(messages, model: "native-model", response_format: response_format)
      response.client = mock_client

      expect {
        response.structured_output(auto_heal: true)
      }.to raise_error(StandardError, "Network timeout")
    end
  end
end

================
File: spec/vcr/basic_completion_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter Basic Completions", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])
  end

  let(:simple_messages) do
    [{ role: "user", content: "Say hello world" }]
  end

  describe "single model completion", vcr: { cassette_name: 'basic_completion_single_model' } do
    it "completes a simple message with gpt-3.5-turbo" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        extras: { max_tokens: 50 }
      )
      
      expect(response).to be_a(OpenRouter::Response)
      expect(response.content).to be_a(String)
      expect(response.content.length).to be > 0
      expect(response.id).to be_a(String)
      expect(response.model).to include("gpt-3.5-turbo")
      expect(response.usage).to be_a(Hash)
      expect(response.usage["prompt_tokens"]).to be > 0
      expect(response.usage["completion_tokens"]).to be > 0
      expect(response.usage["total_tokens"]).to be > 0
    end
  end

  describe "completion with parameters", vcr: { cassette_name: 'basic_completion_with_parameters' } do
    it "respects max_tokens parameter" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        extras: { 
          max_tokens: 10,
          temperature: 0.7,
          top_p: 0.9
        }
      )
      
      expect(response.content).to be_a(String)
      expect(response.usage["completion_tokens"]).to be <= 10
    end
  end

  describe "completion with different models", vcr: { cassette_name: 'basic_completion_different_models' } do
    it "works with Claude model" do
      response = client.complete(
        simple_messages,
        model: "anthropic/claude-3-haiku",
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.model).to include("claude-3-haiku")
      expect(response.usage).to be_a(Hash)
    end

    it "works with GPT-4 model" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-4o-mini",
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.model).to include("gpt-4o-mini")
      expect(response.usage).to be_a(Hash)
    end
  end

  describe "conversation continuation", vcr: { cassette_name: 'basic_completion_conversation' } do
    it "handles multi-turn conversations" do
      messages = [
        { role: "user", content: "What is 2+2?" },
        { role: "assistant", content: "2+2 equals 4." },
        { role: "user", content: "What about 3+3?" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-3.5-turbo",
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.content.downcase).to include("6")
    end
  end

  describe "system message support", vcr: { cassette_name: 'basic_completion_system_message' } do
    it "respects system messages" do
      messages = [
        { role: "system", content: "You are a helpful assistant that always responds in all caps." },
        { role: "user", content: "Hello" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-3.5-turbo",
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.content).to eq(response.content.upcase)
    end
  end

  describe "response metadata", vcr: { cassette_name: 'basic_completion_metadata' } do
    it "includes all expected response fields" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        extras: { max_tokens: 50 }
      )
      
      # Test Response object methods
      expect(response.id).to be_a(String)
      expect(response.object).to eq("chat.completion")
      expect(response.created).to be_a(Integer)
      expect(response.model).to be_a(String)
      expect(response.choices).to be_an(Array)
      expect(response.choices.length).to eq(1)
      
      # Test choice structure
      choice = response.choices.first
      expect(choice["message"]["role"]).to eq("assistant")
      expect(choice["message"]["content"]).to be_a(String)
      expect(["stop", "length"]).to include(choice["finish_reason"])
      
      # Test usage information
      expect(response.usage["prompt_tokens"]).to be > 0
      expect(response.usage["completion_tokens"]).to be > 0
      expect(response.usage["total_tokens"]).to be > 0
      
      # Test convenience methods
      expect(response.has_content?).to be true
      expect(response.error?).to be false
      expect(response.has_tool_calls?).to be false
    end
  end

  describe "backward compatibility", vcr: { cassette_name: 'basic_completion_backward_compatibility' } do
    it "maintains hash access patterns" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        extras: { max_tokens: 50 }
      )
      
      # Test hash-style access (backward compatibility)
      expect(response["id"]).to eq(response.id)
      expect(response["model"]).to eq(response.model)
      expect(response["usage"]).to eq(response.usage)
      expect(response.dig("choices", 0, "message", "content")).to eq(response.content)
      expect(response.key?("id")).to be true
      expect(response.key?("nonexistent")).to be false
      
      # Test to_h conversion
      hash_response = response.to_h
      expect(hash_response).to be_a(Hash)
      expect(hash_response["id"]).to eq(response.id)
    end
  end

  describe "providers parameter", vcr: { cassette_name: 'basic_completion_providers' } do
    it "accepts provider preferences" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        providers: ["openai"],
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.model).to include("gpt-3.5-turbo")
    end
  end

  describe "transforms parameter", vcr: { cassette_name: 'basic_completion_transforms' } do
    it "accepts transform instructions" do
      response = client.complete(
        simple_messages,
        model: "openai/gpt-3.5-turbo",
        transforms: ["middle-out"],
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
    end
  end
end

================
File: spec/vcr/error_handling_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter Error Handling", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])
  end

  let(:invalid_key_client) do
    OpenRouter::Client.new(access_token: "invalid_key_12345")
  end

  describe "authentication errors", vcr: { cassette_name: 'error_handling_invalid_auth' } do
    it "handles invalid API key gracefully" do
      expect {
        invalid_key_client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message).to include("Unauthorized")
      end
    end

    it "handles missing API key" do
      no_key_client = OpenRouter::Client.new(access_token: nil)
      
      expect {
        no_key_client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError)
    end
  end

  describe "model errors", vcr: { cassette_name: 'error_handling_model_errors' } do
    it "handles non-existent model" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "nonexistent/fake-model-12345",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to include("model")
      end
    end

    it "handles model access restrictions" do
      # Try to use a model that might require special permissions
      # Note: This test might pass if the model is available to the account
      begin
        response = client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-4",  # GPT-4 might have restrictions
          extras: { max_tokens: 50 }
        )
        # If it succeeds, that's fine - the model is available
        expect(response).to be_a(OpenRouter::Response)
      rescue OpenRouter::ServerError => e
        # If it fails due to access restrictions, that's expected
        expect(e.message.downcase).to match(/access|permission|forbidden|unauthorized/)
      end
    end
  end

  describe "parameter validation errors", vcr: { cassette_name: 'error_handling_parameter_validation' } do
    it "handles invalid max_tokens values" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: -1 }  # Invalid negative value
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to include("max_tokens")
      end
    end

    it "handles invalid temperature values" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-3.5-turbo",
          extras: { 
            max_tokens: 50,
            temperature: 5.0  # Invalid - should be 0-2
          }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to include("temperature")
      end
    end

    it "handles empty messages array" do
      expect {
        client.complete(
          [],  # Empty messages
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to include("message")
      end
    end

    it "handles malformed message structure" do
      expect {
        client.complete(
          [{ invalid: "structure" }],  # Missing role and content
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to match(/message|role|content/)
      end
    end
  end

  describe "tool calling errors", vcr: { cassette_name: 'error_handling_tool_calling' } do
    let(:invalid_tool) do
      {
        type: "function",
        function: {
          name: "invalid_tool",
          description: "A tool with invalid parameters",
          parameters: {
            type: "invalid_type",  # Invalid parameter type
            properties: {}
          }
        }
      }
    end

    it "handles invalid tool definitions" do
      expect {
        client.complete(
          [{ role: "user", content: "Use a tool" }],
          model: "openai/gpt-4o-mini",
          tools: [invalid_tool],
          extras: { max_tokens: 500 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to match(/tool|parameter|schema/)
      end
    end

    it "handles tool choice for non-existent tool" do
      simple_tool = OpenRouter::Tool.define do
        name "simple_tool"
        description "A simple tool"
      end

      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openai/gpt-4o-mini",
          tools: [simple_tool],
          tool_choice: { type: "function", function: { name: "nonexistent_tool" } },
          extras: { max_tokens: 500 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to include("tool")
      end
    end
  end

  describe "structured output errors", vcr: { cassette_name: 'error_handling_structured_outputs' } do
    let(:invalid_schema) do
      {
        type: "json_schema",
        json_schema: {
          name: "invalid_schema",
          schema: {
            type: "invalid_type",  # Invalid JSON schema type
            properties: {}
          }
        }
      }
    end

    it "handles invalid schema definitions" do
      expect {
        client.complete(
          [{ role: "user", content: "Give me structured output" }],
          model: "openai/gpt-4o-mini",
          response_format: invalid_schema,
          extras: { max_tokens: 500 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message.downcase).to match(/schema|format/)
      end
    end

    it "handles models that don't support structured outputs" do
      simple_schema = OpenRouter::Schema.define("simple") do
        string "message", required: true
      end

      # Try with a model that might not support structured outputs
      begin
        response = client.complete(
          [{ role: "user", content: "Hello" }],
          model: "meta-llama/llama-3.1-8b-instruct",
          response_format: simple_schema,
          extras: { max_tokens: 500 }
        )
        # If it works, that's fine
        expect(response).to be_a(OpenRouter::Response)
      rescue OpenRouter::ServerError => e
        # If it fails because the model doesn't support structured outputs
        expect(e.message.downcase).to match(/structured|format|support/)
      end
    end
  end

  describe "rate limiting", vcr: { cassette_name: 'error_handling_rate_limiting' } do
    it "handles rate limit responses gracefully" do
      # Note: This is hard to test reliably without actually hitting rate limits
      # We'll make multiple rapid requests and see if we get rate limited
      
      begin
        5.times do |i|
          response = client.complete(
            [{ role: "user", content: "Request #{i}" }],
            model: "openai/gpt-3.5-turbo",
            extras: { max_tokens: 10 }
          )
          expect(response).to be_a(OpenRouter::Response)
        end
      rescue OpenRouter::ServerError => e
        # If we get rate limited, the error should mention it
        if e.message.downcase.include?("rate") || e.message.include?("429")
          expect(e.message.downcase).to match(/rate|limit|too many/)
        else
          raise e  # Re-raise if it's a different error
        end
      end
    end
  end

  describe "network and timeout errors", vcr: { cassette_name: 'error_handling_network_timeouts' } do
    it "handles timeout scenarios" do
      # Create a client with a very short timeout
      timeout_client = OpenRouter::Client.new(
        access_token: ENV['OPENROUTER_API_KEY'],
        request_timeout: 1  # 1 second timeout
      )

      begin
        response = timeout_client.complete(
          [{ role: "user", content: "This is a simple request" }],
          model: "openai/gpt-3.5-turbo",
          extras: { max_tokens: 50 }
        )
        # If it completes quickly, that's fine
        expect(response).to be_a(OpenRouter::Response)
      rescue => e
        # Should get a timeout or network error
        expect(e.class.name).to match(/Timeout|Network|Connection/)
      end
    end
  end

  describe "malformed response handling", vcr: { cassette_name: 'error_handling_malformed_responses' } do
    it "handles empty responses" do
      # This would be caught by the client's empty response check
      # We can test this by mocking, but with VCR we rely on the actual API behavior
      
      # The client should raise ServerError for empty responses
      # This is tested in the client logic: "Empty response from OpenRouter"
    end

    it "handles responses with error fields" do
      # Try to trigger an error response from the API
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "definitely-nonexistent-model-name-12345",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        expect(error.message).to be_a(String)
        expect(error.message.length).to be > 0
      end
    end
  end

  describe "model registry error handling", vcr: { cassette_name: 'error_handling_model_registry' } do
    it "handles ModelRegistry API failures gracefully" do
      # Clear cache to force fresh API call
      OpenRouter::ModelRegistry.clear_cache!
      
      begin
        models = OpenRouter::ModelRegistry.all_models
        expect(models).to be_a(Hash)
        expect(models.keys.length).to be > 0
      rescue OpenRouter::ModelRegistryError => e
        # If ModelRegistry fails, it should raise appropriate error
        expect(e.message).to include("OpenRouter")
      end
    end

    it "handles ModelRegistry with invalid API responses" do
      # This would require mocking invalid JSON responses
      # With VCR, we rely on real API responses which should be valid
      
      # Test the error handling logic directly
      expect {
        OpenRouter::ModelRegistry.send(:process_api_models, "invalid_data")
      }.to raise_error(NoMethodError)  # Should fail trying to call .each on string
    end
  end

  describe "smart completion error scenarios", vcr: { cassette_name: 'error_handling_smart_completion' } do
    it "handles cases where no models meet requirements" do
      expect {
        client.smart_complete(
          [{ role: "user", content: "Hello" }],
          requirements: {
            capabilities: [:function_calling],
            max_input_cost: 0.000001,  # Impossibly low cost
            min_context_length: 1_000_000  # Impossibly high context
          }
        )
      }.to raise_error(OpenRouter::ModelSelectionError, /No model found matching requirements/)
    end

    it "handles smart_complete_with_fallback when all models fail" do
      # Clear ModelRegistry cache to ensure fresh data
      OpenRouter::ModelRegistry.clear_cache!
      
      expect {
        client.smart_complete_with_fallback(
          [{ role: "user", content: "Hello" }],
          requirements: {
            max_input_cost: 0.000001  # Impossibly low cost
          },
          max_retries: 2
        )
      }.to raise_error(OpenRouter::ModelSelectionError, /No models found matching requirements/)
    end
  end

  describe "error message quality", vcr: { cassette_name: 'error_handling_message_quality' } do
    it "provides informative error messages" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "nonexistent/model",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        # Error message should be informative
        expect(error.message).to be_a(String)
        expect(error.message.length).to be > 10  # Should have substantial content
        expect(error.message).not_to eq("Error")  # Should be more specific
      end
    end
  end

  describe "client error response structure", vcr: { cassette_name: 'error_handling_response_structure' } do
    it "properly extracts error information from API responses" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "invalid-model-name",
          extras: { max_tokens: 50 }
        )
      }.to raise_error(OpenRouter::ServerError) do |error|
        # Should extract the actual error message from the API response
        expect(error.message).to be_a(String)
        # Should not be a generic message about response structure
        expect(error.message).not_to include("dig")
        expect(error.message).not_to include("[]")
      end
    end
  end

  describe "concurrent request error handling", vcr: { cassette_name: 'error_handling_concurrent_requests' } do
    it "handles multiple simultaneous requests with some failures" do
      # Create multiple threads to make concurrent requests
      # Some might succeed, some might fail
      
      threads = []
      results = []
      
      3.times do |i|
        threads << Thread.new do
          begin
            # Mix of valid and invalid requests
            model = i.even? ? "openai/gpt-3.5-turbo" : "invalid-model-#{i}"
            
            response = client.complete(
              [{ role: "user", content: "Request #{i}" }],
              model: model,
              extras: { max_tokens: 20 }
            )
            results << { success: true, response: response }
          rescue OpenRouter::ServerError => e
            results << { success: false, error: e.message }
          end
        end
      end
      
      threads.each(&:join)
      
      # Should have a mix of successes and failures
      successes = results.select { |r| r[:success] }
      failures = results.select { |r| !r[:success] }
      
      expect(successes.length).to be > 0  # At least some should succeed
      expect(failures.length).to be > 0   # At least some should fail
      
      # All errors should be properly formatted
      failures.each do |failure|
        expect(failure[:error]).to be_a(String)
        expect(failure[:error].length).to be > 0
      end
    end
  end
end

================
File: spec/vcr/model_fallback_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter Model Fallback", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])
  end

  let(:simple_messages) do
    [{ role: "user", content: "Say hello" }]
  end

  describe "models array with fallback route", vcr: { cassette_name: 'model_fallback_basic' } do
    it "uses fallback route with array of models" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      expect(response).to be_a(OpenRouter::Response)
      expect(response.content).to be_a(String)
      expect(response.content.length).to be > 0
      
      # Should succeed with one of the models in the array
      expect(response.model).to be_a(String)
      used_model = response.model
      
      # Verify the used model was one of our fallback options
      model_matched = models.any? { |model| used_model.include?(model.split('/').last) }
      expect(model_matched).to be true
      
      expect(response.usage).to be_a(Hash)
      expect(response.usage["total_tokens"]).to be > 0
    end
  end

  describe "ordered fallback preference", vcr: { cassette_name: 'model_fallback_ordered' } do
    it "attempts models in specified order" do
      # Put a less common or potentially unavailable model first
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      expect(response.content).to be_a(String)
      expect(response.model).to be_a(String)
      
      # The response should indicate which model was actually used
      # This helps verify the fallback mechanism worked
      puts "Used model: #{response.model}"
    end
  end

  describe "fallback with specific providers", vcr: { cassette_name: 'model_fallback_with_providers' } do
    it "respects provider preferences in fallback" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        providers: ["openai"],
        extras: { max_tokens: 50 }
      )

      expect(response.content).to be_a(String)
      expect(response.model).to include("openai")
    end
  end

  describe "fallback with tool calling", vcr: { cassette_name: 'model_fallback_tool_calling' } do
    let(:simple_tool) do
      OpenRouter::Tool.define do
        name "get_time"
        description "Get the current time"
        parameters do
          string "timezone", required: false, description: "Timezone (optional)"
        end
      end
    end

    it "handles tool calling with model fallback" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      messages = [
        { role: "user", content: "What time is it?" }
      ]

      response = client.complete(
        messages,
        model: models,
        tools: [simple_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response).to be_a(OpenRouter::Response)
      
      # Should either have tool calls or regular content
      if response.has_tool_calls?
        expect(response.tool_calls.first.function_name).to eq("get_time")
      else
        expect(response.content).to be_a(String)
      end
    end
  end

  describe "fallback with structured outputs", vcr: { cassette_name: 'model_fallback_structured_outputs' } do
    let(:simple_schema) do
      OpenRouter::Schema.define("greeting_response") do
        string "greeting", required: true, description: "A greeting message"
        string "language", required: false, description: "Language of the greeting"
      end
    end

    it "handles structured outputs with model fallback" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      expect(response.content).to be_a(String)
      
      # Test structured output parsing
      structured = response.structured_output
      expect(structured).to be_a(Hash)
      expect(structured["greeting"]).to be_a(String)
    end
  end

  describe "smart completion with fallback", vcr: { cassette_name: 'model_fallback_smart_completion' } do
    it "uses smart_complete_with_fallback method" do
      requirements = {
        capabilities: [:function_calling],
        max_input_cost: 0.01
      }

      response = client.smart_complete_with_fallback(
        simple_messages,
        requirements: requirements,
        optimization: :cost,
        max_retries: 3,
        extras: { max_tokens: 100 }
      )

      expect(response).to be_a(OpenRouter::Response)
      expect(response.content).to be_a(String)
      expect(response.model).to be_a(String)
      
      puts "Smart completion used model: #{response.model}"
    end
  end

  describe "fallback behavior with different request parameters", vcr: { cassette_name: 'model_fallback_parameters' } do
    it "maintains request parameters across fallback attempts" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { 
          max_tokens: 20,
          temperature: 0.7,
          top_p: 0.9
        }
      )

      expect(response.content).to be_a(String)
      # The response should respect the max_tokens limit
      expect(response.usage["completion_tokens"]).to be <= 20
    end
  end

  describe "mixed model families in fallback", vcr: { cassette_name: 'model_fallback_mixed_families' } do
    it "successfully falls back across different model families" do
      models = [
        "openai/gpt-4o-mini",
        "anthropic/claude-3-haiku",
        "meta-llama/llama-3.1-8b-instruct"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      expect(response.content).to be_a(String)
      expect(response.model).to be_a(String)
      
      # Should work with any of the different model families
      used_model = response.model.downcase
      expect(
        used_model.include?("gpt") || 
        used_model.include?("claude") || 
        used_model.include?("llama")
      ).to be true
    end
  end

  describe "fallback error scenarios", vcr: { cassette_name: 'model_fallback_errors', allow_unused_http_interactions: true } do
    it "handles case where some models in array are unavailable" do
      # Include a model that might not exist or be unavailable
      models = [
        "nonexistent/fake-model",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      # Should succeed with one of the valid models
      expect(response.content).to be_a(String)
      expect(response.model).to be_a(String)
      
      # Should not use the fake model
      expect(response.model).not_to include("fake-model")
    end
  end

  describe "fallback with conversation continuation", vcr: { cassette_name: 'model_fallback_conversation' } do
    it "maintains consistency in multi-turn conversations with fallback" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      # First turn
      first_response = client.complete(
        [{ role: "user", content: "My name is Alice. Remember this." }],
        model: models,
        extras: { max_tokens: 50 }
      )

      expect(first_response.content).to be_a(String)
      
      # Second turn - continue conversation
      conversation = [
        { role: "user", content: "My name is Alice. Remember this." },
        { role: "assistant", content: first_response.content },
        { role: "user", content: "What is my name?" }
      ]

      second_response = client.complete(
        conversation,
        model: models,
        extras: { max_tokens: 50 }
      )

      expect(second_response.content).to be_a(String)
      expect(second_response.content.downcase).to include("alice")
    end
  end

  describe "fallback response metadata", vcr: { cassette_name: 'model_fallback_metadata' } do
    it "provides complete metadata for fallback responses" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      # All standard response fields should be present
      expect(response.id).to be_a(String)
      expect(response.object).to eq("chat.completion")
      expect(response.created).to be_a(Integer)
      expect(response.model).to be_a(String)
      expect(response.choices).to be_an(Array)
      expect(response.choices.length).to eq(1)
      expect(response.usage).to be_a(Hash)
      expect(response.usage["prompt_tokens"]).to be > 0
      expect(response.usage["completion_tokens"]).to be > 0
      expect(response.usage["total_tokens"]).to be > 0

      # Test backward compatibility
      expect(response["id"]).to eq(response.id)
      expect(response["model"]).to eq(response.model)
      expect(response.to_h).to be_a(Hash)
    end
  end

  describe "performance characteristics of fallback", vcr: { cassette_name: 'model_fallback_performance' } do
    it "completes requests efficiently with fallback" do
      models = [
        "openai/gpt-3.5-turbo",  # Fast, inexpensive model
        "openai/gpt-4o-mini"     # Backup option
      ]

      start_time = Time.now
      
      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 20 }
      )
      
      end_time = Time.now
      
      expect(response.content).to be_a(String)
      
      # Should complete in reasonable time (this is somewhat subjective)
      response_time = end_time - start_time
      expect(response_time).to be < 30 # 30 seconds max
      
      puts "Fallback request completed in #{response_time.round(2)} seconds"
    end
  end

  describe "route parameter validation", vcr: { cassette_name: 'model_fallback_route_validation' } do
    it "correctly sets route parameter for model arrays" do
      models = [
        "openai/gpt-4o-mini",
        "openai/gpt-3.5-turbo"
      ]

      response = client.complete(
        simple_messages,
        model: models,
        extras: { max_tokens: 50 }
      )

      # The route should be set to "fallback" internally when using model arrays
      # We can't directly test this without inspecting the request, but we can verify
      # that the fallback mechanism worked by getting a valid response
      expect(response.content).to be_a(String)
      expect(response.model).to be_a(String)
    end
  end
end

================
File: spec/vcr/model_registry_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter ModelRegistry", :vcr do
  before(:each) do
    # Clear cache before each test to ensure fresh API calls
    OpenRouter::ModelRegistry.clear_cache!
  end

  after(:all) do
    # Clean up cache after tests
    OpenRouter::ModelRegistry.clear_cache!
  end

  describe "fetching models from API", vcr: { cassette_name: 'model_registry_fetch_from_api' } do
    it "successfully fetches model list from OpenRouter API" do
      models_data = OpenRouter::ModelRegistry.fetch_models_from_api
      
      expect(models_data).to be_a(Hash)
      expect(models_data).to have_key("data")
      expect(models_data["data"]).to be_an(Array)
      expect(models_data["data"].length).to be > 0
      
      # Check structure of first model
      first_model = models_data["data"].first
      expect(first_model).to have_key("id")
      expect(first_model).to have_key("name")
      expect(first_model).to have_key("pricing")
      expect(first_model["pricing"]).to have_key("prompt")
      expect(first_model["pricing"]).to have_key("completion")
      expect(first_model).to have_key("context_length")
    end
  end

  describe "caching behavior", vcr: { cassette_name: 'model_registry_caching' } do
    it "caches models after first fetch" do
      # First call should fetch from API and cache
      models = OpenRouter::ModelRegistry.all_models
      expect(models).to be_a(Hash)
      expect(models.keys.length).to be > 0
      
      # Check that cache files were created
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_DATA_FILE)).to be true
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_METADATA_FILE)).to be true
      
      # Verify some expected models exist
      expect(models).to have_key("openai/gpt-3.5-turbo")
      expect(models).to have_key("anthropic/claude-3-haiku")
    end

    it "loads from cache on subsequent calls" do
      # First call to populate cache
      OpenRouter::ModelRegistry.all_models
      
      # Verify cache exists
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_DATA_FILE)).to be true
      
      # Load cached data directly
      cached_data = OpenRouter::ModelRegistry.read_cache_if_fresh
      expect(cached_data).to be_a(Hash)
      expect(cached_data).to have_key("data")
    end

    it "refreshes cache when explicitly requested" do
      # Initial fetch
      OpenRouter::ModelRegistry.all_models
      
      # Refresh cache
      refreshed_models = OpenRouter::ModelRegistry.refresh!
      expect(refreshed_models).to be_a(Hash)
      expect(refreshed_models.keys.length).to be > 0
    end
  end

  describe "model processing", vcr: { cassette_name: 'model_registry_processing' } do
    it "processes API models into internal format" do
      models = OpenRouter::ModelRegistry.all_models
      
      # Check a known model
      gpt_model = models["openai/gpt-3.5-turbo"]
      expect(gpt_model).to be_a(Hash)
      
      # Check required fields
      expect(gpt_model).to have_key(:name)
      expect(gpt_model).to have_key(:cost_per_1k_tokens)
      expect(gpt_model).to have_key(:context_length)
      expect(gpt_model).to have_key(:capabilities)
      expect(gpt_model).to have_key(:description)
      expect(gpt_model).to have_key(:performance_tier)
      
      # Check cost structure
      expect(gpt_model[:cost_per_1k_tokens]).to have_key(:input)
      expect(gpt_model[:cost_per_1k_tokens]).to have_key(:output)
      expect(gpt_model[:cost_per_1k_tokens][:input]).to be_a(Float)
      expect(gpt_model[:cost_per_1k_tokens][:output]).to be_a(Float)
      
      # Check capabilities
      expect(gpt_model[:capabilities]).to be_an(Array)
      expect(gpt_model[:capabilities]).to include(:chat)
      
      # Check context length
      expect(gpt_model[:context_length]).to be_a(Integer)
      expect(gpt_model[:context_length]).to be > 0
    end

    it "extracts capabilities correctly" do
      models = OpenRouter::ModelRegistry.all_models
      
      # Find a model with function calling support
      function_calling_model = models.find { |_, specs| specs[:capabilities].include?(:function_calling) }
      expect(function_calling_model).not_to be_nil
      
      # Find a model with vision support (if any)
      vision_models = models.select { |_, specs| specs[:capabilities].include?(:vision) }
      # Note: Vision models may not always be available, so we don't assert their existence
      
      # Find a model with long context
      long_context_models = models.select { |_, specs| specs[:capabilities].include?(:long_context) }
      expect(long_context_models.length).to be >= 0 # Some models should have long context
    end

    it "assigns performance tiers correctly" do
      models = OpenRouter::ModelRegistry.all_models
      
      premium_models = models.select { |_, specs| specs[:performance_tier] == :premium }
      standard_models = models.select { |_, specs| specs[:performance_tier] == :standard }
      
      expect(premium_models.length).to be > 0
      expect(standard_models.length).to be > 0
    end
  end

  describe "model lookup methods", vcr: { cassette_name: 'model_registry_lookups' } do
    it "checks if models exist" do
      expect(OpenRouter::ModelRegistry.model_exists?("openai/gpt-3.5-turbo")).to be true
      expect(OpenRouter::ModelRegistry.model_exists?("nonexistent/model")).to be false
    end

    it "retrieves model information" do
      model_info = OpenRouter::ModelRegistry.get_model_info("openai/gpt-3.5-turbo")
      expect(model_info).to be_a(Hash)
      expect(model_info[:name]).to be_a(String)
      expect(model_info[:cost_per_1k_tokens]).to be_a(Hash)
      
      # Non-existent model should return nil
      expect(OpenRouter::ModelRegistry.get_model_info("nonexistent/model")).to be_nil
    end

    it "calculates estimated costs" do
      cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "openai/gpt-3.5-turbo",
        input_tokens: 1000,
        output_tokens: 500
      )
      
      expect(cost).to be_a(Float)
      expect(cost).to be > 0
      
      # Zero cost for non-existent model
      zero_cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "nonexistent/model",
        input_tokens: 1000,
        output_tokens: 500
      )
      expect(zero_cost).to eq(0)
    end
  end

  describe "model selection", vcr: { cassette_name: 'model_registry_selection' } do
    it "finds models meeting capability requirements" do
      models = OpenRouter::ModelRegistry.models_meeting_requirements(
        capabilities: [:function_calling]
      )
      
      expect(models).to be_a(Hash)
      expect(models.length).to be > 0
      
      # All returned models should have function calling capability
      models.each do |_, specs|
        expect(specs[:capabilities]).to include(:function_calling)
      end
    end

    it "finds models within cost constraints" do
      models = OpenRouter::ModelRegistry.models_meeting_requirements(
        max_input_cost: 0.00001 # Very low cost constraint
      )
      
      expect(models).to be_a(Hash)
      
      # All returned models should be within cost constraint
      models.each do |_, specs|
        expect(specs[:cost_per_1k_tokens][:input]).to be <= 0.00001
      end
    end

    it "finds models with minimum context length" do
      models = OpenRouter::ModelRegistry.models_meeting_requirements(
        min_context_length: 32000
      )
      
      expect(models).to be_a(Hash)
      
      # All returned models should meet context requirement
      models.each do |_, specs|
        expect(specs[:context_length]).to be >= 32000
      end
    end

    it "finds best model with multiple constraints" do
      model_pair = OpenRouter::ModelRegistry.find_best_model(
        capabilities: [:function_calling],
        max_input_cost: 0.01,
        min_context_length: 4000
      )
      
      if model_pair # May be nil if no models meet all constraints
        model_id, model_specs = model_pair
        expect(model_id).to be_a(String)
        expect(model_specs[:capabilities]).to include(:function_calling)
        expect(model_specs[:cost_per_1k_tokens][:input]).to be <= 0.01
        expect(model_specs[:context_length]).to be >= 4000
      end
    end

    it "respects pick_newer preference" do
      # Find newest model overall
      newest_model = OpenRouter::ModelRegistry.find_best_model(
        pick_newer: true
      )
      
      expect(newest_model).not_to be_nil
      model_id, model_specs = newest_model
      expect(model_id).to be_a(String)
      expect(model_specs[:created_at]).to be_a(Integer)
    end
  end

  describe "error handling", vcr: { cassette_name: 'model_registry_errors', allow_unused_http_interactions: true } do
    it "handles network errors gracefully" do
      # Temporarily modify API base to cause network error
      original_base = OpenRouter::ModelRegistry::API_BASE
      OpenRouter::ModelRegistry.const_set(:API_BASE, "https://invalid-domain-12345.com/api/v1")
      
      expect {
        OpenRouter::ModelRegistry.fetch_models_from_api
      }.to raise_error(OpenRouter::ModelRegistryError, /Network error/)
      
      # Restore original API base
      OpenRouter::ModelRegistry.const_set(:API_BASE, original_base)
    end
  end

  describe "integration with Client", vcr: { cassette_name: 'model_registry_client_integration' } do
    let(:client) { OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY']) }

    it "validates that models from registry work with client" do
      # Get a few models from registry
      models = OpenRouter::ModelRegistry.all_models
      test_models = models.keys.first(3)
      
      test_models.each do |model_id|
        next unless OpenRouter::ModelRegistry.model_exists?(model_id)
        
        begin
          response = client.complete(
            [{ role: "user", content: "Hello" }],
            model: model_id,
            extras: { max_tokens: 10 }
          )
          
          expect(response).to be_a(OpenRouter::Response)
          expect(response.content).to be_a(String)
        rescue OpenRouter::ServerError => e
          # Some models may not be available or may have restrictions
          # Log but don't fail the test
          puts "Model #{model_id} not available: #{e.message}"
        end
      end
    end
  end

  describe "cache file management", vcr: { cassette_name: 'model_registry_cache_management' } do
    it "handles corrupted cache files" do
      # Create cache directory and corrupted cache file
      FileUtils.mkdir_p(OpenRouter::ModelRegistry::CACHE_DIR)
      File.write(OpenRouter::ModelRegistry::CACHE_DATA_FILE, "invalid json content")
      
      # Should handle corruption gracefully and fetch fresh data
      models = OpenRouter::ModelRegistry.all_models
      expect(models).to be_a(Hash)
      expect(models.keys.length).to be > 0
    end

    it "clears cache completely" do
      # Ensure cache exists
      OpenRouter::ModelRegistry.all_models
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_DATA_FILE)).to be true
      
      # Clear cache
      OpenRouter::ModelRegistry.clear_cache!
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_DATA_FILE)).to be false
      expect(File.exist?(OpenRouter::ModelRegistry::CACHE_METADATA_FILE)).to be false
      expect(Dir.exist?(OpenRouter::ModelRegistry::CACHE_DIR)).to be false
    end
  end
end

================
File: spec/vcr/response_healing_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Response Healing with Real API", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV.fetch('OPENROUTER_API_KEY')) do |config|
      config.auto_heal_responses = true
      config.healer_model = "openai/gpt-4o-mini"
      config.max_heal_attempts = 2
    end
  end

  let(:basic_schema) do
    OpenRouter::Schema.define("person") do
      string :name, required: true, description: "Person's name"
      integer :age, required: true, description: "Person's age in years"
      string :email, required: false, description: "Email address"
    end
  end

  let(:complex_schema) do
    OpenRouter::Schema.define("analysis") do
      string :summary, required: true, description: "Brief summary"
      object :data, required: true, description: "Analysis data" do
        array :items, required: true, description: "List of items" do
          object do
            string :name, required: true
            number :score, required: true
          end
        end
        boolean :valid, required: true
      end
    end
  end

  describe "testing malformed JSON scenarios", vcr: { cassette_name: 'healing_malformed_json' } do
    it "attempts to generate and heal malformed JSON with tricky prompts" do
      # Use a prompt designed to potentially generate malformed JSON
      messages = [
        {
          role: "user", 
          content: <<~PROMPT
            Generate a JSON response for a person. Here's an example that might confuse you:
            {"name": "John's \"Nickname\"", "age": 30, "comment": "Say: I'm 30"}
            
            Now create a similar JSON but for someone named O'Reilly with age 25.
            Make sure to include quotes and apostrophes in realistic ways.
            
            Important: Respond ONLY with the JSON, no explanations or markdown formatting.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-2024-08-06", # Use a model that supports structured outputs
        response_format: basic_schema,
        extras: { max_tokens: 200, temperature: 0.7 } # Higher temperature for more variability
      )

      # The healing should handle any JSON issues automatically
      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to be_a(String)
      expect(structured["age"]).to be_a(Integer)
      
      # Verify the response structure matches our schema expectations
      expect(structured["name"]).to include("O'Reilly")
      expect(structured["age"]).to eq(25)
      
      # Log what we actually got for debugging
      puts "Generated content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "edge case content that might break parsing", vcr: { cassette_name: 'healing_edge_cases' } do
    it "handles responses with embedded JSON examples" do
      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Create a JSON response about a developer named Sarah who is 28 years old.
            
            Before you respond, consider this example: {"broken": json, "missing": "quotes"}
            But make sure YOUR response is valid JSON that matches the required schema.
            
            Respond with proper JSON only.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-2024-08-06", # Use model that supports structured outputs
        response_format: basic_schema,
        extras: { max_tokens: 300, temperature: 0.3 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to include("Sarah")
      expect(structured["age"]).to eq(28)
      
      puts "Edge case content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "complex nested structure healing", vcr: { cassette_name: 'healing_complex_structures' } do
    it "heals complex nested JSON structures" do
      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Analyze these items and provide a summary with data:
            - Apple (score: 0.95)
            - Banana (score: 0.87)
            - Cherry (score: 0.92)
            
            Create a complex analysis JSON with nested objects and arrays.
            Include a summary and data object with items array and valid boolean.
            
            Here's a confusing example with bad syntax: {"data": {"items": [{"name": Apple, score: 0.95}], "valid": true}}
            But make YOUR response syntactically correct JSON.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-2024-08-06",
        response_format: complex_schema,
        extras: { max_tokens: 500, temperature: 0.5 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["summary"]).to be_a(String)
      expect(structured["data"]).to be_a(Hash)
      expect(structured["data"]["items"]).to be_an(Array)
      expect([true, false]).to include(structured["data"]["valid"])
      
      # Check item structure
      structured["data"]["items"].each do |item|
        expect(item["name"]).to be_a(String)
        expect(item["score"]).to be_a(Numeric)
      end
      
      puts "Complex structure content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "healing with different healer models", vcr: { cassette_name: 'healing_different_models' } do
    it "uses Claude as a healer model for GPT responses" do
      # Client configured to use Claude for healing
      claude_healer_client = OpenRouter::Client.new(access_token: ENV.fetch('OPENROUTER_API_KEY')) do |config|
        config.auto_heal_responses = true
        config.healer_model = "anthropic/claude-3-haiku"
        config.max_heal_attempts = 1
      end

      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Create JSON for person data. Include some tricky characters:
            - Name: "François O'Brien-Smith"  
            - Age: 35
            - Email: "test@example.com"
            
            Example of what NOT to do: {"name": François, "age": "35", "email": test@example.com}
            Make sure your JSON is properly formatted with correct quotes and types.
          PROMPT
        }
      ]

      response = claude_healer_client.complete(
        messages,
        model: "openai/gpt-4o-2024-08-06", # Primary model
        response_format: basic_schema,
        extras: { max_tokens: 300, temperature: 0.6 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to include("François")
      expect(structured["name"]).to include("O'Brien")
      expect(structured["age"]).to eq(35)
      
      puts "Multi-model healing content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "testing healing attempt limits", vcr: { cassette_name: 'healing_attempt_limits' } do
    it "respects max_heal_attempts configuration" do
      # Client with only 1 healing attempt
      limited_client = OpenRouter::Client.new(access_token: ENV.fetch('OPENROUTER_API_KEY')) do |config|
        config.auto_heal_responses = true
        config.healer_model = "openai/gpt-4o-mini"
        config.max_heal_attempts = 1
      end

      messages = [
        {
          role: "user",
          content: <<~PROMPT
            This is a challenging prompt designed to potentially create JSON issues.
            
            Create JSON for: name="Emily Davis", age=29, email="emily@test.com"
            
            But be aware of these problematic patterns:
            {"name": Emily Davis, "age": twenty-nine, "email": emily@test.com}
            {"name": "Emily" + "Davis", "age": 29}
            {name: "Emily Davis", age: 29, email: "emily@test.com"}
            
            Generate proper JSON that follows the schema exactly.
          PROMPT
        }
      ]

      response = limited_client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: basic_schema,
        extras: { max_tokens: 250, temperature: 0.8 } # Higher temperature for more chance of issues
      )

      # Even with potential issues, healing should work or fail gracefully
      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to include("Emily")
      expect(structured["age"]).to eq(29)
      
      puts "Limited attempts content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "healing disabled vs enabled comparison", vcr: { cassette_name: 'healing_comparison' } do
    it "compares behavior with and without healing enabled" do
      # Test without healing first
      no_heal_client = OpenRouter::Client.new(access_token: ENV.fetch('OPENROUTER_API_KEY')) do |config|
        config.auto_heal_responses = false
      end

      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Generate a simple JSON response for a person named Michael, age 40.
            
            Warning: This broken example should NOT be copied: {"name": Michael, "age": "40"}
            Instead, create proper JSON with correct syntax and types.
          PROMPT
        }
      ]

      # First try without healing
      no_heal_response = no_heal_client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: basic_schema,
        extras: { max_tokens: 200, temperature: 0.4 }
      )

      # Then try with healing enabled
      heal_response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: basic_schema,
        extras: { max_tokens: 200, temperature: 0.4 }
      )

      # Both should work in ideal cases, but healing provides a safety net
      no_heal_structured = no_heal_response.structured_output(auto_heal: false)
      heal_structured = heal_response.structured_output(auto_heal: true)
      
      expect(no_heal_structured).to be_a(Hash)
      expect(heal_structured).to be_a(Hash)
      
      expect(no_heal_structured["name"]).to include("Michael")
      expect(heal_structured["name"]).to include("Michael")
      expect(no_heal_structured["age"]).to eq(40)
      expect(heal_structured["age"]).to eq(40)
      
      puts "No heal content: #{no_heal_response.content}"
      puts "With heal content: #{heal_response.content}"
      puts "No heal parsed: #{no_heal_structured.inspect}"
      puts "With heal parsed: #{heal_structured.inspect}"
    end
  end

  describe "real-world malformation scenarios", vcr: { cassette_name: 'healing_real_world' } do
    it "handles common JSON malformation patterns from LLM responses" do
      messages = [
        {
          role: "user",
          content: <<~PROMPT
            You need to create a JSON response for contact information.
            
            Create JSON for:
            - Name: Dr. Jane Smith-Johnson
            - Age: 42
            - Email: jane.smith+work@company-name.co.uk
            
            Common mistakes to avoid:
            1. Missing quotes around strings
            2. Using single quotes instead of double
            3. Trailing commas
            4. Unescaped quotes in values
            
            Return ONLY valid JSON matching the required schema.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: basic_schema,
        extras: { max_tokens: 300, temperature: 0.7 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to include("Jane")
      expect(structured["name"]).to include("Smith")
      expect(structured["age"]).to eq(42)
      
      if structured["email"]
        expect(structured["email"]).to include("jane.smith")
        expect(structured["email"]).to include("@")
      end
      
      puts "Real-world scenario content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end

  describe "healing validation with json-schema", vcr: { cassette_name: 'healing_schema_validation' } do
    it "heals responses that fail schema validation" do
      # Skip if json-schema gem not available
      skip "json-schema gem not available" unless defined?(JSON::Validator)
      
      # Create a schema that's likely to catch type errors
      strict_schema = OpenRouter::Schema.define("strict_person") do
        string :name, required: true
        integer :age, required: true  # Must be integer, not string
        string :status, required: true # Must be present
      end

      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Create JSON for a person with:
            - name: "Alice Cooper"
            - age: 35 (make sure this is a number, not a string!)
            - status: "active"
            
            Common error would be: {"name": "Alice Cooper", "age": "35", "status": "active"}
            But age should be numeric, not string.
            
            Provide correct JSON with proper types.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: strict_schema,
        extras: { max_tokens: 200, temperature: 0.5 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to eq("Alice Cooper")
      expect(structured["age"]).to be_a(Integer)
      expect(structured["age"]).to eq(35)
      expect(structured["status"]).to eq("active")
      
      # Test validation
      expect(response.valid_structured_output?).to be true
      expect(response.validation_errors).to be_empty
      
      puts "Schema validation content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
      puts "Validation status: #{response.valid_structured_output?}"
    end
  end

  describe "healing with different temperature settings", vcr: { cassette_name: 'healing_temperature_test' } do
    it "tests healing with high temperature responses" do
      messages = [
        {
          role: "user",
          content: <<~PROMPT
            Generate creative JSON for a fictional character:
            - name: Something creative with special characters
            - age: Random age between 20-60
            
            Be creative but ensure valid JSON syntax.
            Special characters in names are okay if properly escaped.
          PROMPT
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: basic_schema,
        extras: { max_tokens: 300, temperature: 1.0 } # Maximum creativity
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to be_a(String)
      expect(structured["name"]).not_to be_empty
      expect(structured["age"]).to be_a(Integer)
      expect(structured["age"]).to be_between(20, 60)
      
      puts "High temperature content: #{response.content}"
      puts "Parsed structure: #{structured.inspect}"
    end
  end
end

================
File: spec/vcr/simple_healing_test.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Simple Healing Test", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV.fetch('OPENROUTER_API_KEY')) do |config|
      config.auto_heal_responses = true
      config.healer_model = "openai/gpt-4o-mini"
      config.max_heal_attempts = 2
    end
  end

  let(:basic_schema) do
    OpenRouter::Schema.define("person") do
      string :name, required: true
      integer :age, required: true
    end
  end

  describe "basic structured output", vcr: { cassette_name: 'simple_structured_output' } do
    it "generates clean structured output" do
      messages = [
        {
          role: "user",
          content: "Create JSON for a person named Alice who is 25 years old. Use proper JSON format."
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-2024-08-06", # Model that supports structured outputs
        response_format: basic_schema,
        extras: { max_tokens: 200, temperature: 0.3 }
      )

      structured = response.structured_output(auto_heal: true)
      
      expect(structured).to be_a(Hash)
      expect(structured["name"]).to eq("Alice")
      expect(structured["age"]).to eq(25)
      
      puts "Content: #{response.content}"
      puts "Parsed: #{structured.inspect}"
    end
  end

  describe "basic completion without structured output", vcr: { cassette_name: 'simple_basic_completion' } do
    it "works with basic completion" do
      messages = [
        {
          role: "user",
          content: "Say hello"
        }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        extras: { max_tokens: 50 }
      )
      
      expect(response.content).to be_a(String)
      expect(response.content).not_to be_empty
      
      puts "Basic completion: #{response.content}"
    end
  end
end

================
File: spec/vcr/structured_outputs_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter Structured Outputs", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])
  end

  # Simple schema for testing
  let(:simple_schema) do
    OpenRouter::Schema.define("simple_response") do
      string "message", required: true, description: "A simple message"
      integer "count", required: false, description: "An optional count"
    end
  end

  # Complex schema with nested objects
  let(:complex_schema) do
    OpenRouter::Schema.define("analysis_result") do
      string "summary", required: true, description: "Summary of the analysis"
      object "details", required: true, description: "Detailed analysis" do
        string "category", required: true, description: "Category of analysis"
        number "confidence", required: true, description: "Confidence score 0-1"
        array "keywords", required: false, description: "Relevant keywords" do
          string description: "Individual keyword"
        end
      end
      boolean "requires_followup", required: false, description: "Whether followup is needed"
    end
  end

  # Schema with array of objects
  let(:array_schema) do
    OpenRouter::Schema.define("task_list") do
      array "tasks", required: true, description: "List of tasks" do
        object do
          string "title", required: true, description: "Task title"
          string "priority", required: true, description: "Priority level"
          boolean "completed", required: false, description: "Whether task is completed"
        end
      end
      integer "total_count", required: true, description: "Total number of tasks"
    end
  end

  describe "simple structured output", vcr: { cassette_name: 'structured_outputs_simple' } do
    it "returns valid JSON matching the schema" do
      messages = [
        { role: "user", content: "Give me a simple greeting message with count 5" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      expect(response).to be_a(OpenRouter::Response)
      expect(response.content).to be_a(String)

      # Test structured output parsing
      structured = response.structured_output
      expect(structured).to be_a(Hash)
      expect(structured["message"]).to be_a(String)
      expect(structured["count"]).to be_a(Integer) if structured["count"]
      
      # Test that it matches the expected schema structure
      expect(structured).to have_key("message")
    end
  end

  describe "complex nested structured output", vcr: { cassette_name: 'structured_outputs_complex' } do
    it "handles nested objects and arrays correctly" do
      messages = [
        { role: "user", content: "Analyze the text 'Ruby is a programming language' and provide detailed analysis" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: complex_schema,
        extras: { max_tokens: 1000 }
      )

      structured = response.structured_output
      expect(structured).to be_a(Hash)

      # Check required fields
      expect(structured["summary"]).to be_a(String)
      expect(structured["details"]).to be_a(Hash)

      # Check nested object
      details = structured["details"]
      expect(details["category"]).to be_a(String)
      expect(details["confidence"]).to be_a(Numeric)
      expect(details["confidence"]).to be_between(0, 1)

      # Check optional array
      if details["keywords"]
        expect(details["keywords"]).to be_an(Array)
        details["keywords"].each do |keyword|
          expect(keyword).to be_a(String)
        end
      end

      # Check optional boolean
      if structured["requires_followup"]
        expect([true, false]).to include(structured["requires_followup"])
      end
    end
  end

  describe "array of objects schema", vcr: { cassette_name: 'structured_outputs_array_objects' } do
    it "correctly structures arrays of complex objects" do
      messages = [
        { role: "user", content: "Create a task list with 3 tasks: writing documentation (high priority), testing code (medium priority), and reviewing pull request (low priority)" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: array_schema,
        extras: { max_tokens: 1000 }
      )

      structured = response.structured_output
      expect(structured).to be_a(Hash)

      # Check main structure
      expect(structured["tasks"]).to be_an(Array)
      expect(structured["total_count"]).to be_a(Integer)
      expect(structured["total_count"]).to eq(structured["tasks"].length)

      # Check each task object
      structured["tasks"].each do |task|
        expect(task).to be_a(Hash)
        expect(task["title"]).to be_a(String)
        expect(task["priority"]).to be_a(String)
        expect(["high", "medium", "low"]).to include(task["priority"])
        
        if task.key?("completed")
          expect([true, false]).to include(task["completed"])
        end
      end
    end
  end

  describe "hash-based response format", vcr: { cassette_name: 'structured_outputs_hash_format' } do
    it "works with hash-based response format specification" do
      response_format = {
        type: "json_schema",
        json_schema: {
          name: "simple_response",
          strict: true,
          schema: {
            type: "object",
            properties: {
              greeting: { type: "string", description: "A greeting message" },
              timestamp: { type: "string", description: "Current timestamp" }
            },
            required: ["greeting", "timestamp"],
            additionalProperties: false
          }
        }
      }

      messages = [
        { role: "user", content: "Give me a greeting with the current timestamp" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: response_format,
        extras: { max_tokens: 500 }
      )

      structured = response.structured_output
      expect(structured).to be_a(Hash)
      expect(structured["greeting"]).to be_a(String)
    end
  end

  describe "schema validation", vcr: { cassette_name: 'structured_outputs_validation' } do
    # Note: Validation requires json-schema gem which may not be available
    it "validates response against schema when json-schema is available" do
      messages = [
        { role: "user", content: "Give me a simple message 'Hello World' with count 42" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      structured = response.structured_output

      # Test validation if available
      if response.valid_structured_output?
        expect(response.validation_errors).to be_empty
      else
        # If validation isn't available, we can't test it
        puts "JSON Schema validation not available (json-schema gem not loaded)"
      end

      # Basic structural validation regardless
      expect(structured).to be_a(Hash)
      expect(structured).to have_key("message")
    end
  end

  describe "error handling", vcr: { cassette_name: 'structured_outputs_error_handling' } do
    it "handles invalid schema definitions" do
      expect {
        OpenRouter::Schema.new("", {})
      }.to raise_error(ArgumentError, /Schema name is required/)

      expect {
        OpenRouter::Schema.new("test", "not a hash")
      }.to raise_error(ArgumentError, /Schema definition must be a hash/)
    end

    # Note: This test would require a model that returns malformed JSON,
    # which is unlikely with real API responses but good for error handling
    it "handles JSON parsing errors gracefully" do
      # We can't easily test this with real API responses since they should always return valid JSON
      # But we can test the error handling logic directly
      raw_response = {
        "choices" => [
          {
            "message" => {
              "content" => "invalid json content {"
            }
          }
        ]
      }

      response = OpenRouter::Response.new(raw_response, response_format: simple_schema)

      expect {
        response.structured_output
      }.to raise_error(OpenRouter::StructuredOutputError, /Failed to parse structured output/)
    end
  end

  describe "schema builder DSL", vcr: { cassette_name: 'structured_outputs_dsl' } do
    it "correctly builds schemas using DSL" do
      schema = OpenRouter::Schema.define("test_schema") do
        string "name", required: true, description: "Person's name"
        integer "age", required: false, description: "Person's age"
        boolean "active", required: true, description: "Whether person is active"
        
        array "hobbies", required: false, description: "List of hobbies" do
          string description: "Individual hobby"
        end
        
        object "address", required: true, description: "Address information" do
          string "street", required: true, description: "Street address"
          string "city", required: true, description: "City"
          string "country", required: false, description: "Country"
        end
      end

      schema_hash = schema.to_h
      expect(schema_hash[:name]).to eq("test_schema")
      expect(schema_hash[:strict]).to be true
      expect(schema_hash[:schema][:type]).to eq("object")
      expect(schema_hash[:schema][:properties]).to have_key("name")
      expect(schema_hash[:schema][:properties]).to have_key("address")
      expect(schema_hash[:schema][:required]).to include("name", "active", "address")
    end
  end

  describe "response metadata with structured outputs", vcr: { cassette_name: 'structured_outputs_metadata' } do
    it "maintains all response metadata while providing structured output" do
      messages = [
        { role: "user", content: "Give me a simple greeting" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      # Test that all normal response fields are present
      expect(response.id).to be_a(String)
      expect(response.object).to eq("chat.completion")
      expect(response.created).to be_a(Integer)
      expect(response.model).to include("gpt-4o-mini")
      expect(response.usage).to be_a(Hash)
      expect(response.usage["prompt_tokens"]).to be > 0
      expect(response.usage["completion_tokens"]).to be > 0
      expect(response.usage["total_tokens"]).to be > 0

      # Test that structured output is also available
      expect(response.structured_output).to be_a(Hash)
      expect(response.content).to be_a(String)

      # Test backward compatibility
      expect(response["id"]).to eq(response.id)
      expect(response["usage"]).to eq(response.usage)
    end
  end

  describe "strict vs non-strict schemas", vcr: { cassette_name: 'structured_outputs_strict_mode' } do
    let(:non_strict_schema) do
      OpenRouter::Schema.define("flexible_response", strict: false) do
        string "message", required: true, description: "A message"
        additional_properties(true)
      end
    end

    it "handles non-strict schemas allowing additional properties" do
      messages = [
        { role: "user", content: "Give me a message and any additional information you think is relevant" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: non_strict_schema,
        extras: { max_tokens: 500 }
      )

      structured = response.structured_output
      expect(structured).to be_a(Hash)
      expect(structured["message"]).to be_a(String)

      # May have additional properties beyond what's defined in schema
      # This is allowed in non-strict mode
    end
  end

  describe "schema serialization for API", vcr: { cassette_name: 'structured_outputs_serialization' } do
    it "properly serializes schemas for the OpenRouter API" do
      messages = [
        { role: "user", content: "Give me a simple message" }
      ]

      # Test Schema object as response_format
      response1 = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      # Test hash with Schema object as response_format
      response2 = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: { type: "json_schema", json_schema: simple_schema },
        extras: { max_tokens: 500 }
      )

      # Both should work and return structured output
      expect(response1.structured_output).to be_a(Hash)
      expect(response2.structured_output).to be_a(Hash)
    end
  end

  describe "integration with different models", vcr: { cassette_name: 'structured_outputs_different_models' } do
    it "works with different models that support structured outputs" do
      messages = [
        { role: "user", content: "Give me a simple greeting message" }
      ]

      # Test with GPT-4o-mini
      response_gpt = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        response_format: simple_schema,
        extras: { max_tokens: 500 }
      )

      expect(response_gpt.structured_output).to be_a(Hash)
      expect(response_gpt.structured_output["message"]).to be_a(String)

      # Test with other models that support structured outputs
      # Note: Not all models may support structured outputs,
      # so we may need to handle errors gracefully
      begin
        response_claude = client.complete(
          messages,
          model: "anthropic/claude-3-haiku",
          response_format: simple_schema,
          extras: { max_tokens: 500 }
        )
        
        if response_claude.structured_output
          expect(response_claude.structured_output).to be_a(Hash)
        end
      rescue OpenRouter::ServerError => e
        # Some models may not support structured outputs
        puts "Claude model may not support structured outputs: #{e.message}"
      end
    end
  end
end

================
File: spec/vcr/tool_calling_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "OpenRouter Tool Calling", :vcr do
  let(:client) do
    OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])
  end

  # Simple calculator tool for testing
  let(:calculator_tool) do
    OpenRouter::Tool.define do
      name "calculator"
      description "Perform basic arithmetic operations"
      parameters do
        string "operation", required: true, description: "The operation to perform: add, subtract, multiply, divide"
        number "a", required: true, description: "First number"
        number "b", required: true, description: "Second number"
      end
    end
  end

  # Weather tool for testing
  let(:weather_tool) do
    OpenRouter::Tool.define do
      name "get_weather"
      description "Get current weather for a location"
      parameters do
        string "location", required: true, description: "City name"
        string "units", required: false, description: "Temperature units (celsius/fahrenheit)"
      end
    end
  end

  # Hash-based tool definition
  let(:hash_tool) do
    {
      type: "function",
      function: {
        name: "search_database",
        description: "Search for information in the database",
        parameters: {
          type: "object",
          properties: {
            query: {
              type: "string",
              description: "Search query"
            },
            limit: {
              type: "integer",
              description: "Maximum number of results"
            }
          },
          required: ["query"]
        }
      }
    }
  end

  describe "single tool call", vcr: { cassette_name: 'tool_calling_single_tool' } do
    it "successfully makes a tool call with DSL-defined tool" do
      messages = [
        { role: "user", content: "What is 15 + 27?" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response).to be_a(OpenRouter::Response)
      expect(response.has_tool_calls?).to be true
      expect(response.tool_calls.length).to eq(1)

      tool_call = response.tool_calls.first
      expect(tool_call).to be_a(OpenRouter::ToolCall)
      expect(tool_call.function_name).to eq("calculator")
      expect(tool_call.id).to be_a(String)
      expect(tool_call.type).to eq("function")

      # Check arguments
      args = tool_call.arguments
      expect(args).to be_a(Hash)
      expect(args["operation"]).to eq("add")
      expect([15, 15.0]).to include(args["a"])
      expect([27, 27.0]).to include(args["b"])
    end

    it "works with hash-defined tools" do
      messages = [
        { role: "user", content: "Search for 'ruby programming' in the database" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [hash_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first
      expect(tool_call.function_name).to eq("search_database")

      args = tool_call.arguments
      expect(args["query"]).to include("ruby")
    end
  end

  describe "multiple tools available", vcr: { cassette_name: 'tool_calling_multiple_tools' } do
    it "chooses appropriate tool from multiple options" do
      messages = [
        { role: "user", content: "What's the weather like in San Francisco?" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool, weather_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first
      expect(tool_call.function_name).to eq("get_weather")

      args = tool_call.arguments
      expect(args["location"]).to include("San Francisco")
    end
  end

  describe "tool choice parameter", vcr: { cassette_name: 'tool_calling_tool_choice' } do
    it "respects specific tool choice" do
      messages = [
        { role: "user", content: "Calculate something for me" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool, weather_tool],
        tool_choice: { type: "function", function: { name: "calculator" } },
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first
      expect(tool_call.function_name).to eq("calculator")
    end

    it "respects required tool choice" do
      messages = [
        { role: "user", content: "Hello there" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "required",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
    end

    it "respects none tool choice" do
      messages = [
        { role: "user", content: "What is 5 + 5?" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "none",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be false
      expect(response.content).to be_a(String)
    end
  end

  describe "tool execution and conversation continuation", vcr: { cassette_name: 'tool_calling_execution_continuation' } do
    it "completes full tool calling workflow" do
      # Initial message
      messages = [
        { role: "user", content: "What is 25 * 4?" }
      ]

      # First request - should trigger tool call
      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first

      # Execute the tool call
      tool_result = tool_call.execute do |name, args|
        case name
        when "calculator"
          a, b = args["a"], args["b"]
          case args["operation"]
          when "add"
            a + b
          when "subtract"
            a - b
          when "multiply"
            a * b
          when "divide"
            a / b
          end
        end
      end

      expect(tool_result).to be_a(OpenRouter::ToolResult)
      expect(tool_result.success?).to be true
      expect(tool_result.result).to eq(100)

      # Continue conversation with tool result
      updated_messages = messages + [
        response.to_message,
        tool_result.to_message
      ]

      final_response = client.complete(
        updated_messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        extras: { max_tokens: 500 }
      )

      expect(final_response.has_content?).to be true
      expect(final_response.content.downcase).to include("100")
    end
  end

  describe "multiple tool calls in one response", vcr: { cassette_name: 'tool_calling_multiple_calls' } do
    it "handles multiple tool calls when requested" do
      messages = [
        { role: "user", content: "Calculate 10 + 5 and also 20 * 3" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      # May have multiple tool calls or one call that handles both
      expect(response.has_tool_calls?).to be true
      expect(response.tool_calls.length).to be >= 1

      # Verify each tool call is valid
      response.tool_calls.each do |tool_call|
        expect(tool_call.function_name).to eq("calculator")
        expect(tool_call.arguments).to be_a(Hash)
        expect(["add", "multiply"]).to include(tool_call.arguments["operation"])
      end
    end
  end

  describe "tool call serialization", vcr: { cassette_name: 'tool_calling_serialization' } do
    it "properly serializes tool calls to conversation messages" do
      messages = [
        { role: "user", content: "What is 7 + 3?" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      # Test Response.to_message
      message = response.to_message
      expect(message[:role]).to eq("assistant")
      expect(message[:tool_calls]).to be_an(Array)
      expect(message[:tool_calls].length).to be > 0

      # Test ToolCall.to_message
      tool_call = response.tool_calls.first
      tool_message = tool_call.to_message
      expect(tool_message[:role]).to eq("assistant")
      expect(tool_message[:tool_calls]).to be_an(Array)

      # Test tool result message
      result_message = tool_call.to_result_message("Result: 10")
      expect(result_message[:role]).to eq("tool")
      expect(result_message[:tool_call_id]).to eq(tool_call.id)
      expect(result_message[:name]).to eq(tool_call.function_name)
      expect(result_message[:content]).to eq("Result: 10")
    end
  end

  describe "tool call error handling", vcr: { cassette_name: 'tool_calling_error_handling' } do
    it "handles tool execution errors gracefully" do
      messages = [
        { role: "user", content: "Divide 10 by 0" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first

      # Execute with error handling
      tool_result = tool_call.execute do |name, args|
        case name
        when "calculator"
          a, b = args["a"], args["b"]
          if args["operation"] == "divide" && b == 0
            raise "Division by zero error"
          end
          # ... other operations
        end
      end

      expect(tool_result.failure?).to be true
      expect(tool_result.error).to include("Division by zero")
    end

    it "handles malformed tool arguments" do
      # This test would require a response with malformed JSON, 
      # which is unlikely from the real API but worth testing the error handling
      malformed_data = {
        "id" => "test_id",
        "type" => "function",
        "function" => {
          "name" => "calculator",
          "arguments" => "invalid json content"
        }
      }

      expect {
        tool_call = OpenRouter::ToolCall.new(malformed_data)
        tool_call.arguments
      }.to raise_error(OpenRouter::ToolCallError, /Failed to parse tool call arguments/)
    end
  end

  describe "tool validation", vcr: { cassette_name: 'tool_calling_validation' } do
    it "validates tool definitions correctly" do
      expect {
        OpenRouter::Tool.define do
          # Missing name - should raise error
          description "A tool without a name"
        end
      }.to raise_error(ArgumentError, /Function must have a name/)

      expect {
        OpenRouter::Tool.define do
          name "test_tool"
          # Missing description - should raise error
        end
      }.to raise_error(ArgumentError, /Function must have a description/)
    end

    it "validates hash-based tool definitions" do
      invalid_tool = {
        type: "function",
        function: {
          name: "test_tool"
          # Missing description
        }
      }

      expect {
        OpenRouter::Tool.new(invalid_tool)
      }.to raise_error(ArgumentError, /Function must have a description/)
    end
  end

  describe "complex parameter types", vcr: { cassette_name: 'tool_calling_complex_parameters' } do
    let(:complex_tool) do
      OpenRouter::Tool.define do
        name "process_data"
        description "Process complex data structures"
        parameters do
          string "name", required: true, description: "Name of the dataset"
          array "items", required: true, description: "Array of items to process" do
            string description: "Individual item"
          end
          object "config", required: false, description: "Configuration object" do
            boolean "enabled", required: true, description: "Whether processing is enabled"
            integer "max_items", required: false, description: "Maximum items to process"
          end
        end
      end
    end

    it "handles complex parameter structures" do
      messages = [
        { role: "user", content: "Process a dataset called 'test_data' with items ['a', 'b', 'c'] and enable processing with max 100 items" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [complex_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first
      expect(tool_call.function_name).to eq("process_data")

      args = tool_call.arguments
      expect(args["name"]).to be_a(String)
      expect(args["items"]).to be_an(Array)
      
      if args["config"]
        expect(args["config"]).to be_a(Hash)
        expect([true, false]).to include(args["config"]["enabled"])
      end
    end
  end

  describe "response structure validation", vcr: { cassette_name: 'tool_calling_response_structure' } do
    it "validates complete response structure with tool calls" do
      messages = [
        { role: "user", content: "Add 5 and 10" }
      ]

      response = client.complete(
        messages,
        model: "openai/gpt-4o-mini",
        tools: [calculator_tool],
        tool_choice: "auto",
        extras: { max_tokens: 500 }
      )

      # Test Response structure
      expect(response.id).to be_a(String)
      expect(response.object).to eq("chat.completion")
      expect(response.created).to be_a(Integer)
      expect(response.model).to include("gpt-4o-mini")
      expect(response.usage).to be_a(Hash)
      expect(response.usage["prompt_tokens"]).to be > 0

      # Test tool call structure
      expect(response.has_tool_calls?).to be true
      tool_call = response.tool_calls.first

      expect(tool_call.id).to be_a(String)
      expect(tool_call.type).to eq("function")
      expect(tool_call.function_name).to be_a(String)
      expect(tool_call.arguments_string).to be_a(String)
      expect(tool_call.arguments).to be_a(Hash)

      # Test serialization
      expect(tool_call.to_h).to be_a(Hash)
      expect(tool_call.to_json).to be_a(String)

      # Test backward compatibility
      expect(response["choices"]).to be_an(Array)
      choice = response["choices"].first
      expect(choice["message"]["tool_calls"]).to be_an(Array)
    end
  end
end

================
File: spec/client_capability_validation_spec.rb
================
# frozen_string_literal: true

require 'spec_helper'

RSpec.describe 'OpenRouter Client Capability Validation' do
  let(:mock_models) do
    {
      "basic/text-model" => {
        name: "Basic Text Model",
        capabilities: [:chat],
        cost_per_1k_tokens: { input: 0.0005, output: 0.001 },
        context_length: 2048
      },
      "advanced/tool-model" => {
        name: "Advanced Tool Model", 
        capabilities: [:chat, :function_calling],
        cost_per_1k_tokens: { input: 0.001, output: 0.002 },
        context_length: 4096
      },
      "vision/multimodal-model" => {
        name: "Vision Multimodal Model",
        capabilities: [:chat, :vision, :structured_outputs],
        cost_per_1k_tokens: { input: 0.01, output: 0.03 },
        context_length: 8192
      }
    }
  end

  before do
    allow(OpenRouter::ModelRegistry).to receive(:all_models).and_return(mock_models)
    # Mock HTTP to avoid actual API calls
    allow_any_instance_of(OpenRouter::Client).to receive(:post).and_raise(Faraday::UnauthorizedError.new("Mocked error"))
  end

  describe "strict mode disabled (default)" do
    let(:client) do
      OpenRouter::Client.new(access_token: "test_token") do |config|
        config.strict_mode = false
      end
    end

    it "shows warnings for unsupported tool calling but allows request" do
      expect { 
        expect {
          client.complete(
            [{ role: "user", content: "Hello" }],
            model: "basic/text-model",
            tools: [{ name: "test_tool", description: "A test tool" }]
          )
        }.to output(/OpenRouter Warning.*tool calling/).to_stderr
      }.to raise_error(Faraday::UnauthorizedError) # Expected API error, not capability error
    end

    it "shows warnings for unsupported structured outputs but allows request" do
      expect { 
        expect {
          client.complete(
            [{ role: "user", content: "Hello" }],
            model: "basic/text-model",
            response_format: { type: "json_schema", json_schema: { name: "test" } }
          )
        }.to output(/OpenRouter Warning.*structured outputs/).to_stderr
      }.to raise_error(Faraday::UnauthorizedError)
    end

    it "shows warnings for unsupported vision but allows request" do
      expect { 
        expect {
          client.complete([
            { 
              role: "user", 
              content: [
                { type: "text", text: "What's in this image?" },
                { type: "image_url", image_url: { url: "data:image/png;base64,abc123" } }
              ] 
            }
          ], model: "basic/text-model")
        }.to output(/OpenRouter Warning.*vision/).to_stderr
      }.to raise_error(Faraday::UnauthorizedError)
    end

    it "doesn't show warnings when model supports the feature" do
      expect {
        expect {
          client.complete(
            [{ role: "user", content: "Hello" }],
            model: "advanced/tool-model",
            tools: [{ name: "test_tool", description: "A test tool" }]
          )
        }.not_to output.to_stderr
      }.to raise_error(Faraday::UnauthorizedError) # Only API error, no warning
    end
  end

  describe "strict mode enabled" do
    let(:client) do
      OpenRouter::Client.new(access_token: "test_token") do |config|
        config.strict_mode = true
      end
    end

    it "raises CapabilityError for unsupported tool calling" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "basic/text-model",
          tools: [{ name: "test_tool", description: "A test tool" }]
        )
      }.to raise_error(OpenRouter::CapabilityError, /tool calling.*missing :function_calling/)
    end

    it "raises CapabilityError for unsupported structured outputs" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "basic/text-model",
          response_format: { type: "json_schema", json_schema: { name: "test" } }
        )
      }.to raise_error(OpenRouter::CapabilityError, /structured outputs.*missing :structured_outputs/)
    end

    it "raises CapabilityError for unsupported vision" do
      expect {
        client.complete([
          { 
            role: "user", 
            content: [
              { type: "text", text: "What's in this image?" },
              { type: "image_url", image_url: { url: "data:image/png;base64,abc123" } }
            ] 
          }
        ], model: "basic/text-model")
      }.to raise_error(OpenRouter::CapabilityError, /vision.*missing :vision/)
    end

    it "allows requests when model supports the feature" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "advanced/tool-model",
          tools: [{ name: "test_tool", description: "A test tool" }]
        )
      }.to raise_error(Faraday::UnauthorizedError) # API error, not capability error
    end

    it "doesn't raise for array models (fallbacks)" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: ["basic/text-model", "advanced/tool-model"],
          tools: [{ name: "test_tool", description: "A test tool" }]
        )
      }.to raise_error(Faraday::UnauthorizedError) # Should not raise CapabilityError
    end

    it "doesn't raise for auto model selection" do
      expect {
        client.complete(
          [{ role: "user", content: "Hello" }],
          model: "openrouter/auto",
          tools: [{ name: "test_tool", description: "A test tool" }]
        )
      }.to raise_error(Faraday::UnauthorizedError) # Should not raise CapabilityError
    end
  end

  describe "configuration" do
    it "defaults strict_mode to false" do
      config = OpenRouter::Configuration.new
      expect(config.strict_mode).to be false
    end

    it "can be configured via environment variable" do
      ENV['OPENROUTER_STRICT_MODE'] = 'true'
      config = OpenRouter::Configuration.new
      expect(config.strict_mode).to be true
      ENV.delete('OPENROUTER_STRICT_MODE')
    end

    it "can be configured programmatically" do
      OpenRouter.configure do |config|
        config.strict_mode = true
      end
      expect(OpenRouter.configuration.strict_mode).to be true
      
      # Reset for other tests
      OpenRouter.configure { |config| config.strict_mode = false }
    end
  end
end

================
File: spec/client_integration_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::Client do
  let(:client) do
    OpenRouter::Client.new(access_token: "test_token")
  end

  let(:mock_response) do
    {
      "id" => "chatcmpl-123",
      "object" => "chat.completion",
      "created" => 1_677_652_288,
      "model" => "gpt-3.5-turbo",
      "choices" => [
        {
          "index" => 0,
          "message" => {
            "role" => "assistant",
            "content" => "Hello! How can I help you today?"
          },
          "finish_reason" => "stop"
        }
      ],
      "usage" => {
        "prompt_tokens" => 12,
        "completion_tokens" => 8,
        "total_tokens" => 20
      }
    }
  end

  describe "#complete with tools" do
    let(:search_tool) do
      OpenRouter::Tool.define do
        name "search_books"
        description "Search for books"
        parameters do
          string :query, required: true, description: "Search query"
          integer :limit, description: "Maximum results"
        end
      end
    end

    let(:tool_response) do
      mock_response.merge(
        "choices" => [
          {
            "index" => 0,
            "message" => {
              "role" => "assistant",
              "content" => nil,
              "tool_calls" => [
                {
                  "id" => "call_abc123",
                  "type" => "function",
                  "function" => {
                    "name" => "search_books",
                    "arguments" => '{"query": "Ruby programming", "limit": 5}'
                  }
                }
              ]
            },
            "finish_reason" => "tool_calls"
          }
        ]
      )
    end

    it "includes tools in the request" do
      expect(client).to receive(:post).with(
        path: "/chat/completions",
        parameters: hash_including(
          tools: [search_tool.to_h],
          tool_choice: "auto"
        )
      ).and_return(mock_response)

      messages = [{ role: "user", content: "Search for Ruby books" }]
      client.complete(messages, tools: [search_tool], tool_choice: "auto")
    end

    it "handles Tool objects and hashes" do
      tool_hash = {
        type: "function",
        function: {
          name: "test_tool",
          description: "A test tool",
          parameters: { type: "object", properties: {} }
        }
      }

      expect(client).to receive(:post).with(
        path: "/chat/completions",
        parameters: hash_including(
          tools: [search_tool.to_h, tool_hash]
        )
      ).and_return(mock_response)

      messages = [{ role: "user", content: "Test" }]
      client.complete(messages, tools: [search_tool, tool_hash])
    end

    it "returns Response object with tool calls" do
      allow(client).to receive(:post).and_return(tool_response)

      messages = [{ role: "user", content: "Search for Ruby books" }]
      response = client.complete(messages, tools: [search_tool])

      expect(response).to be_a(OpenRouter::Response)
      expect(response.has_tool_calls?).to be true
      expect(response.tool_calls.first.name).to eq("search_books")
    end
  end

  describe "#complete with structured outputs" do
    let(:weather_schema) do
      OpenRouter::Schema.define("weather") do
        string :location, required: true, description: "City name"
        number :temperature, required: true, description: "Temperature"
        string :conditions, required: true, description: "Weather conditions"
      end
    end

    let(:structured_response) do
      mock_response.merge(
        "choices" => [
          {
            "index" => 0,
            "message" => {
              "role" => "assistant",
              "content" => '{"location": "London", "temperature": 18, "conditions": "Partly cloudy"}'
            },
            "finish_reason" => "stop"
          }
        ]
      )
    end

    it "includes response_format in the request with Schema object" do
      expect(client).to receive(:post).with(
        path: "/chat/completions",
        parameters: hash_including(
          response_format: {
            type: "json_schema",
            json_schema: weather_schema.to_h
          }
        )
      ).and_return(mock_response)

      messages = [{ role: "user", content: "What's the weather in London?" }]
      client.complete(messages, response_format: weather_schema)
    end

    it "includes response_format in the request with hash format" do
      response_format = {
        type: "json_schema",
        json_schema: weather_schema
      }

      expect(client).to receive(:post).with(
        path: "/chat/completions",
        parameters: hash_including(
          response_format: {
            type: "json_schema",
            json_schema: weather_schema.to_h
          }
        )
      ).and_return(mock_response)

      messages = [{ role: "user", content: "What's the weather in London?" }]
      client.complete(messages, response_format:)
    end

    it "returns Response object with structured output" do
      allow(client).to receive(:post).and_return(structured_response)

      messages = [{ role: "user", content: "What's the weather in London?" }]
      response = client.complete(messages, response_format: weather_schema)

      expect(response).to be_a(OpenRouter::Response)
      expect(response.structured_output).to be_a(Hash)
      expect(response.structured_output["location"]).to eq("London")
    end
  end

  describe "#complete backward compatibility" do
    it "maintains backward compatibility for basic requests" do
      allow(client).to receive(:post).and_return(mock_response)

      messages = [{ role: "user", content: "Hello" }]
      response = client.complete(messages)

      expect(response).to be_a(OpenRouter::Response)
      expect(response.content).to eq("Hello! How can I help you today?")

      # Should still work as hash for backward compatibility
      expect(response["id"]).to eq("chatcmpl-123")
      expect(response.dig("choices", 0, "message", "content")).to eq("Hello! How can I help you today?")
    end

    it "handles extras parameter" do
      expect(client).to receive(:post).with(
        path: "/chat/completions",
        parameters: hash_including(
          max_tokens: 100,
          temperature: 0.7
        )
      ).and_return(mock_response)

      messages = [{ role: "user", content: "Hello" }]
      client.complete(messages, extras: { max_tokens: 100, temperature: 0.7 })
    end
  end

  describe "error handling" do
    it "raises ServerError for API errors" do
      error_response = {
        "error" => {
          "message" => "Invalid API key",
          "type" => "invalid_request_error"
        }
      }

      allow(client).to receive(:post).and_return(error_response)

      expect do
        client.complete([{ role: "user", content: "Hello" }])
      end.to raise_error(OpenRouter::ServerError, "Invalid API key")
    end

    it "raises ServerError for empty responses" do
      allow(client).to receive(:post).and_return(nil)

      expect do
        client.complete([{ role: "user", content: "Hello" }])
      end.to raise_error(OpenRouter::ServerError, /Empty response/)
    end
  end

  describe "parameter validation" do
    it "validates tool parameters" do
      expect do
        client.complete(
          [{ role: "user", content: "Hello" }],
          tools: ["invalid tool"]
        )
      end.to raise_error(ArgumentError, /Tools must be/)
    end
  end
end

================
File: spec/contract_tests_spec.rb
================
# frozen_string_literal: true

# Contract tests for external API assumptions

RSpec.describe "OpenRouter API Contract Tests" do
  describe "API response structure contracts" do
    let(:sample_model) do
      JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))["data"][0]
    end

    it "validates expected model structure from OpenRouter API" do
      # These tests document our assumptions about the API structure
      expect(sample_model).to have_key("id")
      expect(sample_model).to have_key("name")
      expect(sample_model).to have_key("context_length")
      expect(sample_model).to have_key("pricing")
      expect(sample_model).to have_key("supported_parameters")

      # Pricing structure
      pricing = sample_model["pricing"]
      expect(pricing).to have_key("prompt")
      expect(pricing).to have_key("completion")
      expect(pricing["prompt"]).to be_a(String) # API returns as string
      expect(pricing["completion"]).to be_a(String)

      # Context length
      expect(sample_model["context_length"]).to be_a(Numeric)
      expect(sample_model["context_length"]).to be > 0

      # Supported parameters (our capability detection depends on this)
      supported = sample_model["supported_parameters"]
      expect(supported).to be_an(Array)

      # These are the parameters we use for capability detection

      # At least some models should support these
      has_tools = supported.include?("tools") && supported.include?("tool_choice")
      has_response_format = supported.include?("response_format")

      # Document what we expect to find
      puts "✅ Model #{sample_model["id"]} supports function calling" if has_tools

      puts "✅ Model #{sample_model["id"]} supports structured outputs" if has_response_format
    end

    it "validates architecture structure for vision capability detection" do
      architecture = sample_model["architecture"]

      if architecture
        expect(architecture).to have_key("input_modalities")

        input_modalities = architecture["input_modalities"]
        puts "✅ Model #{sample_model["id"]} supports vision (image input)" if input_modalities&.include?("image")
      end
    end

    it "ensures pricing values can be converted to floats" do
      pricing = sample_model["pricing"]

      # Our cost calculations depend on these being convertible to Float
      expect { Float(pricing["prompt"]) }.not_to raise_error
      expect { Float(pricing["completion"]) }.not_to raise_error

      prompt_cost = Float(pricing["prompt"])
      completion_cost = Float(pricing["completion"])

      expect(prompt_cost).to be >= 0
      expect(completion_cost).to be >= 0
    end

    it "validates created timestamp format" do
      created = sample_model["created"]
      expect(created).to be_a(Numeric)
      expect(created).to be > 0

      # Should be a reasonable Unix timestamp (after year 2000)
      expect(created).to be > 946_684_800 # Jan 1, 2000

      # Should be before far future (year 2100)
      expect(created).to be < 4_102_444_800 # Jan 1, 2100
    end
  end

  describe "ModelRegistry assumptions validation" do
    before do
      fixture_data = JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))
      allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(fixture_data)
      OpenRouter::ModelRegistry.clear_cache!
    end

    it "validates our capability detection logic" do
      models = OpenRouter::ModelRegistry.all_models

      models.each do |model_id, specs|
        capabilities = specs[:capabilities]

        # Every model should have basic chat capability
        expect(capabilities).to include(:chat)

        # If we detected function_calling, verify the logic
        if capabilities.include?(:function_calling)
          # This model should support tools in its original data
          original_model = OpenRouter::ModelRegistry.find_original_model_data(model_id)
          supported = original_model&.dig("supported_parameters") || []
          expect(supported).to include("tools").or include("tool_choice")
        end

        # If we detected vision, verify the logic
        next unless capabilities.include?(:vision)

        original_model = OpenRouter::ModelRegistry.find_original_model_data(model_id)
        input_modalities = original_model&.dig("architecture", "input_modalities") || []
        expect(input_modalities).to include("image")
      end
    end

    it "validates performance tier logic is consistent" do
      models = OpenRouter::ModelRegistry.all_models

      premium_models = models.select { |_, specs| specs[:performance_tier] == :premium }
      standard_models = models.select { |_, specs| specs[:performance_tier] == :standard }

      # Premium models should generally be more expensive than standard
      if premium_models.any? && standard_models.any?
        avg_premium_cost = premium_models.values.map { |s| s[:cost_per_1k_tokens][:input] }.sum / premium_models.size
        avg_standard_cost = standard_models.values.map { |s| s[:cost_per_1k_tokens][:input] }.sum / standard_models.size

        # This is a business logic assumption we should validate
        puts "Average premium model cost: $#{avg_premium_cost}"
        puts "Average standard model cost: $#{avg_standard_cost}"

        # Premium should generally cost more (allow some overlap)
        expect(avg_premium_cost).to be >= avg_standard_cost * 0.5
      end
    end
  end
end

================
File: spec/debug_healing_spec.rb
================
# frozen_string_literal: true

require 'spec_helper'
require 'open_router'

RSpec.describe "Debug Healing" do
  let(:client) do
    OpenRouter::Client.new(access_token: "test-key") do |config|
      config.auto_heal_responses = true
      config.healer_model = "openai/gpt-4o-mini"
      config.max_heal_attempts = 2
    end
  end

  let(:malformed_json) { '{"name": "John", age: 30}' }
  let(:valid_json) { '{"name": "John", "age": 30}' }
  let(:basic_schema) { { type: "json_schema", json_schema: { schema: { type: "object" } } } }

  it "debugs the healing flow" do
    # Create response with malformed JSON
    response = OpenRouter::Response.new(
      { "choices" => [{ "message" => { "content" => malformed_json } }] },
      response_format: basic_schema
    )
    response.client = client

    # Check if structured output is expected
    puts "Structured output expected: #{response.send(:structured_output_expected?)}"
    puts "Has content: #{response.has_content?}"
    puts "Content: #{response.content}"
    puts "Client present: #{!response.client.nil?}"
    puts "Auto heal enabled: #{response.client.configuration.auto_heal_responses}"

    # Mock the healing response
    healed_response = double("Response", content: valid_json)
    expect(client).to receive(:complete).and_return(healed_response)

    begin
      result = response.structured_output(auto_heal: true)
      puts "Result: #{result.inspect}"
      expect(result).to eq({ "name" => "John", "age" => 30 })
    rescue => e
      puts "Error: #{e.class} - #{e.message}"
      puts "Backtrace: #{e.backtrace.first(5).join("\n")}"
      raise e
    end
  end
end

================
File: spec/force_structured_output_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Force structured output on unsupported models" do
  let(:schema) do
    OpenRouter::Schema.define("test_user") do
      string :name, required: true
      integer :age, required: true
    end
  end
  
  let(:response_format) do
    {
      type: "json_schema",
      json_schema: schema
    }
  end

  let(:messages) { [{ role: "user", content: "Create a user" }] }

  # Mock model data for testing
  before do
    allow(OpenRouter::ModelRegistry).to receive(:has_capability?) do |model, capability|
      case [model, capability]
      when ["supported-model", :structured_outputs]
        true
      when ["unsupported-model", :structured_outputs]
        false
      else
        false
      end
    end
  end

  describe "Client#complete" do
    let(:client) { OpenRouter::Client.new(access_token: "test") }

    context "with force_structured_output: true" do
      it "does NOT send response_format to API" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).not_to have_key(:response_format)
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "any-model", response_format: response_format, force_structured_output: true)
      end

      it "injects schema instructions into messages" do
        injected_messages = nil
        expect(client).to receive(:post) do |path:, parameters:|
          injected_messages = parameters[:messages]
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "any-model", response_format: response_format, force_structured_output: true)
        
        expect(injected_messages).to have_attributes(size: 2)
        expect(injected_messages.last[:role]).to eq("system")
        expect(injected_messages.last[:content]).to include("JSON")
        expect(injected_messages.last[:content]).to include("schema")
      end

      it "returns Response with forced_extraction flag" do
        allow(client).to receive(:post).and_return({ "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] })
        
        response = client.complete(messages, model: "any-model", response_format: response_format, force_structured_output: true)
        
        expect(response).to be_a(OpenRouter::Response)
        expect(response.instance_variable_get(:@forced_extraction)).to be true
      end

      it "warns about forcing on any model" do
        allow(client).to receive(:post).and_return({ "choices" => [{ "message" => { "content" => '{}' } }] })
        
        # When explicitly forcing, no warning is expected (it's intentional)
        expect { 
          client.complete(messages, model: "unsupported-model", response_format: response_format, force_structured_output: true)
        }.not_to output(/warning/i).to_stderr
      end
    end

    context "with force_structured_output: false" do
      it "sends response_format to API normally" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).to have_key(:response_format)
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "supported-model", response_format: response_format, force_structured_output: false)
      end

      it "does not inject schema instructions" do
        original_messages = messages.dup
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters[:messages]).to eq(original_messages)
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "supported-model", response_format: response_format, force_structured_output: false)
      end
    end

    context "with force_structured_output: nil (auto-detect)" do
      it "auto-forces when model lacks structured_outputs capability" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).not_to have_key(:response_format)  # Should be forced
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        expect { 
          client.complete(messages, model: "unsupported-model", response_format: response_format)
        }.to output(/doesn't support native structured outputs.*Automatically using forced extraction/).to_stderr
      end

      it "uses native format when model supports structured outputs" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).to have_key(:response_format)  # Should use native
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "supported-model", response_format: response_format)
      end

      it "skips auto-detection for model arrays (fallbacks)" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).to have_key(:response_format)  # Should use native
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: ["unsupported-model", "supported-model"], response_format: response_format)
      end

      it "skips auto-detection for openrouter/auto" do
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters).to have_key(:response_format)  # Should use native
          { "choices" => [{ "message" => { "content" => '{"name": "John", "age": 30}' } }] }
        end

        client.complete(messages, model: "openrouter/auto", response_format: response_format)
      end
    end

    context "without response_format" do
      it "does not force or modify anything" do
        original_messages = messages.dup
        expect(client).to receive(:post) do |path:, parameters:|
          expect(parameters[:messages]).to eq(original_messages)
          expect(parameters).not_to have_key(:response_format)
          { "choices" => [{ "message" => { "content" => "Regular response" } }] }
        end

        client.complete(messages, model: "unsupported-model")
      end
    end
  end

  describe "Response#structured_output with forced extraction" do
    context "with JSON in markdown code blocks" do
      let(:response_content) do
        <<~CONTENT
          Here's the user data you requested:

          ```json
          {"name": "John", "age": 30}
          ```

          This represents a typical user profile.
        CONTENT
      end
      
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => response_content}}]}, response_format: response_format, forced_extraction: true) }

      it "extracts JSON from markdown code blocks" do
        result = response.structured_output
        expect(result).to eq({"name" => "John", "age" => 30})
      end

      it "works in gentle mode" do
        result = response.structured_output(mode: :gentle)
        expect(result).to eq({"name" => "John", "age" => 30})
      end
    end

    context "with JSON in plain text response" do
      let(:response_content) { '{"name": "Alice", "age": 25}' }
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => response_content}}]}, response_format: response_format, forced_extraction: true) }

      it "extracts JSON from plain text response" do
        result = response.structured_output
        expect(result).to eq({"name" => "Alice", "age" => 25})
      end
    end

    context "with malformed JSON requiring healing" do
      let(:response_content) do
        <<~CONTENT
          ```json
          {"name": "Bob", "age": "twenty-five"}
          ```
        CONTENT
      end
      
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => response_content}}]}, response_format: response_format, forced_extraction: true) }
      let(:mock_client) do
        double("client", configuration: double(auto_heal_responses: true, max_heal_attempts: 2, healer_model: "gpt-3.5-turbo"))
      end

      before do
        response.client = mock_client
      end

      it "sends full response content to first heal attempt" do
        expect(response).to receive(:heal_structured_response) do |content, schema|
          expect(content).to include("```json")  # Full response, not just JSON
          expect(content).to include("twenty-five")
          {"name" => "Bob", "age" => 25}
        end
        
        result = response.structured_output(auto_heal: true)
        expect(result).to eq({"name" => "Bob", "age" => 25})
      end

      it "heals extracted JSON in strict mode without additional validation" do
        # When healing is enabled, the healing process handles validation internally
        # No additional validation should happen after healing succeeds
        expect(response).to receive(:heal_structured_response).and_return({"name" => "Bob", "age" => 25})
        
        result = response.structured_output(mode: :strict, auto_heal: true)
        expect(result).to eq({"name" => "Bob", "age" => 25})
      end

      it "returns extracted JSON without validation in gentle mode" do
        # Gentle mode should not attempt healing
        expect(response).not_to receive(:heal_structured_response)
        
        result = response.structured_output(mode: :gentle)
        expect(result).to eq({"name" => "Bob", "age" => "twenty-five"})  # Valid JSON, no schema validation in gentle mode
      end
    end

    context "with no JSON found in response" do
      let(:response_content) { "I cannot provide that information." }
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => response_content}}]}, response_format: response_format, forced_extraction: true) }

      it "returns nil in gentle mode" do
        result = response.structured_output(mode: :gentle)
        expect(result).to be_nil
      end

      it "attempts healing with full content in strict mode" do
        mock_client = double("client", configuration: double(auto_heal_responses: true, max_heal_attempts: 2, healer_model: "gpt-3.5-turbo"))
        response.client = mock_client
        
        expect(response).to receive(:heal_structured_response) do |content, schema|
          expect(content).to eq(response_content)  # Full response sent to healer
          {"name" => "Generated", "age" => 0}
        end
        
        result = response.structured_output(auto_heal: true)
        expect(result).to eq({"name" => "Generated", "age" => 0})
      end
    end

    context "without forced_extraction flag" do
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '```json\n{"name": "John"}\n```'}}]}, response_format: response_format) }

      it "does not extract from markdown blocks in normal mode" do
        # Should look for structured output in standard location, not extract from markdown
        result = response.structured_output
        expect(result).to be_nil
      end
    end
  end

  describe "schema instruction injection" do
    let(:client) { OpenRouter::Client.new(access_token: "test") }
    
    it "creates clear format instructions" do
      allow(client).to receive(:post).and_return({ "choices" => [{ "message" => { "content" => '{}' } }] })
      
      injected_messages = nil
      expect(client).to receive(:post) do |path:, parameters:|
        injected_messages = parameters[:messages]
        { "choices" => [{ "message" => { "content" => '{}' } }] }
      end

      client.complete(messages, model: "any-model", response_format: response_format, force_structured_output: true)
      
      instruction = injected_messages.last[:content]
      expect(instruction).to include("JSON")
      expect(instruction).to include("schema")
      expect(instruction).to include(schema.to_h.to_json)
      expect(instruction).to include("ONLY")  # Emphasize only JSON response
    end
  end
end

================
File: spec/healing_with_errors_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Self-healing with detailed error context" do
  let(:schema) do
    OpenRouter::Schema.define("detailed_user") do
      string :name, required: true
      integer :age, required: true
      string :email, required: true, format: "email"
      string :status, enum: ["active", "inactive"]
    end
  end
  
  let(:response_format) do
    {
      type: "json_schema",
      json_schema: schema.to_h
    }
  end

  let(:mock_client) do
    double("client", 
      configuration: double(
        auto_heal_responses: true, 
        max_heal_attempts: 3,
        healer_model: "gpt-3.5-turbo"
      )
    )
  end

  let(:healed_json) { '{"name": "John", "age": 30, "email": "john@example.com", "status": "active"}' }

  describe "JSON parse error healing" do
    let(:invalid_json) { '{"name": "John", "age": thirty, "email": "john@example.com"}' }  # Missing quotes around thirty
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => invalid_json}}]}, response_format: response_format) }

    before do
      response.client = mock_client
    end

    it "includes JSON parse errors in healing prompt" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      expect(healing_prompt).to include("Invalid JSON")
      expect(healing_prompt).to include("unexpected token")  # JSON parse error details
      expect(healing_prompt).to include(invalid_json)  # Original content
    end

    it "succeeds after healing with parse error context" do
      allow(mock_client).to receive(:complete).and_return(
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      )

      result = response.structured_output(auto_heal: true)
      expect(result).to eq(JSON.parse(healed_json))
    end
  end

  describe "schema validation error healing" do
    let(:invalid_data) { '{"name": "John", "age": "thirty", "email": "not-an-email", "status": "unknown"}' }
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => invalid_data}}]}, response_format: response_format) }

    before do
      response.client = mock_client
      # Skip if json-schema gem not available
    end

    it "includes specific schema validation errors in healing prompt" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      expect(healing_prompt).to include("Schema validation failed")
      expect(healing_prompt).to include("age")  # Field that failed
      expect(healing_prompt).to include("email")  # Field that failed
      expect(healing_prompt).to include("status")  # Field that failed
      expect(healing_prompt).to include("type")  # Error about wrong type
    end

    it "provides field-level validation errors" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      # Check for specific validation errors
      expect(healing_prompt).to match(/age.*integer.*string/i)  # Age should be integer, got string
      expect(healing_prompt).to match(/email.*format/i)  # Email format violation
      expect(healing_prompt).to match(/status.*enum/i)  # Status not in enum
    end

    it "succeeds more often with detailed error context" do
      allow(mock_client).to receive(:complete).and_return(
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      )

      result = response.structured_output(auto_heal: true)
      expect(result).to eq(JSON.parse(healed_json))
    end
  end

  describe "forced extraction first heal attempt" do
    let(:response_with_explanation) do
      <<~CONTENT
        I'll create a user for you. Here's the JSON data:

        ```json
        {"name": "Bob", "age": "twenty-five", "email": "bob@example.com", "status": "active"}
        ```

        This user has an invalid age format that needs to be corrected.
      CONTENT
    end
    
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => response_with_explanation}}]}, response_format: response_format, forced_extraction: true) }

    before do
      response.client = mock_client
    end

    it "sends full response content to first heal attempt" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      # Should include the full response with explanation
      expect(healing_prompt).to include("I'll create a user for you")
      expect(healing_prompt).to include("```json")
      expect(healing_prompt).to include("This user has an invalid")
      expect(healing_prompt).to include("twenty-five")  # The problematic value
    end

    it "provides context about extraction vs validation" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      expect(healing_prompt).to include("extract")
      expect(healing_prompt).to include("schema")
    end
  end

  describe "subsequent heal attempts with extracted JSON" do
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"name": "Alice", "age": "25", "email": "alice@example.com", "status": "active"}'}}]}, response_format: response_format) }
    
    before do
      response.client = mock_client
      # Mock multiple heal attempts
      allow(mock_client).to receive(:complete).and_return(
        # First attempt still has errors
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"name": "Alice", "age": "still-wrong", "email": "alice@example.com", "status": "active"}'}}]}),
        # Second attempt succeeds
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      )
    end

    it "sends only extracted JSON for subsequent heals" do
      healing_prompts = []
      
      allow(mock_client).to receive(:complete) do |messages, **options|
        healing_prompts << messages.last[:content]
        case healing_prompts.size
        when 1
          # First heal still returns invalid data
          OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"name": "Alice", "age": "still-wrong", "email": "alice@example.com", "status": "active"}'}}]})
        when 2
          # Second heal succeeds
          OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
        end
      end

      response.structured_output(auto_heal: true)
      
      # First heal should get parsed JSON, not full response
      expect(healing_prompts.first).to include('{"name": "Alice"')
      expect(healing_prompts.first).not_to include("I'll create")  # No explanation text
      
      # Second heal should also get just JSON
      expect(healing_prompts.last).to include('{"name": "Alice"')
      expect(healing_prompts.last).to include("still-wrong")  # Previous attempt's output
    end
  end

  describe "maximum heal attempts with error progression" do
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"broken": "json"}'}}]}, response_format: response_format) }
    
    before do
      response.client = mock_client
    end

    it "includes attempt count in final error message" do
      # Mock all healing attempts to fail
      allow(mock_client).to receive(:complete).and_return(
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"still": "broken"}'}}]})
      )

      expect {
        response.structured_output(auto_heal: true)
      }.to raise_error(OpenRouter::StructuredOutputError, /after 3 healing attempts/)
    end

    it "includes last validation errors in final error message" do
      # Mock healing that continues to have validation errors
      allow(mock_client).to receive(:complete).and_return(
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"name": "John", "age": "invalid", "email": "not_an_email_at_all", "status": "wrong"}'}}]})
      )

      expect {
        response.structured_output(auto_heal: true)
      }.to raise_error(OpenRouter::StructuredOutputError, /Last errors:.*(age|status).*(age|status)/)
    end
  end

  describe "healing prompt structure" do
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"name": "Test", "age": "invalid"}'}}]}, response_format: response_format) }
    
    before do
      response.client = mock_client
    end

    it "includes clear sections in healing prompt" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      expect(healing_prompt).to include("Validation Errors:")
      expect(healing_prompt).to include("Original Content to Fix:")
      expect(healing_prompt).to include("Required JSON Schema:")
      expect(healing_prompt).to include("Return ONLY the fixed")
    end

    it "includes the schema in JSON format" do
      healing_prompt = nil
      
      expect(mock_client).to receive(:complete) do |messages, **options|
        healing_prompt = messages.last[:content]
        OpenRouter::Response.new({"choices" => [{"message" => {"content" => healed_json}}]})
      end

      response.structured_output(auto_heal: true)
      
      expect(healing_prompt).to include(schema.to_h.to_json)
    end
  end

  describe "no healing when disabled" do
    let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"broken": json}'}}]}, response_format: response_format) }
    
    before do
      response.client = mock_client
    end

    it "does not attempt healing when auto_heal is false" do
      expect(mock_client).not_to receive(:complete)
      
      expect {
        response.structured_output(auto_heal: false)
      }.to raise_error(OpenRouter::StructuredOutputError)
    end

    it "does not attempt healing in gentle mode" do
      expect(mock_client).not_to receive(:complete)
      
      result = response.structured_output(mode: :gentle, auto_heal: true)
      expect(result).to be_nil
    end
  end
end

================
File: spec/integration_scenarios_spec.rb
================
# frozen_string_literal: true

# Integration scenarios that could be added

RSpec.describe "OpenRouter Integration Scenarios" do
  before do
    # Use fixture data
    fixture_data = JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))
    allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(fixture_data)
    OpenRouter::ModelRegistry.clear_cache!
  end

  describe "Client + ModelSelector integration" do
    let(:client) { OpenRouter::Client.new(access_token: "test_token") }

    it "uses ModelSelector for smart_complete" do
      # Mock the completion response
      allow(client).to receive(:post).and_return({
                                                   "id" => "completion-123",
                                                   "choices" => [{ "message" => { "role" => "assistant", "content" => "Hello!" } }]
                                                 })

      # This would test a theoretical smart_complete method
      # response = client.smart_complete(
      #   messages: [{ role: "user", content: "Hello" }],
      #   requirements: { capabilities: [:function_calling], max_cost: 0.01 }
      # )

      # expect(response).to be_a(OpenRouter::Response)
    end
  end

  describe "Full workflow scenarios" do
    it "handles model selection -> tool creation -> completion flow" do
      # 1. Select appropriate model for function calling
      selector = OpenRouter::ModelSelector.new
                                          .require(:function_calling)
                                          .optimize_for(:cost)

      model = selector.choose
      expect(model).not_to be_nil

      # 2. Create tool for the task
      search_tool = OpenRouter::Tool.define do
        name "search"
        description "Search for information"
        parameters do
          string :query, required: true
        end
      end

      # 3. Mock completion with tool call
      client = OpenRouter::Client.new(access_token: "test_token")
      tool_response = {
        "choices" => [{
          "message" => {
            "role" => "assistant",
            "tool_calls" => [{
              "id" => "call_123",
              "type" => "function",
              "function" => {
                "name" => "search",
                "arguments" => '{"query": "test"}'
              }
            }]
          }
        }]
      }

      allow(client).to receive(:post).and_return(tool_response)

      response = client.complete(
        [{ role: "user", content: "Search for something" }],
        model:,
        tools: [search_tool]
      )

      expect(response.has_tool_calls?).to be true
    end

    it "handles graceful degradation across the stack" do
      # Scenario: very restrictive requirements that need fallback
      selector = OpenRouter::ModelSelector.new
                                          .require(:function_calling, :vision, :structured_outputs)
                                          .within_budget(max_cost: 0.0000001)  # Impossibly low
                                          .min_context(1_000_000)              # Impossibly high

      # Should still find a model via fallback
      model = selector.choose_with_fallback
      expect(model).not_to be_nil

      # The model should still have core requirements (function calling)
      info = OpenRouter::ModelRegistry.get_model_info(model)
      expect(info[:capabilities]).to include(:function_calling)
    end
  end

  describe "Performance edge cases" do
    it "handles large model registries efficiently" do
      # Test with many models to ensure O(n) operations don't become O(n²)
      start_time = Time.now

      10.times do
        OpenRouter::ModelRegistry.find_best_model(capabilities: [:function_calling])
      end

      elapsed = Time.now - start_time
      expect(elapsed).to be < 0.1  # Should be fast even with repeated calls
    end
  end
end

================
File: spec/model_registry_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::ModelRegistry do
  let(:fixture_data) do
    JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))
  end

  before do
    # Mock the HTTP client to return our fixture data
    allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(fixture_data)
    # Clear any cached data
    OpenRouter::ModelRegistry.clear_cache!
  end
  describe ".fetch_and_cache_models" do
    it "fetches models from API and caches them" do
      expect(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).once.and_return(fixture_data)

      models = OpenRouter::ModelRegistry.fetch_and_cache_models
      expect(models).to have_key("mistralai/mistral-medium-3.1")
      expect(models["mistralai/mistral-medium-3.1"]).to have_key(:cost_per_1k_tokens)
      expect(models["mistralai/mistral-medium-3.1"]).to have_key(:capabilities)
    end
  end

  describe ".refresh!" do
    it "clears cache and fetches fresh data" do
      # Load initial data
      OpenRouter::ModelRegistry.all_models

      # Mock new data for refresh
      new_data = { "data" => [{ "id" => "new-model", "pricing" => { "prompt" => "0.001", "completion" => "0.002" } }] }
      allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(new_data)

      OpenRouter::ModelRegistry.refresh!
      models = OpenRouter::ModelRegistry.all_models
      expect(models).to have_key("new-model")
    end
  end

  describe ".find_best_model" do
    context "with no requirements" do
      it "returns the cheapest available model" do
        model, specs = OpenRouter::ModelRegistry.find_best_model
        expect(model).to be_a(String)
        expect(specs).to be_a(Hash)
        expect(specs).to have_key(:cost_per_1k_tokens)
        expect(specs).to have_key(:capabilities)
        # Should return cheapest model (ai21/jamba-mini-1.7 based on fixture data)
        expect(model).to eq("ai21/jamba-mini-1.7")
      end
    end

    context "with capability requirements" do
      it "finds models that support vision" do
        model, specs = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:vision]
        )
        expect(specs[:capabilities]).to include(:vision)
        # Models with image input support vision
        expect(["mistralai/mistral-medium-3.1", "z-ai/glm-4.5v", "openai/gpt-5-chat"]).to include(model)
      end

      it "finds models that support function calling" do
        model, specs = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:function_calling]
        )
        expect(specs[:capabilities]).to include(:function_calling)
        # All models in fixture support tools/tool_choice
        expect(["mistralai/mistral-medium-3.1", "z-ai/glm-4.5v", "ai21/jamba-mini-1.7", "ai21/jamba-large-1.7"]).to include(model)
      end

      it "finds models that support structured outputs" do
        model, specs = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:structured_outputs]
        )
        expect(specs[:capabilities]).to include(:structured_outputs)
        # Should return cheapest model with structured_outputs capability
        # Based on fixture, ai21/jamba-mini-1.7 is the cheapest with response_format support
        expect(model).to eq("ai21/jamba-mini-1.7")
      end

      it "returns nil when no models support required capabilities" do
        result = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:nonexistent_capability]
        )
        expect(result).to be_nil
      end
    end

    context "with cost requirements" do
      it "finds models within max cost limit" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          max_input_cost: 0.01
        )
        expect(specs[:cost_per_1k_tokens][:input]).to be <= 0.01
      end

      it "returns cheapest model when multiple options exist" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          max_input_cost: 0.05
        )

        # Should return the cheapest model that meets requirements
        all_qualifying = OpenRouter::ModelRegistry.models_meeting_requirements(
          max_input_cost: 0.05
        )
        cheapest_cost = all_qualifying.min_by { |_, s| s[:cost_per_1k_tokens][:input] }&.last
        expect(specs[:cost_per_1k_tokens][:input]).to eq(cheapest_cost[:cost_per_1k_tokens][:input])
      end

      it "returns nil when no models are within cost limit" do
        result = OpenRouter::ModelRegistry.find_best_model(
          max_input_cost: 0.0000001 # Unreasonably low cost (lower than fixture models)
        )
        expect(result).to be_nil
      end
    end

    context "with context length requirements" do
      it "finds models with sufficient context length" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          min_context_length: 100_000
        )
        expect(specs[:context_length]).to be >= 100_000
      end

      it "returns nil when no models have sufficient context" do
        result = OpenRouter::ModelRegistry.find_best_model(
          min_context_length: 1_000_000 # Unreasonably high context
        )
        expect(result).to be_nil
      end
    end

    context "with performance tier requirements" do
      it "finds models with specified performance tier" do
        model, specs = OpenRouter::ModelRegistry.find_best_model(
          performance_tier: :standard
        )
        expect(specs[:performance_tier]).to eq(:standard)
        # All fixture models should be standard tier based on their low pricing
        expect(["mistralai/mistral-medium-3.1", "z-ai/glm-4.5v", "ai21/jamba-mini-1.7", "ai21/jamba-large-1.7", "openai/gpt-5-chat"]).to include(model)
      end

      it "accepts models with higher performance tiers" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          performance_tier: :standard
        )
        expect(%i[standard premium]).to include(specs[:performance_tier])
      end
    end

    context "with date-based requirements" do
      it "finds models released after a specific date" do
        # Use a timestamp from early 2024 to filter out older models
        cutoff_date = Time.new(2024, 1, 1).to_i

        _, specs = OpenRouter::ModelRegistry.find_best_model(
          released_after_date: cutoff_date
        )

        expect(specs[:created_at]).to be >= cutoff_date
      end

      it "prioritizes newer models when pick_newer is true" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          pick_newer: true
        )

        # Should return the newest model in the fixture
        all_timestamps = OpenRouter::ModelRegistry.all_models.values.map { |s| s[:created_at] }
        expect(specs[:created_at]).to eq(all_timestamps.max)
      end

      it "returns nil when no models meet date requirements" do
        # Use a future timestamp
        future_date = Time.new(2030, 1, 1).to_i

        result = OpenRouter::ModelRegistry.find_best_model(
          released_after_date: future_date
        )

        expect(result).to be_nil
      end
    end

    context "with combined requirements" do
      it "finds models meeting all requirements" do
        _, specs = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:function_calling],
          max_input_cost: 0.02,
          min_context_length: 50_000
        )

        expect(specs[:capabilities]).to include(:function_calling)
        expect(specs[:cost_per_1k_tokens][:input]).to be <= 0.02
        expect(specs[:context_length]).to be >= 50_000
      end

      it "prioritizes cost when multiple models qualify" do
        results = OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:chat],
          max_input_cost: 0.10
        )

        # Should return cheapest option
        expect(results).not_to be_nil
      end
    end
  end

  describe ".get_fallbacks" do
    it "returns fallback models for known models" do
      fallbacks = OpenRouter::ModelRegistry.get_fallbacks("mistralai/mistral-medium-3.1")
      expect(fallbacks).to be_an(Array)
      # For now our implementation returns empty fallbacks, which is fine
      expect(fallbacks).to eq([])
    end

    it "returns empty array for unknown models" do
      fallbacks = OpenRouter::ModelRegistry.get_fallbacks("unknown-model")
      expect(fallbacks).to eq([])
    end

    it "returns fallbacks that are also in the registry" do
      fallbacks = OpenRouter::ModelRegistry.get_fallbacks("mistralai/mistral-medium-3.1")
      fallbacks.each do |fallback_model|
        expect(OpenRouter::ModelRegistry.model_exists?(fallback_model)).to be true
      end
    end
  end

  describe ".model_exists?" do
    it "returns true for registered models" do
      model, = OpenRouter::ModelRegistry.find_best_model
      expect(OpenRouter::ModelRegistry.model_exists?(model)).to be true
    end

    it "returns false for unregistered models" do
      expect(OpenRouter::ModelRegistry.model_exists?("nonexistent-model")).to be false
    end
  end

  describe ".has_capability?" do
    it "returns true when model has the specified capability" do
      # Find a model that has function calling support
      model_with_tools = OpenRouter::ModelRegistry.all_models.find { |_, specs| specs[:capabilities].include?(:function_calling) }
      if model_with_tools
        model_id, _ = model_with_tools
        expect(OpenRouter::ModelRegistry.has_capability?(model_id, :function_calling)).to be true
      end
    end

    it "returns false when model doesn't have the specified capability" do
      # Find a model without vision capability for testing
      model_without_vision = OpenRouter::ModelRegistry.all_models.find { |_, specs| !specs[:capabilities].include?(:vision) }
      if model_without_vision
        model_id, _ = model_without_vision
        expect(OpenRouter::ModelRegistry.has_capability?(model_id, :vision)).to be false
      end
    end

    it "returns false for non-existent models" do
      expect(OpenRouter::ModelRegistry.has_capability?("nonexistent/model", :chat)).to be false
    end
  end

  describe ".get_model_info" do
    it "returns model specifications for registered models" do
      model, = OpenRouter::ModelRegistry.find_best_model
      info = OpenRouter::ModelRegistry.get_model_info(model)

      expect(info).to be_a(Hash)
      expect(info).to have_key(:cost_per_1k_tokens)
      expect(info).to have_key(:capabilities)
      expect(info).to have_key(:context_length)
      expect(info).to have_key(:performance_tier)
    end

    it "returns nil for unregistered models" do
      info = OpenRouter::ModelRegistry.get_model_info("nonexistent-model")
      expect(info).to be_nil
    end
  end

  describe ".all_models" do
    it "returns all registered models" do
      models = OpenRouter::ModelRegistry.all_models
      expect(models).to be_a(Hash)
      expect(models).not_to be_empty

      models.each do |model_name, specs|
        expect(model_name).to be_a(String)
        expect(specs).to be_a(Hash)
        expect(specs).to have_key(:cost_per_1k_tokens)
        expect(specs).to have_key(:capabilities)
      end
    end
  end

  describe ".calculate_estimated_cost" do
    it "calculates cost for input tokens" do
      cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "mistralai/mistral-medium-3.1",
        input_tokens: 1000,
        output_tokens: 0
      )
      expect(cost).to be > 0
      expect(cost).to be_a(Numeric)
    end

    it "calculates cost for output tokens" do
      cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "mistralai/mistral-medium-3.1",
        input_tokens: 0,
        output_tokens: 1000
      )
      expect(cost).to be > 0
      expect(cost).to be_a(Numeric)
    end

    it "calculates combined cost for input and output tokens" do
      input_cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "mistralai/mistral-medium-3.1",
        input_tokens: 1000,
        output_tokens: 0
      )
      output_cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "mistralai/mistral-medium-3.1",
        input_tokens: 0,
        output_tokens: 1000
      )
      combined_cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "mistralai/mistral-medium-3.1",
        input_tokens: 1000,
        output_tokens: 1000
      )

      expect(combined_cost).to eq(input_cost + output_cost)
    end

    it "returns 0 for unknown models" do
      cost = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "unknown-model",
        input_tokens: 1000,
        output_tokens: 1000
      )
      expect(cost).to eq(0)
    end
  end

  describe "model data validation" do
    it "includes models from the fixture" do
      models = OpenRouter::ModelRegistry.all_models

      # Should include models from our test fixture
      expect(models).to have_key("mistralai/mistral-medium-3.1")
      expect(models).to have_key("ai21/jamba-mini-1.7")

      # Should include at least 5 models from fixture
      expect(models.keys.size).to be >= 5
    end

    it "has valid cost structure for all models" do
      OpenRouter::ModelRegistry.all_models.each do |_model, specs|
        expect(specs[:cost_per_1k_tokens]).to be_a(Hash)
        expect(specs[:cost_per_1k_tokens]).to have_key(:input)
        expect(specs[:cost_per_1k_tokens]).to have_key(:output)
        expect(specs[:cost_per_1k_tokens][:input]).to be >= 0
        expect(specs[:cost_per_1k_tokens][:output]).to be >= 0
      end
    end

    it "has valid capabilities for all models" do
      OpenRouter::ModelRegistry.all_models.each do |_model, specs|
        expect(specs[:capabilities]).to be_an(Array)
        expect(specs[:capabilities]).not_to be_empty
        expect(specs[:capabilities]).to include(:chat) # All should support basic chat
      end
    end

    it "has valid context lengths for all models" do
      OpenRouter::ModelRegistry.all_models.each do |_model, specs|
        expect(specs[:context_length]).to be_a(Numeric)
        expect(specs[:context_length]).to be > 0
      end
    end

    it "has valid creation timestamps for all models" do
      OpenRouter::ModelRegistry.all_models.each do |_model, specs|
        expect(specs[:created_at]).to be_a(Numeric)
        expect(specs[:created_at]).to be > 0
      end
    end
  end
end

================
File: spec/model_selector_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::ModelSelector do
  let(:fixture_data) do
    JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))
  end

  before do
    # Mock the HTTP client to return our fixture data
    allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(fixture_data)
    # Clear any cached data
    OpenRouter::ModelRegistry.clear_cache!
  end

  describe "#initialize" do
    it "creates a new selector with default settings" do
      selector = described_class.new
      expect(selector.selection_criteria[:strategy]).to eq(:cost)
      expect(selector.selection_criteria[:requirements]).to be_empty
      expect(selector.selection_criteria[:provider_preferences]).to be_empty
    end
  end

  describe "#optimize_for" do
    let(:selector) { described_class.new }

    it "sets cost optimization strategy" do
      new_selector = selector.optimize_for(:cost)
      expect(new_selector.selection_criteria[:strategy]).to eq(:cost)
    end

    it "sets performance optimization strategy" do
      new_selector = selector.optimize_for(:performance)
      expect(new_selector.selection_criteria[:strategy]).to eq(:performance)
      expect(new_selector.selection_criteria[:requirements][:performance_tier]).to eq(:premium)
    end

    it "sets latest optimization strategy" do
      new_selector = selector.optimize_for(:latest)
      expect(new_selector.selection_criteria[:strategy]).to eq(:latest)
      expect(new_selector.selection_criteria[:requirements][:pick_newer]).to be true
    end

    it "sets context optimization strategy" do
      new_selector = selector.optimize_for(:context)
      expect(new_selector.selection_criteria[:strategy]).to eq(:context)
    end

    it "raises error for unknown strategy" do
      expect { selector.optimize_for(:unknown) }.to raise_error(ArgumentError, /Unknown strategy/)
    end

    it "returns new instance for method chaining" do
      result = selector.optimize_for(:cost)
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "#require" do
    let(:selector) { described_class.new }

    it "adds single capability requirement" do
      new_selector = selector.require(:function_calling)
      expect(new_selector.selection_criteria[:requirements][:capabilities]).to eq([:function_calling])
    end

    it "adds multiple capability requirements" do
      new_selector = selector.require(:function_calling, :vision, :structured_outputs)
      expect(new_selector.selection_criteria[:requirements][:capabilities]).to eq(%i[function_calling vision structured_outputs])
    end

    it "accumulates capabilities across multiple calls" do
      new_selector = selector.require(:function_calling).require(:vision)
      expect(new_selector.selection_criteria[:requirements][:capabilities]).to eq(%i[function_calling vision])
    end

    it "handles duplicate capabilities" do
      new_selector = selector.require(:function_calling, :vision).require(:function_calling)
      expect(new_selector.selection_criteria[:requirements][:capabilities]).to eq(%i[function_calling vision])
    end

    it "returns new instance for method chaining" do
      result = selector.require(:function_calling)
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "#within_budget" do
    let(:selector) { described_class.new }

    it "sets max input cost requirement" do
      new_selector = selector.within_budget(max_cost: 0.01)
      expect(new_selector.selection_criteria[:requirements][:max_input_cost]).to eq(0.01)
    end

    it "sets both input and output cost requirements" do
      new_selector = selector.within_budget(max_cost: 0.01, max_output_cost: 0.02)
      expect(new_selector.selection_criteria[:requirements][:max_input_cost]).to eq(0.01)
      expect(new_selector.selection_criteria[:requirements][:max_output_cost]).to eq(0.02)
    end

    it "returns new instance for method chaining" do
      result = selector.within_budget(max_cost: 0.01)
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "#min_context" do
    let(:selector) { described_class.new }

    it "sets minimum context length requirement" do
      new_selector = selector.min_context(100_000)
      expect(new_selector.selection_criteria[:requirements][:min_context_length]).to eq(100_000)
    end

    it "returns new instance for method chaining" do
      result = selector.min_context(50_000)
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "#newer_than" do
    let(:selector) { described_class.new }

    it "sets date requirement with Date object" do
      date = Date.new(2024, 1, 1)
      new_selector = selector.newer_than(date)
      expect(new_selector.selection_criteria[:requirements][:released_after_date]).to eq(date)
    end

    it "sets date requirement with Time object" do
      time = Time.new(2024, 1, 1)
      new_selector = selector.newer_than(time)
      expect(new_selector.selection_criteria[:requirements][:released_after_date]).to eq(time)
    end

    it "returns new instance for method chaining" do
      result = selector.newer_than(Date.new(2024, 1, 1))
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "provider filtering methods" do
    let(:selector) { described_class.new }

    describe "#prefer_providers" do
      it "sets preferred providers" do
        new_selector = selector.prefer_providers("anthropic", "openai")
        expect(new_selector.selection_criteria[:provider_preferences][:preferred]).to eq(%w[anthropic openai])
      end

      it "handles array input" do
        new_selector = selector.prefer_providers(%w[anthropic openai])
        expect(new_selector.selection_criteria[:provider_preferences][:preferred]).to eq(%w[anthropic openai])
      end

      it "returns new instance for method chaining" do
        result = selector.prefer_providers("anthropic")
        expect(result).to be_a(described_class)
        expect(result).not_to be(selector)
      end
    end

    describe "#require_providers" do
      it "sets required providers" do
        new_selector = selector.require_providers("anthropic")
        expect(new_selector.selection_criteria[:provider_preferences][:required]).to eq(["anthropic"])
      end

      it "returns new instance for method chaining" do
        result = selector.require_providers("anthropic")
        expect(result).to be_a(described_class)
        expect(result).not_to be(selector)
      end
    end

    describe "#avoid_providers" do
      it "sets avoided providers" do
        new_selector = selector.avoid_providers("google")
        expect(new_selector.selection_criteria[:provider_preferences][:avoided]).to eq(["google"])
      end

      it "returns new instance for method chaining" do
        result = selector.avoid_providers("google")
        expect(result).to be_a(described_class)
        expect(result).not_to be(selector)
      end
    end

    describe "#avoid_patterns" do
      it "sets avoided patterns" do
        new_selector = selector.avoid_patterns("*-free", "*-preview")
        expect(new_selector.selection_criteria[:provider_preferences][:avoided_patterns]).to eq(["*-free", "*-preview"])
      end

      it "returns new instance for method chaining" do
        result = selector.avoid_patterns("*-free")
        expect(result).to be_a(described_class)
        expect(result).not_to be(selector)
      end
    end
  end

  describe "#with_fallbacks" do
    let(:selector) { described_class.new }

    it "sets fallback options" do
      new_selector = selector.with_fallbacks(max: 5, strategy: :similar)
      expect(new_selector.selection_criteria[:fallback_options][:max_fallbacks]).to eq(5)
      expect(new_selector.selection_criteria[:fallback_options][:strategy]).to eq(:similar)
    end

    it "uses default options" do
      new_selector = selector.with_fallbacks
      expect(new_selector.selection_criteria[:fallback_options][:max_fallbacks]).to eq(3)
      expect(new_selector.selection_criteria[:fallback_options][:strategy]).to eq(:similar)
    end

    it "returns new instance for method chaining" do
      result = selector.with_fallbacks
      expect(result).to be_a(described_class)
      expect(result).not_to be(selector)
    end
  end

  describe "#choose" do
    context "with cost optimization" do
      it "returns the cheapest model" do
        selector = described_class.new.optimize_for(:cost)
        model = selector.choose
        expect(model).to be_a(String)
        # Based on fixture data, ai21/jamba-mini-1.7 should be cheapest
        expect(model).to eq("ai21/jamba-mini-1.7")
      end

      it "returns model with specs when requested" do
        selector = described_class.new.optimize_for(:cost)
        model, specs = selector.choose(return_specs: true)
        expect(model).to be_a(String)
        expect(specs).to be_a(Hash)
        expect(specs).to have_key(:cost_per_1k_tokens)
        expect(specs).to have_key(:capabilities)
      end
    end

    context "with capability requirements" do
      it "finds models with function calling capability" do
        selector = described_class.new.require(:function_calling)
        model = selector.choose
        expect(model).to be_a(String)

        # Verify the model actually has the capability
        model_info = OpenRouter::ModelRegistry.get_model_info(model)
        expect(model_info[:capabilities]).to include(:function_calling)
      end

      it "finds models with vision capability" do
        selector = described_class.new.require(:vision)
        model = selector.choose
        expect(model).to be_a(String)

        # Verify the model actually has the capability
        model_info = OpenRouter::ModelRegistry.get_model_info(model)
        expect(model_info[:capabilities]).to include(:vision)
      end

      it "finds models with multiple capabilities" do
        selector = described_class.new.require(:function_calling, :structured_outputs)
        model = selector.choose
        expect(model).to be_a(String)

        # Verify the model has both capabilities
        model_info = OpenRouter::ModelRegistry.get_model_info(model)
        expect(model_info[:capabilities]).to include(:function_calling, :structured_outputs)
      end

      it "returns nil when no models have required capabilities" do
        selector = described_class.new.require(:nonexistent_capability)
        model = selector.choose
        expect(model).to be_nil
      end
    end

    context "with cost constraints" do
      it "finds models within budget" do
        selector = described_class.new.within_budget(max_cost: 0.01)
        model = selector.choose
        expect(model).to be_a(String)

        # Verify the model is within budget
        model_info = OpenRouter::ModelRegistry.get_model_info(model)
        expect(model_info[:cost_per_1k_tokens][:input]).to be <= 0.01
      end

      it "returns nil when no models are within budget" do
        selector = described_class.new.within_budget(max_cost: 0.0000001)
        model = selector.choose
        expect(model).to be_nil
      end
    end

    context "with context length requirements" do
      it "finds models with sufficient context" do
        selector = described_class.new.min_context(100_000)
        model = selector.choose

        if model # Some fixtures might not have large context models
          model_info = OpenRouter::ModelRegistry.get_model_info(model)
          expect(model_info[:context_length]).to be >= 100_000
        else
          expect(model).to be_nil
        end
      end
    end

    context "with provider filtering" do
      it "only returns models from required providers" do
        selector = described_class.new.require_providers("ai21")
        model = selector.choose
        expect(model).to be_a(String)
        expect(model).to start_with("ai21/")
      end

      it "avoids models from excluded providers" do
        selector = described_class.new.avoid_providers("ai21")
        model = selector.choose
        expect(model).to be_a(String)
        expect(model).not_to start_with("ai21/")
      end

      it "avoids models matching patterns" do
        selector = described_class.new.avoid_patterns("*-mini*")
        model = selector.choose
        expect(model).to be_a(String)
        expect(model).not_to include("mini")
      end

      it "returns nil when no models match provider requirements" do
        selector = described_class.new.require_providers("nonexistent-provider")
        model = selector.choose
        expect(model).to be_nil
      end
    end

    context "with performance optimization" do
      it "prefers premium tier models when available, otherwise standard" do
        selector = described_class.new.optimize_for(:performance)
        model = selector.choose

        if model
          expect(model).to be_a(String)
          model_info = OpenRouter::ModelRegistry.get_model_info(model)
          expect(%i[standard premium]).to include(model_info[:performance_tier])
        else
          # No models meet the performance tier requirement
          expect(model).to be_nil
        end
      end
    end

    context "with latest optimization" do
      it "returns the newest model" do
        selector = described_class.new.optimize_for(:latest)
        model, specs = selector.choose(return_specs: true)
        expect(model).to be_a(String)

        # Verify it's the newest among all models
        all_models = OpenRouter::ModelRegistry.all_models
        max_timestamp = all_models.values.map { |s| s[:created_at] }.max
        expect(specs[:created_at]).to eq(max_timestamp)
      end
    end

    context "with context optimization" do
      it "returns the model with largest context window" do
        selector = described_class.new.optimize_for(:context)
        model, specs = selector.choose(return_specs: true)
        expect(model).to be_a(String)

        # Verify it has the largest context among all models
        all_models = OpenRouter::ModelRegistry.all_models
        max_context = all_models.values.map { |s| s[:context_length] }.max
        expect(specs[:context_length]).to eq(max_context)
      end
    end
  end

  describe "#choose_with_fallbacks" do
    it "returns array of model IDs" do
      selector = described_class.new.optimize_for(:cost)
      models = selector.choose_with_fallbacks(limit: 3)
      expect(models).to be_an(Array)
      expect(models.size).to be <= 3
      expect(models.all? { |m| m.is_a?(String) }).to be true
    end

    it "returns models in order of preference" do
      selector = described_class.new.optimize_for(:cost)
      models = selector.choose_with_fallbacks(limit: 3)

      # Verify they're sorted by cost (cheapest first)
      costs = models.map do |model|
        OpenRouter::ModelRegistry.get_model_info(model)[:cost_per_1k_tokens][:input]
      end
      expect(costs).to eq(costs.sort)
    end

    it "respects the limit parameter" do
      selector = described_class.new
      models = selector.choose_with_fallbacks(limit: 2)
      expect(models.size).to be <= 2
    end

    it "returns empty array when no models match" do
      selector = described_class.new.require(:nonexistent_capability)
      models = selector.choose_with_fallbacks
      expect(models).to eq([])
    end

    it "works with capability requirements" do
      selector = described_class.new.require(:function_calling)
      models = selector.choose_with_fallbacks(limit: 2)
      expect(models).to be_an(Array)

      # Verify all returned models have the required capability
      models.each do |model|
        model_info = OpenRouter::ModelRegistry.get_model_info(model)
        expect(model_info[:capabilities]).to include(:function_calling)
      end
    end
  end

  describe "#choose_with_fallback" do
    it "returns a model when requirements can be met" do
      selector = described_class.new.require(:function_calling)
      model = selector.choose_with_fallback
      expect(model).to be_a(String)
    end

    it "gracefully degrades requirements when needed" do
      # Set very restrictive requirements that likely won't be met
      selector = described_class.new
                                .within_budget(max_cost: 0.0000001)
                                .min_context(1_000_000)
                                .require(:function_calling)

      model = selector.choose_with_fallback
      expect(model).to be_a(String)

      # Should still have the capability requirement since it's most important
      model_info = OpenRouter::ModelRegistry.get_model_info(model)
      expect(model_info[:capabilities]).to include(:function_calling)
    end

    it "returns cheapest model as final fallback" do
      # Set impossible capability requirement
      selector = described_class.new.require(:nonexistent_capability)
      model = selector.choose_with_fallback
      expect(model).to be_a(String)

      # Should be the cheapest available model
      all_models = OpenRouter::ModelRegistry.all_models
      cheapest_model = all_models.min_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
      expect(model).to eq(cheapest_model.first)
    end
  end

  describe "#selection_criteria" do
    it "returns current selection state" do
      selector = described_class.new
                                .optimize_for(:performance)
                                .require(:function_calling)
                                .within_budget(max_cost: 0.01)
                                .prefer_providers("anthropic")

      criteria = selector.selection_criteria
      expect(criteria[:strategy]).to eq(:performance)
      expect(criteria[:requirements][:capabilities]).to eq([:function_calling])
      expect(criteria[:requirements][:max_input_cost]).to eq(0.01)
      expect(criteria[:provider_preferences][:preferred]).to eq(["anthropic"])
    end

    it "returns defensive copies" do
      selector = described_class.new.require(:function_calling)
      criteria1 = selector.selection_criteria
      criteria2 = selector.selection_criteria

      # Modifying one shouldn't affect the other
      criteria1[:requirements][:capabilities] << :vision
      expect(criteria2[:requirements][:capabilities]).to eq([:function_calling])
    end
  end

  describe "#estimate_cost" do
    it "calculates estimated cost for a model" do
      selector = described_class.new
      cost = selector.estimate_cost("ai21/jamba-mini-1.7", input_tokens: 1000, output_tokens: 1000)
      expect(cost).to be_a(Numeric)
      expect(cost).to be > 0
    end

    it "uses default token counts" do
      selector = described_class.new
      cost = selector.estimate_cost("ai21/jamba-mini-1.7")
      expect(cost).to be_a(Numeric)
      expect(cost).to be > 0
    end
  end

  describe "method chaining" do
    it "supports complex chaining" do
      model = described_class.new
                             .optimize_for(:cost)
                             .require(:function_calling, :structured_outputs)
                             .within_budget(max_cost: 0.05)
                             .min_context(50_000)
                             .prefer_providers("anthropic", "openai")
                             .avoid_patterns("*-free")
                             .choose

      expect(model).to be_a(String).or be_nil
    end

    it "maintains immutability of chained calls" do
      base_selector = described_class.new.optimize_for(:cost)

      selector1 = base_selector.require(:function_calling)
      selector2 = base_selector.require(:vision)

      # Base selector should remain unchanged
      expect(base_selector.selection_criteria[:requirements][:capabilities]).to be_nil

      # Each branch should have its own requirements
      expect(selector1.selection_criteria[:requirements][:capabilities]).to eq([:function_calling])
      expect(selector2.selection_criteria[:requirements][:capabilities]).to eq([:vision])
    end
  end

  describe "error handling" do
    it "handles edge cases gracefully" do
      selector = described_class.new

      # Empty requirements should work
      expect { selector.choose }.not_to raise_error

      # Nil inputs should be handled
      expect { selector.within_budget(max_cost: nil) }.not_to raise_error
      expect { selector.min_context(nil) }.not_to raise_error
    end
  end
end

================
File: spec/mutation_test_examples.rb
================
# frozen_string_literal: true

# Examples of code mutations your tests should catch

# Add to Gemfile for mutation testing:
# gem 'mutant-rspec', group: :test

# These are examples of mutations your current tests should catch:

describe "Mutation testing examples" do
  # Example 1: Cost comparison logic
  # If someone accidentally changed < to <= in cost filtering:
  # Original: specs[:cost_per_1k_tokens][:input] < max_cost
  # Mutant:   specs[:cost_per_1k_tokens][:input] <= max_cost

  it "catches boundary condition mutations in cost filtering" do
    # Your current test at exactly the boundary would catch this:
    _, specs = OpenRouter::ModelRegistry.find_best_model(max_input_cost: 0.01)
    expect(specs[:cost_per_1k_tokens][:input]).to be <= 0.01

    # But you might want to add:
    model = OpenRouter::ModelRegistry.find_best_model(max_input_cost: 0.0049999)
    expect(model).not_to be_nil # Should find ai21/jamba-mini-1.7 at 0.003

    model = OpenRouter::ModelRegistry.find_best_model(max_input_cost: 0.0029999)
    expect(model).to be_nil # Should NOT find any models
  end

  # Example 2: Capability accumulation logic
  # If someone changed Array.union to Array.intersection:
  # Original: capabilities |= new_capabilities
  # Mutant:   capabilities &= new_capabilities

  it "catches mutations in capability accumulation" do
    selector = OpenRouter::ModelSelector.new
                                        .require(:function_calling)
                                        .require(:vision)

    capabilities = selector.selection_criteria[:requirements][:capabilities]
    expect(capabilities).to include(:function_calling)
    expect(capabilities).to include(:vision)
    expect(capabilities.size).to eq(2) # Would fail if intersection instead of union
  end

  # Example 3: Strategy logic mutations
  # If someone accidentally switched the cost optimization:
  # Original: models.min_by { |_, specs| specs[:cost_per_1k_tokens][:input] }
  # Mutant:   models.max_by { |_, specs| specs[:cost_per_1k_tokens][:input] }

  it "catches mutations in optimization strategy" do
    cost_selector = OpenRouter::ModelSelector.new.optimize_for(:cost)
    _, cost_specs = cost_selector.choose(return_specs: true)

    # Should be cheapest available model
    all_models = OpenRouter::ModelRegistry.all_models
    cheapest_cost = all_models.values.map { |s| s[:cost_per_1k_tokens][:input] }.min
    expect(cost_specs[:cost_per_1k_tokens][:input]).to eq(cheapest_cost)
  end

  # Example 4: Date comparison mutations
  # Original: model_timestamp >= cutoff_date
  # Mutant:   model_timestamp > cutoff_date

  it "catches boundary mutations in date filtering" do
    # Test with exact timestamp
    exact_timestamp = 1_755_095_639 # From fixture

    model, = OpenRouter::ModelRegistry.find_best_model(
      released_after_date: exact_timestamp
    )
    expect(model).not_to be_nil # Should include models AT the exact timestamp

    OpenRouter::ModelRegistry.find_best_model(
      released_after_date: exact_timestamp + 1
    )
    # This test validates the boundary behavior
  end
end

# To run mutation testing (if you add the gem):
# bundle exec mutant --include lib --require open_router --use rspec OpenRouter::ModelRegistry#find_best_model

================
File: spec/open_router_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter do
  it "has a version number" do
    expect(OpenRouter::VERSION).not_to be nil
  end

  describe OpenRouter::Client do
    let(:client) do
      OpenRouter::Client.new(access_token: ENV["ACCESS_TOKEN"]) do |config|
        config.faraday do |f|
          f.response :logger, ::Logger.new($stdout), { headers: true, bodies: true, errors: true } do |logger|
            logger.filter(/(Bearer) (\S+)/, '\1[REDACTED]')
          end
        end
      end
    end

    describe "#initialize" do
      it "yields the configuration" do
        expect { |b| OpenRouter::Client.new(&b) }.to yield_with_args(OpenRouter.configuration)
      end
    end

    describe "#complete" do
      let(:messages) { [{ role: "user", content: "What is the meaning of life?" }] }
      let(:extras) { { max_tokens: 100 } }
      let(:mock_response) do
        {
          "id" => "chatcmpl-123",
          "object" => "chat.completion",
          "created" => 1_677_652_288,
          "model" => "mistralai/mistral-7b-instruct:free",
          "choices" => [
            {
              "index" => 0,
              "message" => {
                "role" => "assistant",
                "content" => "The meaning of life is 42."
              },
              "finish_reason" => "stop"
            }
          ],
          "usage" => {
            "prompt_tokens" => 12,
            "completion_tokens" => 8,
            "total_tokens" => 20
          }
        }
      end

      it "sends a POST request to the completions endpoint with the correct parameters" do
        expect(client).to receive(:post).with(
          path: "/chat/completions",
          parameters: {
            model: "mistralai/mistral-7b-instruct:free",
            messages:,
            max_tokens: 100
          }
        ).and_return(mock_response)

        response = client.complete(messages, model: "mistralai/mistral-7b-instruct:free", extras:)
        expect(response).to be_a(OpenRouter::Response)
        expect(response.content).to eq("The meaning of life is 42.")
      end
    end

    describe "#models" do
      it "sends a GET request to the models endpoint" do
        expect(client).to receive(:get).with(path: "/models").and_return({ "data" => [] })
        client.models
      end

      it "returns the data from the response" do
        allow(client).to receive(:get).and_return({ "data" => [{ "id" => "model1" }, { "id" => "model2" }] })
        expect(client.models).to eq([{ "id" => "model1" }, { "id" => "model2" }])
      end
    end

    describe "#query_generation_stats" do
      let(:generation_id) { "generation_123" }

      it "sends a GET request to the generation endpoint with the generation ID" do
        expect(client).to receive(:get).with(path: "/generation?id=#{generation_id}").and_return({ "data" => {} })
        client.query_generation_stats(generation_id)
      end

      it "returns the data from the response" do
        allow(client).to receive(:get).and_return({ "data" => { "tokens" => 100, "cost" => 0.01 } })
        expect(client.query_generation_stats(generation_id)).to eq({ "tokens" => 100, "cost" => 0.01 })
      end
    end
  end
end

================
File: spec/performance_spec.rb
================
# frozen_string_literal: true

require 'benchmark'

# Performance tests to add

RSpec.describe "Performance characteristics" do
  before do
    fixture_data = JSON.parse(File.read(File.join(__dir__, "fixtures", "openrouter_models_sample.json")))
    allow(OpenRouter::ModelRegistry).to receive(:fetch_models_from_api).and_return(fixture_data)
    OpenRouter::ModelRegistry.clear_cache!
  end

  describe "ModelRegistry performance" do
    it "caches processed models to avoid repeated computation" do
      skip "Performance test sensitive to execution order and timing"
      
      # Clear any existing cache to ensure fair test
      OpenRouter::ModelRegistry.clear_cache!
      
      # First call processes raw data
      start_time = Time.now
      OpenRouter::ModelRegistry.all_models
      first_call_time = Time.now - start_time

      # Second call should use cached processed data
      start_time = Time.now
      OpenRouter::ModelRegistry.all_models
      second_call_time = Time.now - start_time

      # Second call should be significantly faster (allowing some tolerance)
      expect(second_call_time).to be < (first_call_time * 0.5)
    end

    it "performs model searches efficiently" do
      # Warm up cache
      OpenRouter::ModelRegistry.all_models

      # Measure search performance
      start_time = Time.now

      100.times do
        OpenRouter::ModelRegistry.find_best_model(
          capabilities: [:function_calling],
          max_input_cost: 0.01
        )
      end

      elapsed = Time.now - start_time

      # Should handle 100 searches quickly
      expect(elapsed).to be < 0.1
      puts "100 model searches took: #{elapsed}s (#{elapsed / 100 * 1000}ms per search)"
    end
  end

  describe "ModelSelector performance" do
    it "handles complex chaining without performance degradation" do
      base_time = Benchmark.realtime do
        OpenRouter::ModelSelector.new.choose
      end

      complex_time = Benchmark.realtime do
        OpenRouter::ModelSelector.new
                                 .optimize_for(:cost)
                                 .require(:function_calling, :vision, :structured_outputs)
                                 .within_budget(max_cost: 0.05, max_output_cost: 0.10)
                                 .min_context(50_000)
                                 .prefer_providers("anthropic", "openai")
                                 .avoid_patterns("*-free", "*-preview")
                                 .newer_than(Date.new(2023, 1, 1))
                                 .choose
      end

      # Complex query shouldn't be dramatically slower
      expect(complex_time).to be < (base_time * 5)
      puts "Base query: #{base_time}s, Complex query: #{complex_time}s"
    end
  end
end

================
File: spec/property_based_examples.rb
================
# frozen_string_literal: true

# Example property-based tests to consider adding

describe "ModelRegistry property-based tests" do
  it "cost calculation properties" do
    # Property: cost should be monotonic with token count
    100.times do
      input_tokens = rand(1..10_000)
      output_tokens = rand(1..10_000)

      cost1 = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "ai21/jamba-mini-1.7",
        input_tokens:,
        output_tokens:
      )

      cost2 = OpenRouter::ModelRegistry.calculate_estimated_cost(
        "ai21/jamba-mini-1.7",
        input_tokens: input_tokens * 2,
        output_tokens: output_tokens * 2
      )

      expect(cost2).to be > cost1
    end
  end

  it "model selection consistency" do
    # Property: same requirements should always return same model (when deterministic)
    requirements = { capabilities: [:function_calling], max_input_cost: 0.01 }

    10.times do
      model1 = OpenRouter::ModelRegistry.find_best_model(requirements)
      model2 = OpenRouter::ModelRegistry.find_best_model(requirements)
      expect(model1).to eq(model2)
    end
  end
end

describe "ModelSelector property-based tests" do
  it "chaining order independence for commutative operations" do
    # Property: order of capability requirements shouldn't matter
    caps = %i[function_calling vision structured_outputs].shuffle

    selector1 = OpenRouter::ModelSelector.new.require(*caps)
    selector2 = OpenRouter::ModelSelector.new.require(caps[0]).require(caps[1]).require(caps[2])

    expect(selector1.selection_criteria[:requirements][:capabilities].sort).to eq(
      selector2.selection_criteria[:requirements][:capabilities].sort
    )
  end
end

================
File: spec/response_healing_spec.rb
================
# frozen_string_literal: true

require 'spec_helper'
require 'open_router'

RSpec.describe "Response Healing" do
  let(:client) do
    OpenRouter::Client.new(access_token: "test-key") do |config|
      config.auto_heal_responses = true
      config.healer_model = "openai/gpt-4o-mini"
      config.max_heal_attempts = 2
    end
  end

  let(:valid_json) { '{"name": "John", "age": 30}' }
  let(:malformed_json) { '{"name": "John", age: 30}' } # Missing quotes around key
  let(:partial_json) { '{"name": "John"' } # Incomplete
  let(:json_with_text) { 'Here is the JSON: {"name": "John", "age": 30} and some extra text' }
  let(:basic_schema) { { type: "json_schema", json_schema: { schema: { type: "object" } } } }

  describe "JSON healing" do
    context "with malformed JSON" do
      it "heals JSON parsing errors with simple mock" do
        # Mock the healing response with fixed JSON
        healed_response = double("Response", content: valid_json)
        allow(client).to receive(:complete).and_return(healed_response)

        # Create response with healing enabled
        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => malformed_json } }] },
          response_format: basic_schema
        )
        response.client = client

        result = response.structured_output(auto_heal: true)
        expect(result).to eq({ "name" => "John", "age" => 30 })
        
        # Verify healing was attempted
        expect(client).to have_received(:complete).at_least(:once)
      end
    end

    context "with valid JSON" do
      it "returns parsed JSON without healing" do
        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => valid_json } }] },
          response_format: basic_schema
        )
        response.client = client

        # Should not call the healing client
        expect(client).not_to receive(:complete)

        result = response.structured_output(auto_heal: true)
        expect(result).to eq({ "name" => "John", "age" => 30 })
      end
    end

    context "when auto_heal is disabled" do
      it "raises JSON::ParserError for malformed JSON" do
        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => malformed_json } }] },
          response_format: basic_schema
        )

        expect {
          response.structured_output(auto_heal: false)
        }.to raise_error(OpenRouter::StructuredOutputError)
      end
    end

    context "when client is not available" do
      it "cannot heal and raises the original error" do
        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => malformed_json } }] },
          response_format: basic_schema
        )
        # No client set

        expect {
          response.structured_output(auto_heal: true)
        }.to raise_error(OpenRouter::StructuredOutputError)
      end
    end
  end

  describe "schema healing" do
    let(:schema) do
      OpenRouter::Schema.define("person") do
        string :name, required: true
        integer :age, required: true
        string :email, required: false
      end
    end

    context "with schema validation errors" do
      it "heals schema validation failures" do
        # JSON that parses but fails schema validation (skip if no json-schema)
        skip "json-schema gem not available" unless defined?(JSON::Validator)
        
        invalid_data = '{"name": "John", "age": "thirty"}' # age should be integer
        valid_data = '{"name": "John", "age": 30}'

        healed_response = double("Response", content: valid_data)
        allow(client).to receive(:complete).and_return(healed_response)

        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => invalid_data } }] },
          response_format: schema
        )
        response.client = client

        result = response.structured_output(auto_heal: true)
        expect(result).to eq({ "name" => "John", "age" => 30 })
      end
    end

    context "when json-schema gem is not available" do
      before do
        # Temporarily hide JSON::Validator if it exists
        if defined?(JSON::Validator)
          @original_validator = JSON::Validator
          JSON.send(:remove_const, :Validator) if JSON.const_defined?(:Validator)
        end
      end

      after do
        # Restore JSON::Validator if it existed
        if @original_validator
          JSON::Validator = @original_validator
        end
      end

      it "skips schema validation but still heals JSON parsing" do
        response = OpenRouter::Response.new(
          { "choices" => [{ "message" => { "content" => malformed_json } }] },
          response_format: schema
        )
        response.client = client

        healed_response = double("Response", content: valid_json)
        expect(client).to receive(:complete).and_return(healed_response)

        result = response.structured_output(auto_heal: true)
        expect(result).to eq({ "name" => "John", "age" => 30 })
      end
    end
  end

  describe "configuration options" do
    it "respects custom healer model" do
      custom_client = OpenRouter::Client.new(access_token: "test") do |config|
        config.healer_model = "anthropic/claude-3-haiku"
      end

      response = OpenRouter::Response.new(
        { "choices" => [{ "message" => { "content" => malformed_json } }] },
        response_format: basic_schema
      )
      response.client = custom_client

      healed_response = double("Response", content: valid_json)
      expect(custom_client).to receive(:complete).with(
        anything,
        hash_including(model: "anthropic/claude-3-haiku")
      ).and_return(healed_response)

      response.structured_output(auto_heal: true)
    end

    it "respects custom max_heal_attempts" do
      custom_client = OpenRouter::Client.new(access_token: "test") do |config|
        config.max_heal_attempts = 1
      end
      bad_response = double("Response", content: '{"still": broken}') # Still invalid JSON
      expect(custom_client).to receive(:complete).exactly(1).time.and_return(bad_response)

      response = OpenRouter::Response.new(
        { "choices" => [{ "message" => { "content" => malformed_json } }] },
        response_format: basic_schema
      )
      response.client = custom_client

      expect {
        response.structured_output(auto_heal: true)
      }.to raise_error(OpenRouter::StructuredOutputError)
    end
  end

  describe "healing prompts" do
    it "generates appropriate healing prompts for JSON errors" do
      response = OpenRouter::Response.new(
        { "choices" => [{ "message" => { "content" => malformed_json } }] },
        response_format: basic_schema
      )
      response.client = client

      healed_response = double("Response", content: valid_json)
      
      expect(client).to receive(:complete).with(
        [{ role: "user", content: match(/fix.*json/i) }],
        hash_including(model: "openai/gpt-4o-mini")
      ) do |messages, _options|
        prompt = messages.first[:content]
        expect(prompt).to include(malformed_json)
        expect(prompt).to include("valid JSON")
        expect(prompt).to include("ONLY")
        healed_response
      end

      response.structured_output(auto_heal: true)
    end

    it "includes schema information in healing prompts when available" do
      schema = OpenRouter::Schema.define("test") do
        string :name, required: true
      end

      response = OpenRouter::Response.new(
        { "choices" => [{ "message" => { "content" => '{"age": 30}' } }] },
        response_format: schema
      )
      response.client = client

      healed_response = double("Response", content: '{"name": "John"}')
      
      expect(client).to receive(:complete) do |messages, _options|
        prompt = messages.first[:content]
        expect(prompt).to include("schema")
        expect(prompt).to include("validation failed")
        healed_response
      end

      response.structured_output(auto_heal: true)
    end
  end
end

================
File: spec/response_mode_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Response structured output modes" do
  let(:schema) do
    OpenRouter::Schema.define("test_user") do
      string :name, required: true
      integer :age, required: true
      string :email, format: "email"
    end
  end
  
  let(:response_format) do
    {
      type: "json_schema",
      json_schema: schema
    }
  end

  describe "#structured_output" do
    context "with valid JSON content" do
      let(:json_content) { '{"name": "John", "age": 30, "email": "john@example.com"}' }
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => json_content}}]}, response_format: response_format) }

      context "with mode: :strict (default)" do
        it "parses and returns JSON object" do
          result = response.structured_output(mode: :strict)
          expect(result).to eq({"name" => "John", "age" => 30, "email" => "john@example.com"})
        end

        it "validates against schema when available" do
          expect(schema).to receive(:validate).and_return(true)
          response.structured_output(mode: :strict, auto_heal: false)
        end

        it "uses strict mode by default" do
          result = response.structured_output
          expect(result).to eq({"name" => "John", "age" => 30, "email" => "john@example.com"})
        end
      end

      context "with mode: :gentle" do
        it "parses and returns JSON object without validation" do
          result = response.structured_output(mode: :gentle)
          expect(result).to eq({"name" => "John", "age" => 30, "email" => "john@example.com"})
        end

        it "does not validate against schema" do
          expect(schema).not_to receive(:validate)
          response.structured_output(mode: :gentle)
        end
      end
    end

    context "with invalid JSON content" do
      let(:invalid_json) { '{"name": "John", "age": thirty}' }  # Invalid: age should be number
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => invalid_json}}]}, response_format: response_format) }

      context "with mode: :strict" do
        it "raises StructuredOutputError on invalid JSON" do
          expect {
            response.structured_output(mode: :strict)
          }.to raise_error(OpenRouter::StructuredOutputError, /Failed to parse structured output/)
        end

        context "with auto_heal enabled" do
          let(:mock_client) do
            double("client", configuration: double(auto_heal_responses: true, max_heal_attempts: 2, healer_model: "gpt-3.5-turbo"))
          end

          before do
            response.client = mock_client
          end

          it "attempts healing on parse failure" do
            expect(response).to receive(:heal_structured_response).and_return({"name" => "John", "age" => 30})
            result = response.structured_output(mode: :strict, auto_heal: true)
            expect(result).to eq({"name" => "John", "age" => 30})
          end
        end
      end

      context "with mode: :gentle" do
        it "returns nil on invalid JSON instead of raising" do
          result = response.structured_output(mode: :gentle)
          expect(result).to be_nil
        end

        it "does not attempt healing even if auto_heal is enabled" do
          expect(response).not_to receive(:heal_structured_response)
          response.structured_output(mode: :gentle)
        end
      end
    end

    context "with schema validation failures" do
      let(:invalid_data) { '{"name": "John", "age": "thirty", "email": "invalid-email"}' }  # Wrong types
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => invalid_data}}]}, response_format: response_format) }

      context "with mode: :strict" do
        it "raises StructuredOutputError on schema validation failure" do
          expect {
            response.structured_output(mode: :strict)
          }.to raise_error(OpenRouter::StructuredOutputError, /Schema validation failed/)
        end
      end

      context "with mode: :gentle" do
        it "returns parsed JSON even if schema validation would fail" do
          result = response.structured_output(mode: :gentle)
          expect(result).to eq({"name" => "John", "age" => "thirty", "email" => "invalid-email"})
        end
      end
    end

    context "with no structured output content" do
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => "Just a regular text response"}}]}) }

      it "returns nil in both modes when no response_format is set" do
        expect(response.structured_output(mode: :strict)).to be_nil
        expect(response.structured_output(mode: :gentle)).to be_nil
      end
    end

    context "with invalid mode parameter" do
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => "{}"}}]}, response_format: response_format) }

      it "raises ArgumentError for invalid mode" do
        expect {
          response.structured_output(mode: :invalid)
        }.to raise_error(ArgumentError, /Invalid mode: invalid. Must be :strict or :gentle/)
      end
    end

    context "auto_heal parameter behavior" do
      let(:response) { OpenRouter::Response.new({"choices" => [{"message" => {"content" => '{"broken": json}'}}]}, response_format: response_format) }
      let(:mock_client) do
        double("client", configuration: double(auto_heal_responses: false, max_heal_attempts: 2, healer_model: "gpt-3.5-turbo"))
      end

      before do
        response.client = mock_client
      end

      it "respects explicit auto_heal: true override" do
        expect(response).to receive(:heal_structured_response).and_return({"fixed" => "json"})
        result = response.structured_output(mode: :strict, auto_heal: true)
        expect(result).to eq({"fixed" => "json"})
      end

      it "respects explicit auto_heal: false override" do
        expect(response).not_to receive(:heal_structured_response)
        expect {
          response.structured_output(mode: :strict, auto_heal: false)
        }.to raise_error(OpenRouter::StructuredOutputError)
      end

      it "ignores auto_heal in gentle mode" do
        expect(response).not_to receive(:heal_structured_response)
        result = response.structured_output(mode: :gentle, auto_heal: true)
        expect(result).to be_nil
      end
    end
  end
end

================
File: spec/response_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::Response do
  let(:basic_response) do
    {
      "id" => "chatcmpl-123",
      "object" => "chat.completion",
      "created" => 1_677_652_288,
      "model" => "gpt-3.5-turbo",
      "choices" => [
        {
          "index" => 0,
          "message" => {
            "role" => "assistant",
            "content" => "Hello! How can I help you today?"
          },
          "finish_reason" => "stop"
        }
      ],
      "usage" => {
        "prompt_tokens" => 12,
        "completion_tokens" => 8,
        "total_tokens" => 20
      }
    }
  end

  let(:tool_call_response) do
    {
      "id" => "chatcmpl-123",
      "object" => "chat.completion",
      "created" => 1_677_652_288,
      "model" => "gpt-3.5-turbo",
      "choices" => [
        {
          "index" => 0,
          "message" => {
            "role" => "assistant",
            "content" => nil,
            "tool_calls" => [
              {
                "id" => "call_abc123",
                "type" => "function",
                "function" => {
                  "name" => "search_books",
                  "arguments" => '{"query": "Ruby programming"}'
                }
              }
            ]
          },
          "finish_reason" => "tool_calls"
        }
      ]
    }
  end

  let(:structured_response) do
    {
      "id" => "chatcmpl-123",
      "object" => "chat.completion",
      "created" => 1_677_652_288,
      "model" => "gpt-4",
      "choices" => [
        {
          "index" => 0,
          "message" => {
            "role" => "assistant",
            "content" => '{"location": "London", "temperature": 18, "conditions": "Partly cloudy"}'
          },
          "finish_reason" => "stop"
        }
      ]
    }
  end

  describe "#initialize" do
    it "wraps hash response" do
      response = OpenRouter::Response.new(basic_response)
      expect(response.id).to eq("chatcmpl-123")
      expect(response.model).to eq("gpt-3.5-turbo")
    end

    it "handles non-hash input" do
      response = OpenRouter::Response.new("not a hash")
      expect(response.to_h).to eq({})
    end
  end

  describe "basic accessors" do
    let(:response) { OpenRouter::Response.new(basic_response) }

    it "provides access to response fields" do
      expect(response.id).to eq("chatcmpl-123")
      expect(response.object).to eq("chat.completion")
      expect(response.created).to eq(1_677_652_288)
      expect(response.model).to eq("gpt-3.5-turbo")
      expect(response.usage).to eq(basic_response["usage"])
    end

    it "provides content access" do
      expect(response.content).to eq("Hello! How can I help you today?")
      expect(response.has_content?).to be true
    end

    it "provides choices access" do
      expect(response.choices).to be_an(Array)
      expect(response.choices.first["message"]["content"]).to eq("Hello! How can I help you today?")
    end
  end

  describe "hash delegation" do
    let(:response) { OpenRouter::Response.new(basic_response) }

    it "delegates hash methods" do
      expect(response["id"]).to eq("chatcmpl-123")
      expect(response.dig("choices", 0, "message", "content")).to eq("Hello! How can I help you today?")
      expect(response.key?("model")).to be true
      expect(response.fetch("model")).to eq("gpt-3.5-turbo")
    end

    it "converts to json" do
      json = response.to_json
      expect(JSON.parse(json)["id"]).to eq("chatcmpl-123")
    end
  end

  describe "tool calling" do
    let(:response) { OpenRouter::Response.new(tool_call_response) }

    it "detects tool calls" do
      expect(response.has_tool_calls?).to be true
      expect(response.tool_calls).to be_an(Array)
      expect(response.tool_calls.size).to eq(1)
    end

    it "parses tool calls correctly" do
      tool_call = response.tool_calls.first
      expect(tool_call).to be_a(OpenRouter::ToolCall)
      expect(tool_call.id).to eq("call_abc123")
      expect(tool_call.name).to eq("search_books")
    end

    it "converts to message format" do
      message = response.to_message
      expect(message[:role]).to eq("assistant")
      expect(message[:content]).to be_nil
      expect(message[:tool_calls]).to be_an(Array)
    end
  end

  describe "structured outputs" do
    let(:weather_schema) do
      OpenRouter::Schema.define("weather") do
        string :location, required: true
        number :temperature, required: true
        string :conditions, required: true
      end
    end

    let(:response_format) do
      {
        type: "json_schema",
        json_schema: weather_schema
      }
    end

    let(:response) { OpenRouter::Response.new(structured_response, response_format:) }

    it "parses structured output" do
      output = response.structured_output
      expect(output).to be_a(Hash)
      expect(output["location"]).to eq("London")
      expect(output["temperature"]).to eq(18)
      expect(output["conditions"]).to eq("Partly cloudy")
    end

    it "handles Schema object as response_format" do
      schema_response = OpenRouter::Response.new(structured_response, response_format: weather_schema)
      output = schema_response.structured_output
      expect(output["location"]).to eq("London")
    end

    it "handles invalid JSON gracefully" do
      bad_response = structured_response.dup
      bad_response["choices"][0]["message"]["content"] = "invalid json"

      response = OpenRouter::Response.new(bad_response, response_format:)

      expect do
        response.structured_output
      end.to raise_error(OpenRouter::StructuredOutputError, /parse/)
    end

    # Only test validation if json-schema is available
    if defined?(JSON::Validator)
      it "validates structured output" do
        expect(response.valid_structured_output?).to be true
        expect(response.validation_errors).to be_empty
      end

      it "detects invalid structured output" do
        bad_response = structured_response.dup
        bad_response["choices"][0]["message"]["content"] = '{"location": "London"}' # Missing required fields

        response = OpenRouter::Response.new(bad_response, response_format:)

        expect(response.valid_structured_output?).to be false
        expect(response.validation_errors).not_to be_empty
      end
    end

    it "returns true for validation when no schema provided" do
      basic_resp = OpenRouter::Response.new(basic_response)
      expect(basic_resp.valid_structured_output?).to be true
    end

    it "returns true for validation when JSON::Validator not available" do
      # This test ensures we handle missing validation gracefully
      allow(response).to receive(:validation_available?).and_return(false)
      expect(response.valid_structured_output?).to be true
    end
  end

  describe "error handling" do
    let(:error_response) do
      {
        "error" => {
          "message" => "Invalid API key",
          "type" => "invalid_request_error",
          "code" => "invalid_api_key"
        }
      }
    end

    let(:response) { OpenRouter::Response.new(error_response) }

    it "detects errors" do
      expect(response.error?).to be true
      expect(response.error_message).to eq("Invalid API key")
    end
  end

  describe "empty responses" do
    let(:empty_response) { OpenRouter::Response.new({}) }

    it "handles empty responses gracefully" do
      expect(empty_response.content).to be_nil
      expect(empty_response.has_content?).to be false
      expect(empty_response.has_tool_calls?).to be false
      expect(empty_response.tool_calls).to be_empty
      expect(empty_response.error?).to be false
    end
  end
end

================
File: spec/schema_enhancements_spec.rb
================
# frozen_string_literal: true

require "spec_helper"

RSpec.describe "Schema DSL enhancements" do
  describe "enum support" do
    context "for string fields" do
      let(:schema) do
        OpenRouter::Schema.define("user_with_enum") do
          string :name, required: true
          string :role, required: true, enum: ["admin", "editor", "viewer"]
          string :status, enum: ["active", "inactive", "pending"]
        end
      end

      it "accepts enum parameter for string fields" do
        schema_hash = schema.to_h
        role_property = schema_hash[:schema][:properties][:role]
        
        expect(role_property[:type]).to eq("string")
        expect(role_property[:enum]).to eq(["admin", "editor", "viewer"])
      end

      it "works with optional enum fields" do
        pure_schema_hash = schema.pure_schema
        status_property = pure_schema_hash[:properties][:status]
        
        expect(status_property[:type]).to eq("string")
        expect(status_property[:enum]).to eq(["active", "inactive", "pending"])
        expect(pure_schema_hash[:required]).not_to include("status")
      end

      it "combines enum with other properties" do
        schema = OpenRouter::Schema.define("user_with_enum_desc") do
          string :priority, enum: ["high", "medium", "low"], description: "Task priority level"
        end

        schema_hash = schema.to_h
        priority_property = schema_hash[:schema][:properties][:priority]
        
        expect(priority_property[:enum]).to eq(["high", "medium", "low"])
        expect(priority_property[:description]).to eq("Task priority level")
      end
    end

    context "for integer fields" do
      let(:schema) do
        OpenRouter::Schema.define("config_with_enum") do
          string :name, required: true
          integer :priority, enum: [1, 2, 3, 4, 5]
          integer :category_id, required: true, enum: [10, 20, 30]
        end
      end

      it "accepts enum parameter for integer fields" do
        schema_hash = schema.to_h
        priority_property = schema_hash[:schema][:properties][:priority]
        
        expect(priority_property[:type]).to eq("integer")
        expect(priority_property[:enum]).to eq([1, 2, 3, 4, 5])
      end

      it "works with required integer enum fields" do
        schema_hash = schema.to_h
        category_property = schema_hash[:schema][:properties][:category_id]
        
        expect(category_property[:type]).to eq("integer")
        expect(category_property[:enum]).to eq([10, 20, 30])
        expect(schema_hash[:schema][:required]).to include("category_id")
      end
    end

    context "for number fields" do
      let(:schema) do
        OpenRouter::Schema.define("measurement") do
          number :rating, enum: [1.0, 2.5, 3.0, 4.5, 5.0]
          number :temperature, enum: [-10.5, 0.0, 10.5, 25.0]
        end
      end

      it "accepts enum parameter for number fields" do
        schema_hash = schema.to_h
        rating_property = schema_hash[:schema][:properties][:rating]
        
        expect(rating_property[:type]).to eq("number")
        expect(rating_property[:enum]).to eq([1.0, 2.5, 3.0, 4.5, 5.0])
      end
    end

    context "validation" do
      it "properly serializes enum constraints in schema" do
        schema = OpenRouter::Schema.define("validated_user") do
          string :role, enum: ["admin", "user"]
          integer :level, enum: [1, 2, 3]
        end

        schema_json = JSON.parse(schema.to_h.to_json)
        
        expect(schema_json["schema"]["properties"]["role"]["enum"]).to eq(["admin", "user"])
        expect(schema_json["schema"]["properties"]["level"]["enum"]).to eq([1, 2, 3])
      end
    end
  end

  describe "format support" do
    context "for string fields" do
      let(:schema) do
        OpenRouter::Schema.define("user_with_formats") do
          string :name, required: true
          string :email, required: true, format: "email"
          string :website, format: "uri"
          string :created_at, format: "date-time"
          string :birthday, format: "date"
        end
      end

      it "accepts format parameter for string fields" do
        schema_hash = schema.to_h
        email_property = schema_hash[:schema][:properties][:email]
        
        expect(email_property[:type]).to eq("string")
        expect(email_property[:format]).to eq("email")
      end

      it "supports standard JSON Schema formats" do
        schema_hash = schema.to_h
        properties = schema_hash[:schema][:properties]
        
        expect(properties[:website][:format]).to eq("uri")
        expect(properties[:created_at][:format]).to eq("date-time")
        expect(properties[:birthday][:format]).to eq("date")
      end

      it "combines format with other properties" do
        schema = OpenRouter::Schema.define("contact") do
          string :email, required: true, format: "email", description: "Contact email address"
        end

        schema_hash = schema.to_h
        email_property = schema_hash[:schema][:properties][:email]
        
        expect(email_property[:format]).to eq("email")
        expect(email_property[:description]).to eq("Contact email address")
        expect(schema_hash[:schema][:required]).to include("email")
      end
    end

    context "custom formats" do
      it "accepts custom format values" do
        schema = OpenRouter::Schema.define("custom_formats") do
          string :phone, format: "phone"
          string :slug, format: "slug"
        end

        schema_hash = schema.to_h
        properties = schema_hash[:schema][:properties]
        
        expect(properties[:phone][:format]).to eq("phone")
        expect(properties[:slug][:format]).to eq("slug")
      end
    end

    it "properly serializes format constraints" do
      schema = OpenRouter::Schema.define("formatted_data") do
        string :email, format: "email"
        string :url, format: "uri"
      end

      schema_json = JSON.parse(schema.to_h.to_json)
      
      expect(schema_json["schema"]["properties"]["email"]["format"]).to eq("email")
      expect(schema_json["schema"]["properties"]["url"]["format"]).to eq("uri")
    end
  end

  describe "combined enum and format" do
    it "supports both enum and format on the same field" do
      schema = OpenRouter::Schema.define("complex_field") do
        string :protocol, enum: ["http", "https", "ftp"], format: "uri-reference"
      end

      schema_hash = schema.to_h
      protocol_property = schema_hash[:schema][:properties][:protocol]
      
      expect(protocol_property[:enum]).to eq(["http", "https", "ftp"])
      expect(protocol_property[:format]).to eq("uri-reference")
    end
  end

  describe "array items with enum/format" do
    context "using ItemsBuilder" do
      let(:schema) do
        OpenRouter::Schema.define("tagged_content") do
          string :title, required: true
          array :tags do
            string enum: ["tech", "business", "personal"]
          end
          array :urls do
            string format: "uri"
          end
        end
      end

      it "supports enum in array items" do
        schema_hash = schema.to_h
        tags_property = schema_hash[:schema][:properties][:tags]
        
        expect(tags_property[:type]).to eq("array")
        expect(tags_property[:items][:type]).to eq("string")
        expect(tags_property[:items][:enum]).to eq(["tech", "business", "personal"])
      end

      it "supports format in array items" do
        schema_hash = schema.to_h
        urls_property = schema_hash[:schema][:properties][:urls]
        
        expect(urls_property[:type]).to eq("array")
        expect(urls_property[:items][:type]).to eq("string")
        expect(urls_property[:items][:format]).to eq("uri")
      end
    end

    context "using items parameter" do
      it "works with pre-defined items schema" do
        schema = OpenRouter::Schema.define("predefined_items") do
          array :statuses, items: { type: "string", enum: ["active", "inactive"] }
          array :emails, items: { type: "string", format: "email" }
        end

        schema_hash = schema.to_h
        properties = schema_hash[:schema][:properties]
        
        expect(properties[:statuses][:items][:enum]).to eq(["active", "inactive"])
        expect(properties[:emails][:items][:format]).to eq("email")
      end
    end
  end

  describe "#get_format_instructions" do
    let(:schema) do
      OpenRouter::Schema.define("instruction_test") do
        string :name, required: true, description: "User's full name"
        string :email, required: true, format: "email"
        string :role, enum: ["admin", "editor", "viewer"]
        integer :age
      end
    end

    it "returns clear instructions for model prompting" do
      instructions = schema.get_format_instructions
      
      expect(instructions).to be_a(String)
      expect(instructions).to include("JSON Schema")
      expect(instructions).to include("format your output as a JSON value")
    end

    it "includes schema details in instructions" do
      instructions = schema.get_format_instructions
      
      expect(instructions).to include(schema.to_h.to_json)
      expect(instructions).to include("required")
      expect(instructions).to include("properties")
    end

    it "includes examples and guidelines" do
      instructions = schema.get_format_instructions
      
      expect(instructions).to include("example")
      expect(instructions).to include("match")
      expect(instructions).to include("trailing commas")
    end

    it "emphasizes JSON-only response" do
      instructions = schema.get_format_instructions
      
      expect(instructions).to include("ONLY")
      expect(instructions).to include("no other text")
    end

    context "for forced structured output" do
      it "provides instructions suitable for prompt injection" do
        instructions = schema.get_format_instructions
        
        # Should be suitable for adding to messages
        expect(instructions.length).to be > 100  # Substantial instructions
        expect(instructions).to match(/```json.*```/m)  # Include JSON example
      end
    end
  end

  describe "backward compatibility" do
    it "maintains existing **options behavior" do
      schema = OpenRouter::Schema.define("backward_compat") do
        string :custom_field, custom_option: "value", another: "option"
      end

      schema_hash = schema.to_h
      custom_property = schema_hash[:schema][:properties][:custom_field]
      
      expect(custom_property[:custom_option]).to eq("value")
      expect(custom_property[:another]).to eq("option")
    end

    it "allows **options to override enum/format" do
      # Test override behavior by defining options that would normally conflict
      # The Hash#merge behavior should use the last value for duplicate keys
      options = { enum: ["a", "b"], format: "email" }.merge({ enum: ["x", "y"], format: "uri" })
      
      schema = OpenRouter::Schema.define("override_test") do
        string :field, **options
      end

      schema_hash = schema.to_h
      field_property = schema_hash[:schema][:properties][:field]
      
      # Later values should override
      expect(field_property[:enum]).to eq(["x", "y"])
      expect(field_property[:format]).to eq("uri")
    end
  end

  describe "error handling" do
    it "validates enum values are provided" do
      expect {
        OpenRouter::Schema.define("empty_enum") do
          string :role, enum: []
        end
      }.not_to raise_error  # Empty enum should be allowed
    end

    it "validates format values are strings" do
      expect {
        OpenRouter::Schema.define("invalid_format") do
          string :field, format: 123
        end
      }.not_to raise_error  # Should convert to string
    end
  end
end

================
File: spec/schema_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::Schema do
  describe ".define" do
    it "creates schema with DSL" do
      schema = OpenRouter::Schema.define("weather", strict: true) do
        string :location, required: true, description: "City name"
        number :temperature, required: true, description: "Temperature in Celsius"
        string :conditions, required: true, description: "Weather conditions"
        no_additional_properties
      end

      expect(schema.name).to eq("weather")
      expect(schema.strict).to be true
      expect(schema.schema[:properties][:location][:type]).to eq("string")
      expect(schema.schema[:required]).to include("location", "temperature", "conditions")
      expect(schema.schema[:additionalProperties]).to be false
    end

    it "supports nested objects" do
      schema = OpenRouter::Schema.define("user") do
        string :name, required: true
        object :address, required: true do
          string :street, required: true
          string :city, required: true
          string :country, required: true
        end
      end

      address_props = schema.schema[:properties][:address][:properties]
      expect(address_props[:street][:type]).to eq("string")
      expect(address_props[:city][:type]).to eq("string")
      expect(schema.schema[:properties][:address][:required]).to include("street", "city", "country")
    end

    it "supports arrays with typed items" do
      schema = OpenRouter::Schema.define("book_list") do
        array :books, required: true do
          object do
            string :title, required: true
            string :author, required: true
            integer :pages
          end
        end
      end

      books_def = schema.schema[:properties][:books]
      expect(books_def[:type]).to eq("array")
      expect(books_def[:items][:type]).to eq("object")
      expect(books_def[:items][:properties][:title][:type]).to eq("string")
    end

    it "supports simple array items" do
      schema = OpenRouter::Schema.define("tags") do
        array :tags, required: true do
          string description: "A tag"
        end
      end

      tags_def = schema.schema[:properties][:tags]
      expect(tags_def[:items][:type]).to eq("string")
      expect(tags_def[:items][:description]).to eq("A tag")
    end
  end

  describe ".new" do
    it "creates schema from hash" do
      schema_def = {
        type: "object",
        properties: {
          name: { type: "string" }
        },
        required: [:name]
      }

      schema = OpenRouter::Schema.new("test", schema_def)
      expect(schema.name).to eq("test")
      expect(schema.schema[:properties][:name][:type]).to eq("string")
    end
  end

  describe "#to_h" do
    it "returns proper OpenRouter format" do
      schema = OpenRouter::Schema.define("weather") do
        string :location, required: true
        number :temperature, required: true
      end

      hash = schema.to_h
      expect(hash[:name]).to eq("weather")
      expect(hash[:strict]).to be true
      expect(hash[:schema][:type]).to eq("object")
      expect(hash[:schema][:properties]).to be_a(Hash)
      expect(hash[:schema][:required]).to be_an(Array)
    end
  end

  describe "validation" do
    it "requires name" do
      expect do
        OpenRouter::Schema.new("", {})
      end.to raise_error(ArgumentError, /name/)
    end

    it "requires hash schema" do
      expect do
        OpenRouter::Schema.new("test", "not a hash")
      end.to raise_error(ArgumentError, /hash/)
    end
  end

  describe "property types" do
    it "supports all basic types" do
      schema = OpenRouter::Schema.define("types_test") do
        string :str_field
        integer :int_field
        number :num_field
        boolean :bool_field
        array :arr_field
        object :obj_field
      end

      props = schema.schema[:properties]
      expect(props[:str_field][:type]).to eq("string")
      expect(props[:int_field][:type]).to eq("integer")
      expect(props[:num_field][:type]).to eq("number")
      expect(props[:bool_field][:type]).to eq("boolean")
      expect(props[:arr_field][:type]).to eq("array")
      expect(props[:obj_field][:type]).to eq("object")
    end

    it "supports property options" do
      schema = OpenRouter::Schema.define("options_test") do
        string :name, required: true, description: "User name", minLength: 1, maxLength: 50
        integer :age, minimum: 0, maximum: 150
      end

      name_prop = schema.schema[:properties][:name]
      age_prop = schema.schema[:properties][:age]

      expect(name_prop[:description]).to eq("User name")
      expect(name_prop[:minLength]).to eq(1)
      expect(name_prop[:maxLength]).to eq(50)
      expect(age_prop[:minimum]).to eq(0)
      expect(age_prop[:maximum]).to eq(150)
      expect(schema.schema[:required]).to include("name")
    end
  end

  describe "strict mode" do
    it "defaults to strict true" do
      schema = OpenRouter::Schema.define("test") do
        string :name
      end

      expect(schema.strict).to be true
    end

    it "can be set to false" do
      schema = OpenRouter::Schema.define("test", strict: false) do
        string :name
      end

      expect(schema.strict).to be false
    end

    it "sets additionalProperties to false when strict" do
      schema = OpenRouter::Schema.define("test") do
        strict true
        string :name
      end

      expect(schema.schema[:additionalProperties]).to be false
    end
  end

  # Only test validation if json-schema is available
  describe "validation", if: defined?(JSON::Validator) do
    let(:schema) do
      OpenRouter::Schema.define("person") do
        string :name, required: true
        integer :age, required: true, minimum: 0
      end
    end

    it "validates correct data" do
      data = { "name" => "John", "age" => 30 }
      expect(schema.validate(data)).to be true
      expect(schema.validation_errors(data)).to be_empty
    end

    it "rejects invalid data" do
      data = { "name" => "John", "age" => -5 }
      expect(schema.validate(data)).to be false
      expect(schema.validation_errors(data)).not_to be_empty
    end

    it "rejects missing required fields" do
      data = { "name" => "John" }
      expect(schema.validate(data)).to be false
      expect(schema.validation_errors(data)).not_to be_empty
    end
  end

  describe "validation availability" do
    let(:schema) { OpenRouter::Schema.new("test", {}) }

    it "detects when JSON::Validator is available" do
      if defined?(JSON::Validator)
        expect(schema.validation_available?).to be true
      else
        expect(schema.validation_available?).to be false
      end
    end
  end
end

================
File: spec/spec_helper.rb
================
# frozen_string_literal: true

require 'dotenv/load'
require 'vcr'
require 'webmock/rspec'
require 'json-schema' # Enable schema validation in tests

# Load VCR configuration
require_relative 'support/vcr'

require_relative "../lib/open_router"

RSpec.configure do |config|
  # Enable flags like --only-failures and --next-failure
  config.example_status_persistence_file_path = ".rspec_status"

  # Disable RSpec exposing methods globally on Module and main
  config.disable_monkey_patching!

  config.expect_with :rspec do |c|
    c.syntax = :expect
  end

  # VCR is configured automatically via config.configure_rspec_metadata!
end

================
File: spec/tool_call_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::ToolCall do
  let(:tool_call_data) do
    {
      "id" => "call_abc123",
      "type" => "function",
      "function" => {
        "name" => "search_books",
        "arguments" => '{"query": "Ruby programming", "limit": 5}'
      }
    }
  end

  describe "#initialize" do
    it "parses tool call data correctly" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      expect(tool_call.id).to eq("call_abc123")
      expect(tool_call.type).to eq("function")
      expect(tool_call.function_name).to eq("search_books")
      expect(tool_call.name).to eq("search_books")
    end

    it "raises error for invalid data" do
      invalid_data = { "id" => "call_123", "type" => "function" }

      expect do
        OpenRouter::ToolCall.new(invalid_data)
      end.to raise_error(OpenRouter::ToolCallError, /missing function/)
    end
  end

  describe "#arguments" do
    it "parses JSON arguments correctly" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      expect(tool_call.arguments).to eq({
                                          "query" => "Ruby programming",
                                          "limit" => 5
                                        })
    end

    it "raises error for invalid JSON" do
      bad_data = tool_call_data.dup
      bad_data["function"]["arguments"] = "invalid json"

      tool_call = OpenRouter::ToolCall.new(bad_data)

      expect do
        tool_call.arguments
      end.to raise_error(OpenRouter::ToolCallError, /parse/)
    end

    it "caches parsed arguments" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      args1 = tool_call.arguments
      args2 = tool_call.arguments

      expect(args1).to be(args2) # Same object
    end
  end

  describe "#execute" do
    it "executes with block and returns result" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      result = tool_call.execute do |name, args|
        expect(name).to eq("search_books")
        expect(args["query"]).to eq("Ruby programming")
        "Found 3 books"
      end

      expect(result).to be_a(OpenRouter::ToolResult)
      expect(result.result).to eq("Found 3 books")
      expect(result.success?).to be true
    end

    it "requires a block" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      expect do
        tool_call.execute
      end.to raise_error(ArgumentError, /Block required/)
    end
  end

  describe "#to_message" do
    it "converts to assistant message format" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      message = tool_call.to_message

      expect(message[:role]).to eq("assistant")
      expect(message[:content]).to be_nil
      expect(message[:tool_calls]).to be_an(Array)
      expect(message[:tool_calls].first[:id]).to eq("call_abc123")
    end
  end

  describe "#to_result_message" do
    it "converts result to tool message format" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      message = tool_call.to_result_message("Search completed")

      expect(message[:role]).to eq("tool")
      expect(message[:tool_call_id]).to eq("call_abc123")
      expect(message[:name]).to eq("search_books")
      expect(message[:content]).to eq("Search completed")
    end

    it "converts non-string results to JSON" do
      tool_call = OpenRouter::ToolCall.new(tool_call_data)

      result = { books: ["Book 1", "Book 2"] }
      message = tool_call.to_result_message(result)

      expect(message[:content]).to eq(result.to_json)
    end
  end
end

RSpec.describe OpenRouter::ToolResult do
  let(:tool_call_data) do
    {
      "id" => "call_123",
      "type" => "function",
      "function" => {
        "name" => "test_tool",
        "arguments" => "{}"
      }
    }
  end

  let(:tool_call) { OpenRouter::ToolCall.new(tool_call_data) }

  describe ".success" do
    it "creates successful result" do
      result = OpenRouter::ToolResult.success(tool_call, "Success!")

      expect(result.success?).to be true
      expect(result.failure?).to be false
      expect(result.result).to eq("Success!")
      expect(result.error).to be_nil
    end
  end

  describe ".failure" do
    it "creates failed result" do
      result = OpenRouter::ToolResult.failure(tool_call, "Error occurred")

      expect(result.success?).to be false
      expect(result.failure?).to be true
      expect(result.result).to be_nil
      expect(result.error).to eq("Error occurred")
    end
  end

  describe "#to_message" do
    it "converts to tool message with result" do
      result = OpenRouter::ToolResult.success(tool_call, "Success!")
      message = result.to_message

      expect(message[:role]).to eq("tool")
      expect(message[:content]).to eq("Success!")
    end

    it "converts to tool message with error" do
      result = OpenRouter::ToolResult.failure(tool_call, "Failed!")
      message = result.to_message

      expect(message[:role]).to eq("tool")
      expect(message[:content]).to eq("Failed!")
    end
  end
end

================
File: spec/tool_spec.rb
================
# frozen_string_literal: true

RSpec.describe OpenRouter::Tool do
  describe ".define" do
    it "creates a tool with DSL" do
      tool = OpenRouter::Tool.define do
        name "search_books"
        description "Search for books"

        parameters do
          string :query, required: true, description: "Search query"
          integer :limit, description: "Max results"
        end
      end

      expect(tool.name).to eq("search_books")
      expect(tool.description).to eq("Search for books")
      expect(tool.parameters[:properties][:query][:type]).to eq("string")
      expect(tool.parameters[:required]).to include(:query)
    end

    it "supports array parameters with items" do
      tool = OpenRouter::Tool.define do
        name "process_list"
        description "Process a list"

        parameters do
          array :items, required: true do
            string description: "Item in the list"
          end
        end
      end

      expect(tool.parameters[:properties][:items][:type]).to eq("array")
      expect(tool.parameters[:properties][:items][:items][:type]).to eq("string")
    end

    it "supports nested object parameters" do
      tool = OpenRouter::Tool.define do
        name "create_user"
        description "Create a user"

        parameters do
          object :user, required: true do
            string :name, required: true
            integer :age
            boolean :active, required: true
          end
        end
      end

      user_props = tool.parameters[:properties][:user][:properties]
      expect(user_props[:name][:type]).to eq("string")
      expect(user_props[:age][:type]).to eq("integer")
      expect(user_props[:active][:type]).to eq("boolean")
      expect(tool.parameters[:properties][:user][:required]).to include(:name, :active)
    end
  end

  describe ".new" do
    it "creates a tool from hash definition" do
      definition = {
        name: "test_tool",
        description: "A test tool",
        parameters: {
          type: "object",
          properties: {
            input: { type: "string" }
          }
        }
      }

      tool = OpenRouter::Tool.new(definition)
      expect(tool.name).to eq("test_tool")
      expect(tool.description).to eq("A test tool")
    end

    it "accepts function format" do
      definition = {
        type: "function",
        function: {
          name: "test_tool",
          description: "A test tool",
          parameters: {
            type: "object",
            properties: {
              input: { type: "string" }
            }
          }
        }
      }

      tool = OpenRouter::Tool.new(definition)
      expect(tool.name).to eq("test_tool")
      expect(tool.type).to eq("function")
    end
  end

  describe "#to_h" do
    it "returns proper OpenRouter format" do
      tool = OpenRouter::Tool.define do
        name "search"
        description "Search function"
        parameters do
          string :query, required: true
        end
      end

      hash = tool.to_h
      expect(hash[:type]).to eq("function")
      expect(hash[:function][:name]).to eq("search")
      expect(hash[:function][:description]).to eq("Search function")
      expect(hash[:function][:parameters]).to be_a(Hash)
    end
  end

  describe "validation" do
    it "requires name" do
      expect do
        OpenRouter::Tool.new({ description: "No name" })
      end.to raise_error(ArgumentError, /name/)
    end

    it "requires description" do
      expect do
        OpenRouter::Tool.new({ name: "no_description" })
      end.to raise_error(ArgumentError, /description/)
    end

    it "validates parameters type" do
      expect do
        OpenRouter::Tool.new({
                               name: "test",
                               description: "test",
                               parameters: { type: "string" }
                             })
      end.to raise_error(ArgumentError, /object/)
    end
  end
end

================
File: .gitignore
================
.DS_Store
/.DS_Store

.claude
.cline
CLAUDE.md
/.repomix/

/.bundle/
/.yardoc
/_yardoc/
/coverage/
/doc/
/pkg/
/spec/reports/
/tmp/

# rspec failure tracking
.rspec_status
*.gem
.env
/lib/open_router/.api_key

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
.env
*.json
*.md
spec/support
spec/fixtures
*.xml
*.txt

================
File: .rspec
================
--format documentation
--color
--require spec_helper

================
File: .rubocop_todo.yml
================
# This configuration was generated by
# `rubocop --auto-gen-config`
# on 2025-08-15 16:38:23 UTC using RuboCop version 1.55.1.
# The point is for the user to remove these configuration records
# one by one as the offenses are removed from the code base.
# Note that changes in the inspected code, or installation of new
# versions of RuboCop, may require this file to be generated again.

================
File: .rubocop.yml
================
inherit_from: .rubocop_todo.yml

AllCops:
  TargetRubyVersion: 3.2

Layout/LineLength:
  Enabled: false

Metrics/AbcSize:
  Enabled: false

Metrics/BlockLength:
  Enabled: false

Metrics/CyclomaticComplexity:
  Enabled: false

Metrics/MethodLength:
  Enabled: false

Metrics/ParameterLists:
  Enabled: false

Metrics/PerceivedComplexity:
  Enabled: false

Style/Documentation:
  Enabled: false

Style/StringLiterals:
  Enabled: true
  EnforcedStyle: double_quotes

Style/StringLiteralsInInterpolation:
  Enabled: true
  EnforcedStyle: double_quotes

================
File: .ruby-version
================
3.2.2

================
File: Gemfile
================
# frozen_string_literal: true

source "https://rubygems.org"

# Specify your gem's dependencies in open_router.gemspec
gemspec

gem "activesupport", ">= 6.0"

group :development, :test do
  gem "dotenv", ">= 2"
  gem "pry", ">= 0.14"
  gem "vcr", "~> 6.2"
  gem "webmock", "~> 3.19"
end

group :development do
  gem "rake", "~> 13.0"
  gem "rspec", "~> 3.0"
  gem "rubocop", "~> 1.21"
  gem "solargraph-rails", "~> 0.2.0.pre"
  gem "sorbet"
  gem "tapioca", require: false
end

================
File: Gemfile.lock
================
PATH
  remote: .
  specs:
    open_router (0.3.3)
      activesupport (>= 6.0)
      dotenv (>= 2)
      faraday (>= 1)
      faraday-multipart (>= 1)
      json-schema (~> 4.0)

GEM
  remote: https://rubygems.org/
  specs:
    activesupport (7.1.3.2)
      base64
      bigdecimal
      concurrent-ruby (~> 1.0, >= 1.0.2)
      connection_pool (>= 2.2.5)
      drb
      i18n (>= 1.6, < 2)
      minitest (>= 5.1)
      mutex_m
      tzinfo (~> 2.0)
    addressable (2.8.7)
      public_suffix (>= 2.0.2, < 7.0)
    ast (2.4.2)
    backport (1.2.0)
    base64 (0.2.0)
    benchmark (0.3.0)
    bigdecimal (3.1.7)
    coderay (1.1.3)
    concurrent-ruby (1.2.3)
    connection_pool (2.4.1)
    crack (1.0.0)
      bigdecimal
      rexml
    diff-lcs (1.5.0)
    dotenv (3.1.0)
    drb (2.2.1)
    e2mmap (0.1.0)
    erubi (1.12.0)
    faraday (2.7.10)
      faraday-net_http (>= 2.0, < 3.1)
      ruby2_keywords (>= 0.0.4)
    faraday-multipart (1.0.4)
      multipart-post (~> 2)
    faraday-net_http (3.0.2)
    hashdiff (1.2.0)
    i18n (1.14.4)
      concurrent-ruby (~> 1.0)
    jaro_winkler (1.5.6)
    json (2.6.3)
    json-schema (4.3.1)
      addressable (>= 2.8)
    kramdown (2.4.0)
      rexml
    kramdown-parser-gfm (1.1.0)
      kramdown (~> 2.0)
    language_server-protocol (3.17.0.3)
    method_source (1.0.0)
    mini_portile2 (2.8.9)
    minitest (5.22.3)
    multipart-post (2.3.0)
    mutex_m (0.2.0)
    netrc (0.11.0)
    nokogiri (1.16.4)
      mini_portile2 (~> 2.8.2)
      racc (~> 1.4)
    parallel (1.23.0)
    parser (3.2.2.3)
      ast (~> 2.4.1)
      racc
    prism (0.24.0)
    pry (0.14.2)
      coderay (~> 1.1)
      method_source (~> 1.0)
    public_suffix (6.0.2)
    racc (1.7.1)
    rainbow (3.1.1)
    rake (13.0.6)
    rbi (0.1.10)
      prism (>= 0.18.0, < 0.25)
      sorbet-runtime (>= 0.5.9204)
    rbs (2.8.4)
    regexp_parser (2.8.1)
    reverse_markdown (2.1.1)
      nokogiri
    rexml (3.2.6)
    rspec (3.12.0)
      rspec-core (~> 3.12.0)
      rspec-expectations (~> 3.12.0)
      rspec-mocks (~> 3.12.0)
    rspec-core (3.12.2)
      rspec-support (~> 3.12.0)
    rspec-expectations (3.12.3)
      diff-lcs (>= 1.2.0, < 2.0)
      rspec-support (~> 3.12.0)
    rspec-mocks (3.12.6)
      diff-lcs (>= 1.2.0, < 2.0)
      rspec-support (~> 3.12.0)
    rspec-support (3.12.1)
    rubocop (1.55.1)
      json (~> 2.3)
      language_server-protocol (>= 3.17.0)
      parallel (~> 1.10)
      parser (>= 3.2.2.3)
      rainbow (>= 2.2.2, < 4.0)
      regexp_parser (>= 1.8, < 3.0)
      rexml (>= 3.2.5, < 4.0)
      rubocop-ast (>= 1.28.1, < 2.0)
      ruby-progressbar (~> 1.7)
      unicode-display_width (>= 2.4.0, < 3.0)
    rubocop-ast (1.29.0)
      parser (>= 3.2.1.0)
    ruby-progressbar (1.13.0)
    ruby2_keywords (0.0.5)
    solargraph (0.50.0)
      backport (~> 1.2)
      benchmark
      bundler (~> 2.0)
      diff-lcs (~> 1.4)
      e2mmap
      jaro_winkler (~> 1.5)
      kramdown (~> 2.3)
      kramdown-parser-gfm (~> 1.1)
      parser (~> 3.0)
      rbs (~> 2.0)
      reverse_markdown (~> 2.0)
      rubocop (~> 1.38)
      thor (~> 1.0)
      tilt (~> 2.0)
      yard (~> 0.9, >= 0.9.24)
    solargraph-rails (0.2.2.pre.4)
      activesupport
      solargraph (>= 0.41.1)
    sorbet (0.5.11342)
      sorbet-static (= 0.5.11342)
    sorbet-runtime (0.5.11342)
    sorbet-static (0.5.11342-universal-darwin)
    sorbet-static (0.5.11342-x86_64-linux)
    sorbet-static-and-runtime (0.5.11342)
      sorbet (= 0.5.11342)
      sorbet-runtime (= 0.5.11342)
    spoom (1.3.0)
      erubi (>= 1.10.0)
      prism (>= 0.19.0)
      sorbet-static-and-runtime (>= 0.5.10187)
      thor (>= 0.19.2)
    tapioca (0.13.1)
      bundler (>= 2.2.25)
      netrc (>= 0.11.0)
      parallel (>= 1.21.0)
      rbi (>= 0.1.4, < 0.2)
      sorbet-static-and-runtime (>= 0.5.11087)
      spoom (>= 1.2.0)
      thor (>= 1.2.0)
      yard-sorbet
    thor (1.3.1)
    tilt (2.3.0)
    tzinfo (2.0.6)
      concurrent-ruby (~> 1.0)
    unicode-display_width (2.4.2)
    vcr (6.3.1)
      base64
    webmock (3.25.1)
      addressable (>= 2.8.0)
      crack (>= 0.3.2)
      hashdiff (>= 0.4.0, < 2.0.0)
    yard (0.9.36)
    yard-sorbet (0.8.1)
      sorbet-runtime (>= 0.5)
      yard (>= 0.9)

PLATFORMS
  arm64-darwin-21
  arm64-darwin-24
  x86_64-linux

DEPENDENCIES
  activesupport (>= 6.0)
  dotenv (>= 2)
  open_router!
  pry (>= 0.14)
  rake (~> 13.0)
  rspec (~> 3.0)
  rubocop (~> 1.21)
  solargraph-rails (~> 0.2.0.pre)
  sorbet
  tapioca
  vcr (~> 6.2)
  webmock (~> 3.19)

BUNDLED WITH
   2.4.12

================
File: open_router.gemspec
================
# frozen_string_literal: true

require_relative "lib/open_router/version"

Gem::Specification.new do |spec|
  spec.name = "open_router"
  spec.version = OpenRouter::VERSION
  spec.authors = ["Obie Fernandez"]
  spec.email = ["obiefernandez@gmail.com"]

  spec.summary = "Ruby library for OpenRouter API."
  spec.homepage = "https://github.com/OlympiaAI/open_router"
  spec.license = "MIT"
  spec.required_ruby_version = ">= 3.2.2"

  spec.metadata["homepage_uri"] = spec.homepage
  spec.metadata["source_code_uri"] = "https://github.com/OlympiaAI/open_router"
  spec.metadata["changelog_uri"] = "https://github.com/OlympiaAI/open_router/blob/main/CHANGELOG.md"

  spec.files = Dir.chdir(__dir__) do
    `git ls-files -z`.split("\x0").reject do |f|
      (File.expand_path(f) == __FILE__) || f.end_with?(".gem") || f.start_with?(*%w[bin/ test/ spec/ features/ .git
                                                                                    .circleci appveyor])
    end
  end

  spec.bindir = "exe"
  spec.executables = spec.files.grep(%r{\Aexe/}) { |f| File.basename(f) }
  spec.require_paths = ["lib"]

  spec.add_dependency "activesupport", ">= 6.0"
  spec.add_dependency "dotenv", ">= 2"
  spec.add_dependency "faraday", ">= 1"
  spec.add_dependency "faraday-multipart", ">= 1"
  spec.add_dependency "json-schema", "~> 4.0"
end

================
File: Rakefile
================
# frozen_string_literal: true

require "bundler/gem_tasks"
require "rspec/core/rake_task"

RSpec::Core::RakeTask.new(:spec)

require "rubocop/rake_task"

RuboCop::RakeTask.new

task default: %i[spec rubocop]

================
File: test_structured_output_fix.rb
================
#!/usr/bin/env ruby
# frozen_string_literal: true

require_relative 'lib/open_router'

# Check if API key is available
unless ENV['OPENROUTER_API_KEY']
  puts "❌ No OPENROUTER_API_KEY found. Set environment variable and try again."
  exit 1
end

puts "🧪 Testing structured output fix with real OpenRouter API call..."

client = OpenRouter::Client.new(access_token: ENV['OPENROUTER_API_KEY'])

# Create a simple schema
schema = OpenRouter::Schema.define("simple_person") do
  string :name, required: true, description: "Person's name"
  integer :age, required: true, description: "Person's age in years"
  string :occupation, required: false, description: "Person's job"
end

puts "\n📋 Schema format being sent:"
puts JSON.pretty_generate(client.send(:serialize_response_format, schema))

messages = [
  {
    role: "user", 
    content: "Create JSON for a person named Alice who is 28 years old and works as a software engineer."
  }
]

begin
  puts "\n🚀 Making API request..."
  
  response = client.complete(
    messages,
    model: "openai/gpt-4o-mini", # Use a model that supports structured outputs
    response_format: schema,
    extras: { max_tokens: 200, temperature: 0.1 }
  )
  
  puts "✅ API request succeeded!"
  puts "\n📨 Raw response content:"
  puts response.content
  
  puts "\n📊 Structured output:"
  structured = response.structured_output
  puts structured.inspect
  
  puts "\n🧪 Validation checks:"
  puts "- Name is string: #{structured["name"].is_a?(String)}"
  puts "- Age is integer: #{structured["age"].is_a?(Integer)}"
  puts "- Has name 'Alice': #{structured["name"]&.include?("Alice")}"
  puts "- Age is 28: #{structured["age"] == 28}"
  
  if structured["occupation"]
    puts "- Has occupation: #{structured["occupation"]}"
  end
  
  puts "\n🎉 Test completed successfully! The 400 BadRequestError issue has been fixed."
  
rescue => e
  puts "❌ Test failed with error:"
  puts "Error class: #{e.class.name}"
  puts "Error message: #{e.message}"
  
  if e.is_a?(OpenRouter::ServerError) && e.message.include?("400")
    puts "\n🔍 This is still the 400 error we're trying to fix."
    puts "The schema serialization may still have issues."
  else
    puts "\n📝 This might be a different issue (API key, network, model availability, etc.)"
  end
  
  exit 1
end
